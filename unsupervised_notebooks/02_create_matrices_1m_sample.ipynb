{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50ecfac",
   "metadata": {},
   "source": [
    "# Create Clustering Matrices\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4584b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8bb81e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/datallah/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "### text preprocessing dependencies\n",
    "import nltk\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "### sklearn dependencies\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "### gensim dependencies\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9d33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/datallah/datallah-jaymefis-gibsonce/'\n",
    "sample_size = 'one'\n",
    "random_state = 42\n",
    "stop = {'a', 'about', 'above', 'after', 'again', 'against', 'ain',\n",
    "        'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "        'as', 'at', 'be', 'because', 'been', 'before', 'being',\n",
    "        'below', 'between', 'both', 'but', 'by', 'can', 'couldn',\n",
    "        \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does',\n",
    "        'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during',\n",
    "        'each', 'few', 'for', 'from', 'further', 'had', 'hadn',\n",
    "        \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\",\n",
    "        'having', 'here', 'how', 'i', 'if', 'in', 'into', 'is', 'isn',\n",
    "        \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm',\n",
    "        'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn',\n",
    "        \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor',\n",
    "        'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or',\n",
    "        'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "        're', 's', 'same', 'shan', \"shan't\", 'should', \"should've\",\n",
    "        'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than',\n",
    "        'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves',\n",
    "        'then', 'there', 'these', 'they', 'this', 'those', 'through',\n",
    "        'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn',\n",
    "        \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "        'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won',\n",
    "        \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\",\n",
    "        \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f476b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_path = filepath + 'matrices/'\n",
    "if os.path.exists(v_path) == False:\n",
    "    os.mkdir(v_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f740b",
   "metadata": {},
   "source": [
    "## Load & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c767663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df's in dict iterable\n",
    "def load_df_dict(size = sample_size, typ = None):\n",
    "    \"\"\"\n",
    "    Accepts 'one', 'three', or 'five' as input sizes.\n",
    "    If 'train', 'validate', or 'test' are inserted for type,\n",
    "    only that kind of sample will be loaded.\n",
    "    \"\"\"\n",
    "    df_dict = {}\n",
    "    for sample in os.listdir(filepath + 'samples/'):\n",
    "        if size in sample and (typ is None or typ in sample):\n",
    "            name = sample.replace('.csv', '')\n",
    "            temp_df = pd.read_csv(filepath + 'samples/' + sample)[['source', ' response_text', ' op_gender']]\n",
    "            temp_df = temp_df.rename(columns = {' response_text': 'response_text', ' op_gender': 'op_gender'}).dropna()\n",
    "#             df_dict[name] = temp_df\n",
    "            df_dict['X_' + name] = temp_df.response_text\n",
    "            df_dict['y_' + name] = temp_df.op_gender\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81e96ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_validate_one_million', 'y_validate_one_million', 'X_test_one_million', 'y_test_one_million', 'X_train_one_million', 'y_train_one_million'])\n"
     ]
    }
   ],
   "source": [
    "df_dict = load_df_dict(size = sample_size)\n",
    "print(df_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47862bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove links as these will only apply to specific responses\n",
    "def rm_link(text):\n",
    "    return re.sub(r'https?://\\S+', '', text)\n",
    "\n",
    "# remove punctuation\n",
    "def rm_punct(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# create class that lemmatizes tweet tokens\n",
    "# this will be used when creating the term matrix\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.tt = TweetTokenizer(preserve_case=False, reduce_len=True,\n",
    "                                 strip_handles=True, match_phone_numbers=False)\n",
    "    def __call__(self, docs):\n",
    "        return [self.wnl.lemmatize(t) for t in self.tt.tokenize(rm_link(rm_punct(docs))) if t not in stop]\n",
    "\n",
    "# creates a term matrix\n",
    "def train_vectorizer(text_data, vectorizer=CountVectorizer, tokenizer=LemmaTokenizer(),\n",
    "                     ngram_range_lower = 1, ngram_range_upper = 1, min_df = 1):\n",
    "    \"\"\"\n",
    "    Trains a vectorizer on the provided text data and returns the vectorizer instance,\n",
    "    the document-term matrix, and the feature names.\n",
    "\n",
    "    Parameters:\n",
    "    - text_data: List of text documents to be vectorized.\n",
    "    - vectorizer: Vectorizer class to be used for text vectorization. Defaults to CountVectorizer.\n",
    "    - tokenizer: Tokenizer class to be used for tokenizing the text documents. Defaults to TweetTokenizer.\n",
    "    - ngram_range_lower: What's the minimum length of n-grams we want.\n",
    "    - ngram_range_upper: What's the maximum length of n-grams we want.\n",
    "    - min_df: Minimum data frequency.\n",
    "    - max_df: Maximum data frequency.\n",
    "\n",
    "    Returns:\n",
    "    - instance: The trained vectorizer instance.\n",
    "    - matrix: The document-term matrix resulting from fitting the vectorizer on `text_data`.\n",
    "    - features: An array of feature names generated by the vectorizer.\n",
    "    \"\"\"\n",
    "    # Initialize the vectorizer with specified configurations\n",
    "    instance = vectorizer(\n",
    "        strip_accents=None,  # Do not strip accents\n",
    "        lowercase=True,  # Do not convert characters to lowercase\n",
    "        tokenizer=tokenizer,  # Use the tokenize method of the tokenizer instance\n",
    "        token_pattern=None,  # Since a tokenizer is provided, token_pattern is not used\n",
    "        ngram_range=(ngram_range_lower, ngram_range_upper),  # Consider only single words (1-grams)\n",
    "        min_df=min_df,  # Minimum document frequency for filtering terms\n",
    "        max_features=None  # No limit on the number of features\n",
    "    )\n",
    "\n",
    "    # Fit the vectorizer on the provided text data and transform the data into a matrix\n",
    "    vector = instance.fit(text_data)\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36a1ebb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       oure welcome. Hope youre fine!\n",
       "1                                            Thank you\n",
       "2    As someone that worked in the world of for-pro...\n",
       "3                                                 Neat\n",
       "4    Seriously. My bathroom and bedroom are a freak...\n",
       "Name: response_text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_dict[[key for key in df_dict.keys() if 'X_train' in key][0]]\n",
    "X_val = df_dict[[key for key in df_dict.keys() if 'X_val' in key][0]]\n",
    "X_test = df_dict[[key for key in df_dict.keys() if 'X_test' in key][0]]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377f05f",
   "metadata": {},
   "source": [
    "## Create & Store Count Vector & Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3a5947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_v = train_vectorizer(text_data = X_train,\n",
    "                         vectorizer = CountVectorizer,\n",
    "                         ngram_range_lower = 1,\n",
    "                         ngram_range_upper = 3,\n",
    "                         min_df = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0d15fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming X_train\n",
      "Transforming X_val\n",
      "Transforming X_test\n"
     ]
    }
   ],
   "source": [
    "print(\"Transforming X_train\")\n",
    "cnt_m = cnt_v.transform(X_train)\n",
    "print(\"Transforming X_val\")\n",
    "cnt_m_val = cnt_v.transform(X_val)\n",
    "print(\"Transforming X_test\")\n",
    "cnt_m_test = cnt_v.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b22412dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save transformed matrices\n",
    "sparse.save_npz(v_path + 'cnt_m_' + [key for key in df_dict.keys() if 'X_train' in key][0] + '.npz', cnt_m)\n",
    "sparse.save_npz(v_path + 'cnt_m_' + [key for key in df_dict.keys() if 'X_val' in key][0] + '.npz', cnt_m_val)\n",
    "sparse.save_npz(v_path + 'cnt_m_' + [key for key in df_dict.keys() if 'X_test' in key][0] + '.npz', cnt_m_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f2eef",
   "metadata": {},
   "source": [
    "## Create & Store TF-IDF Vector & Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786eb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v = train_vectorizer(text_data = X_train,\n",
    "                           vectorizer = TfidfVectorizer,\n",
    "                           ngram_range_lower = 1,\n",
    "                           ngram_range_upper = 3,\n",
    "                           min_df = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b97b81fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming X_train\n",
      "Transforming X_val\n",
      "Transforming X_test\n"
     ]
    }
   ],
   "source": [
    "print(\"Transforming X_train\")\n",
    "tfidf_m = tfidf_v.transform(X_train)\n",
    "print(\"Transforming X_val\")\n",
    "tfidf_m_val = tfidf_v.transform(X_val)\n",
    "print(\"Transforming X_test\")\n",
    "tfidf_m_test = tfidf_v.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a09e7855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save transformed matrices\n",
    "sparse.save_npz(v_path + 'tfidf_m_' + [key for key in df_dict.keys() if 'X_train' in key][0] + '.npz', tfidf_m)\n",
    "sparse.save_npz(v_path + 'tfidf_m_' + [key for key in df_dict.keys() if 'X_val' in key][0] + '.npz', tfidf_m_val)\n",
    "sparse.save_npz(v_path + 'tfidf_m_' + [key for key in df_dict.keys() if 'X_test' in key][0] + '.npz', tfidf_m_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c241e9be",
   "metadata": {},
   "source": [
    "## Create & Store Word2Vec Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ccfe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing X_train\n",
      "Tokenizing X_val\n",
      "Tokenizing X_test\n"
     ]
    }
   ],
   "source": [
    "# manually preprocess since we can't use vectorizer function here\n",
    "print(\"Tokenizing X_train\")\n",
    "tokenized_ser = X_train.str.lower().apply(LemmaTokenizer())\n",
    "print(\"Tokenizing X_val\")\n",
    "tokenized_ser_val = X_val.str.lower().apply(LemmaTokenizer())\n",
    "print(\"Tokenizing X_test\")\n",
    "tokenized_ser_test = X_test.str.lower().apply(LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1b552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement minimum word frequency threshold\n",
    "# term_cnts = Counter(term for doc in tokenized_ser for term in doc)\n",
    "# tok_lst = [[term for term in doc if term_cnts[term] >= 5] for doc in tokenized_ser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09322f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pretrained word2vec model to create term matrix...load model\n",
    "w2v_model = KeyedVectors.load(datapath(v_path + 'word2vec_1m.wordvectors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c028aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over document and calculate average vector for each term\n",
    "# this will create a document vector that we can cluster on\n",
    "def create_w2v_matrix(tok_lst, w2v_model = w2v_model):\n",
    "    doc_vectors = []\n",
    "    for doc in tok_lst:\n",
    "        doc_v_lst = []\n",
    "        for tok in doc:\n",
    "            if tok in w2v_model:\n",
    "                tok_v = w2v_model[tok]\n",
    "                doc_v_lst.append(tok_v)\n",
    "        if doc_v_lst: doc_v = np.mean(doc_v_lst, axis = 0)\n",
    "        else: doc_v = np.zeros(w2v_model.vector_size)\n",
    "        doc_vectors.append(doc_v)\n",
    "    return np.vstack(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64f4f9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming X_train\n",
      "Transforming X_val\n",
      "Transforming X_test\n"
     ]
    }
   ],
   "source": [
    "print(\"Transforming X_train\")\n",
    "w2v_m = create_w2v_matrix(tokenized_ser)\n",
    "print(\"Transforming X_val\")\n",
    "w2v_m_val = create_w2v_matrix(tokenized_ser_val)\n",
    "print(\"Transforming X_test\")\n",
    "w2v_m_test = create_w2v_matrix(tokenized_ser_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f16f0478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save transformed matrices\n",
    "np.save(v_path + 'w2v_m_' + [key for key in df_dict.keys() if 'X_train' in key][0] + '.npy', w2v_m)\n",
    "np.save(v_path + 'w2v_m_' + [key for key in df_dict.keys() if 'X_val' in key][0] + '.npy', w2v_m_val)\n",
    "np.save(v_path + 'w2v_m_' + [key for key in df_dict.keys() if 'X_test' in key][0] + '.npy', w2v_m_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
