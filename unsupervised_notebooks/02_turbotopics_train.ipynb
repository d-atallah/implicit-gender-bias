{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa351dc4",
   "metadata": {},
   "source": [
    "# Turbo Topics Train\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b99112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/blei-lab/turbotopics.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e772541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc57ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/datallah/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "### text preprocessing dependencies\n",
    "import nltk\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "### sklearn dependencies\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "### import gensim dependencies\n",
    "from gensim.models import LdaModel, TfidfModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92912331",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/datallah/datallah-jaymefis-gibsonce/'\n",
    "random_state = 42\n",
    "stop = {'a', 'about', 'above', 'after', 'again', 'against', 'ain',\n",
    "        'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "        'as', 'at', 'be', 'because', 'been', 'before', 'being',\n",
    "        'below', 'between', 'both', 'but', 'by', 'can', 'couldn',\n",
    "        \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does',\n",
    "        'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during',\n",
    "        'each', 'few', 'for', 'from', 'further', 'had', 'hadn',\n",
    "        \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\",\n",
    "        'having', 'here', 'how', 'i', 'if', 'in', 'into', 'is', 'isn',\n",
    "        \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm',\n",
    "        'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn',\n",
    "        \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor',\n",
    "        'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or',\n",
    "        'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "        're', 's', 'same', 'shan', \"shan't\", 'should', \"should've\",\n",
    "        'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than',\n",
    "        'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves',\n",
    "        'then', 'there', 'these', 'they', 'this', 'those', 'through',\n",
    "        'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn',\n",
    "        \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "        'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won',\n",
    "        \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\",\n",
    "        \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'}\n",
    "wnl = WordNetLemmatizer()\n",
    "lemma_stop_words = [wnl.lemmatize(wrd) for wrd in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816d0cf",
   "metadata": {},
   "source": [
    "## Load & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897e5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df's in dict iterable\n",
    "def load_df_dict(size = 'one', typ = None):\n",
    "    \"\"\"\n",
    "    Accepts 'one', 'three', or 'five' as input sizes.\n",
    "    If 'train', 'validate', or 'test' are inserted for type,\n",
    "    only that kind of sample will be loaded.\n",
    "    \"\"\"\n",
    "    df_dict = {}\n",
    "    for sample in os.listdir(filepath + 'samples/'):\n",
    "        if size in sample and (typ is not None or typ in sample):\n",
    "            name = sample.replace('.csv', '')\n",
    "            temp_df = pd.read_csv(filepath + 'samples/' + sample)[['source', ' response_text', ' op_gender']]\n",
    "            temp_df = temp_df.rename(columns = {' response_text': 'response_text', ' op_gender': 'op_gender'}).dropna()\n",
    "            df_dict[name] = temp_df\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59ba93d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['validate_one_million', 'test_one_million', 'train_one_million'])\n"
     ]
    }
   ],
   "source": [
    "df_dict = load_df_dict(size = 'one', typ = 'train')\n",
    "print(df_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3db96f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove links as these will only apply to specific responses\n",
    "def rm_link(text):\n",
    "    return re.sub(r'https?://\\S+', '', text)\n",
    "\n",
    "# remove punctuation\n",
    "def rm_punct(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# create class that lemmatizes tweet tokens\n",
    "# this will be used when creating the term matrix\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.tt = TweetTokenizer(preserve_case=False, reduce_len=True,\n",
    "                                 strip_handles=True, match_phone_numbers=False)\n",
    "    def __call__(self, docs):\n",
    "        return [self.wnl.lemmatize(t) for t in self.tt.tokenize(rm_link(rm_punct(docs))) if t not in stop]\n",
    "\n",
    "# create generic vectorizer fit function\n",
    "def train_vectorizer(text_data, vectorizer=CountVectorizer, tokenizer=LemmaTokenizer(),\n",
    "                     ngram_range_lower = 1, ngram_range_upper = 1, min_df = 1):\n",
    "    \"\"\"\n",
    "    Trains a vectorizer on the provided text data and returns the vectorizer instance,\n",
    "    the document-term matrix, and the feature names.\n",
    "\n",
    "    Parameters:\n",
    "    - text_data: List of text documents to be vectorized.\n",
    "    - vectorizer: Vectorizer class to be used for text vectorization. Defaults to CountVectorizer.\n",
    "    - tokenizer: Tokenizer class to be used for tokenizing the text documents. Defaults to TweetTokenizer.\n",
    "    - ngram_range_lower: What's the minimum length of n-grams we want.\n",
    "    - ngram_range_upper: What's the maximum length of n-grams we want.\n",
    "    - min_df: Minimum data frequency.\n",
    "\n",
    "    Returns:\n",
    "    - instance: The trained vectorizer instance.\n",
    "    - matrix: The document-term matrix resulting from fitting the vectorizer on `text_data`.\n",
    "    - features: An array of feature names generated by the vectorizer.\n",
    "    \"\"\"\n",
    "    # Initialize the vectorizer with specified configurations\n",
    "    instance = vectorizer(\n",
    "        strip_accents=None,  # Do not strip accents\n",
    "        lowercase=True,  # Do not convert characters to lowercase\n",
    "        tokenizer=tokenizer,  # Use the tokenize method of the tokenizer instance\n",
    "        token_pattern=None,  # Since a tokenizer is provided, token_pattern is not used\n",
    "        ngram_range=(ngram_range_lower, ngram_range_upper),  # Consider only single words (1-grams)\n",
    "        min_df=min_df,  # Minimum document frequency for filtering terms\n",
    "#         max_df=max_df,  # Maximum document frequency for filtering terms\n",
    "        max_features=None  # No limit on the number of features\n",
    "    )\n",
    "\n",
    "    # Fit the vectorizer on the provided text data and transform the data into a matrix\n",
    "    vector = instance.fit(text_data)\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1412a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_dict['train_one_million'].response_text\n",
    "y_train = df_dict['train_one_million'].op_gender\n",
    "train_tokenized = X_train.apply(LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "446bf334",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(train_tokenized)\n",
    "train_bow = [dictionary.doc2bow(doc) for doc in train_tokenized]\n",
    "tfidf_model = TfidfModel(train_bow)\n",
    "corpus_tfidf = tfidf_model[train_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38bc39d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus_tfidf,\n",
    "                     num_topics = 2, \n",
    "                     id2word = dictionary,\n",
    "                     passes = 10,\n",
    "                     decay = 0.7,\n",
    "                     offset = 30,\n",
    "                     random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aea999ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_ind</th>\n",
       "      <th>assign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>womankind</th>\n",
       "      <td>1.988433e-06</td>\n",
       "      <td>5.818117e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>75563:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gained</th>\n",
       "      <td>1.143797e-06</td>\n",
       "      <td>4.695396e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>19616:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>costco</th>\n",
       "      <td>1.307972e-06</td>\n",
       "      <td>8.466465e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>72738:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>possibly</th>\n",
       "      <td>1.082884e-06</td>\n",
       "      <td>1.276737e-04</td>\n",
       "      <td>1</td>\n",
       "      <td>5015:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transgression</th>\n",
       "      <td>1.057730e-06</td>\n",
       "      <td>1.311828e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>89484:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edgesocieties</th>\n",
       "      <td>9.974635e-07</td>\n",
       "      <td>5.105277e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>125306:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>definitelyy</th>\n",
       "      <td>1.379649e-06</td>\n",
       "      <td>5.752269e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>145264:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demure</th>\n",
       "      <td>9.994652e-07</td>\n",
       "      <td>4.302541e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>52336:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38799</th>\n",
       "      <td>1.847185e-06</td>\n",
       "      <td>1.059828e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>85737:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artistmusician</th>\n",
       "      <td>1.250159e-06</td>\n",
       "      <td>8.298278e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>164198:0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     topic_0       topic_1  topic_ind     assign\n",
       "womankind       1.988433e-06  5.818117e-07          0    75563:0\n",
       "gained          1.143797e-06  4.695396e-05          1    19616:1\n",
       "costco          1.307972e-06  8.466465e-06          1    72738:1\n",
       "possibly        1.082884e-06  1.276737e-04          1     5015:1\n",
       "transgression   1.057730e-06  1.311828e-06          1    89484:1\n",
       "edgesocieties   9.974635e-07  5.105277e-07          0   125306:0\n",
       "definitelyy     1.379649e-06  5.752269e-07          0   145264:0\n",
       "demure          9.994652e-07  4.302541e-07          0    52336:0\n",
       "38799           1.847185e-06  1.059828e-06          0    85737:0\n",
       "artistmusician  1.250159e-06  8.298278e-07          0   164198:0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_df = pd.DataFrame(lda_model.get_topics(), columns = lda_model.id2word.values(),\n",
    "                      index = [f'topic_{i}' for i in range(lda_model.num_topics)]).transpose()\n",
    "top_df['topic_ind'] = np.where(top_df.topic_0 > top_df.topic_1, 0, 1)\n",
    "top_df['assign'] = ' ' + top_df.reset_index().index.astype(str) + ':' + top_df.topic_ind.astype(str)\n",
    "features = top_df.index\n",
    "assigns = '\\n'.join(top_df['assign'])\n",
    "top_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5685b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create storage path if not exists\n",
    "tt_path = filepath + 'turbotopics/'\n",
    "if os.path.exists(tt_path) == False:\n",
    "    os.mkdir(tt_path)\n",
    "# save corpus to text file\n",
    "corpus_path = tt_path + 'corpus.txt'\n",
    "with open(corpus_path, 'w') as f: f.write('\\n'.join(X_train))\n",
    "# create ngrams file for saved results\n",
    "out_path = tt_path + \"ngrams.txt\"\n",
    "out_file = open(out_path, 'w')\n",
    "out_file.close()\n",
    "# save vocab separated by newlines\n",
    "vocab_path = tt_path + 'vocab.dat'\n",
    "with open(vocab_path, 'w') as f: f.write('\\n'.join(features))\n",
    "# save index:topic document\n",
    "assign_path = tt_path + 'assign.dat'\n",
    "with open(assign_path, 'w') as f: f.write(assigns)\n",
    "# assign output location\n",
    "tt_out_path = tt_path + 'tt_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3082ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading vocabulary from /home/datallah/datallah-jaymefis-gibsonce/turbotopics/vocab.dat\n",
      "writing topic 0\n",
      "computing initial counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"lda_topics.py\", line 106, in <module>\n",
      "    pvalue=opt.pvalue)\n",
      "  File \"lda_topics.py\", line 75, in turbo_topic\n",
      "    cnts = tt.nested_sig_bigrams(iter_gen, update_fun, test, min)\n",
      "  File \"/home/datallah/implicit-gender-bias/unsupervised_notebooks/turbotopics/turbotopics.py\", line 425, in nested_sig_bigrams\n",
      "    for doc in iter_generator(): update_fun(counts, doc)\n",
      "  File \"lda_topics.py\", line 72, in update_fun\n",
      "    update_counts_from_topic(doc[0], doc[1], topic, counts)\n",
      "  File \"lda_topics.py\", line 65, in update_counts_from_topic\n",
      "    counts_obj.update_counts(doc, root_filter=root_filter)\n",
      "  File \"/home/datallah/implicit-gender-bias/unsupervised_notebooks/turbotopics/turbotopics.py\", line 67, in update_counts\n",
      "    if (not root_filter(w)): continue\n",
      "  File \"lda_topics.py\", line 64, in <lambda>\n",
      "    root_filter = lambda w: topicmap.get(w.split()[0], -1)==topic\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'cd turbotopics\\npython2.7 lda_topics.py --corpus=\"$1\" --vocab=\"$2\" --assign=\"$3\" --out=\"$4\" --ntopics=2 --pval=0.001\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3033654/816209868.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-s \"$corpus_path\" \"$vocab_path\" \"$assign_path\" \"$tt_out_path\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cd turbotopics\\npython2.7 lda_topics.py --corpus=\"$1\" --vocab=\"$2\" --assign=\"$3\" --out=\"$4\" --ntopics=2 --pval=0.001\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2404\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2406\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2407\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'cd turbotopics\\npython2.7 lda_topics.py --corpus=\"$1\" --vocab=\"$2\" --assign=\"$3\" --out=\"$4\" --ntopics=2 --pval=0.001\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash -s \"$corpus_path\" \"$vocab_path\" \"$assign_path\" \"$tt_out_path\"\n",
    "cd turbotopics\n",
    "python2.7 lda_topics.py --corpus=\"$1\" --vocab=\"$2\" --assign=\"$3\" --out=\"$4\" --ntopics=2 --pval=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243129d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2env",
   "language": "python",
   "name": "py2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
