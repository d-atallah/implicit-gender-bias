{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/d-atallah/implicit_gender_bias/blob/cluster_da/01_unsupervised_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFsfQGhyH9HC"
   },
   "source": [
    "# Preprocessing\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xfq0J7ylIiZW",
    "outputId": "282a877f-60e5-44ef-fda3-3bb192de0cee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/datallah/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "### text preprocessing dependencies\n",
    "import nltk\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "### sklearn dependencies\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### spaCy dependencies\n",
    "import spacy\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W6mgtjY8JQtF",
    "outputId": "3b2761a1-efcd-40e2-8278-989ae4437953"
   },
   "outputs": [],
   "source": [
    "filepath = '/home/datallah/datallah-jaymefis-gibsonce/'\n",
    "random_state = 42\n",
    "stop = {'a', 'about', 'above', 'after', 'again', 'against', 'ain',\n",
    "        'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "        'as', 'at', 'be', 'because', 'been', 'before', 'being',\n",
    "        'below', 'between', 'both', 'but', 'by', 'can', 'couldn',\n",
    "        \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does',\n",
    "        'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during',\n",
    "        'each', 'few', 'for', 'from', 'further', 'had', 'hadn',\n",
    "        \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\",\n",
    "        'having', 'here', 'how', 'i', 'if', 'in', 'into', 'is', 'isn',\n",
    "        \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm',\n",
    "        'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn',\n",
    "        \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor',\n",
    "        'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or',\n",
    "        'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "        're', 's', 'same', 'shan', \"shan't\", 'should', \"should've\",\n",
    "        'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than',\n",
    "        'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves',\n",
    "        'then', 'there', 'these', 'they', 'this', 'those', 'through',\n",
    "        'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn',\n",
    "        \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "        'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won',\n",
    "        \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\",\n",
    "        \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'}\n",
    "wnl = WordNetLemmatizer()\n",
    "lemma_stop_words = [wnl.lemmatize(wrd) for wrd in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNxuySjqLH5z"
   },
   "source": [
    "## Import Response CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "YbsNRj0eJT67",
    "outputId": "5b96215d-a424-4e20-9260-7a31b6ee4546"
   },
   "outputs": [],
   "source": [
    "# create list of filenames\n",
    "filenames = [filename[:-4] for filename in os.listdir(filepath) if '_responses.csv' in filename]\n",
    "# create sampling/bootstrapping function\n",
    "def bootstrap_sample(df, sample_size, random_state = random_state):\n",
    "    \"\"\"\n",
    "    Accepts a dataframe and sample size to other\n",
    "    reduce the size of a dataset or bootstrap it.\n",
    "    \"\"\"\n",
    "    a = df[df.op_gender == 1].sample(n = int(sample_size/2),\n",
    "                                     replace = True,\n",
    "                                     axis = 0,\n",
    "                                     random_state = 42) \n",
    "    b = df[df.op_gender != 1].sample(n = int(sample_size/2),\n",
    "                                     replace = True,\n",
    "                                     axis = 0,\n",
    "                                     random_state = 42) \n",
    "    return pd.concat([a, b], ignore_index = True, copy = False)\n",
    "\n",
    "# iterate through files and create sampled df\n",
    "def concat_dfs(filepath = filepath, filenames = filenames, sample_size = 500000):\n",
    "    \"\"\"\n",
    "    Takes a list of filenames, samples them according to sample_size, \n",
    "    and concatenate all sampled df's together.\n",
    "    \"\"\"\n",
    "    df_lst = []\n",
    "    for filename in filenames:\n",
    "        temp_df = pd.read_csv(filepath + filename + '.csv')[['response_text', 'op_gender']]\n",
    "        temp_df.dropna(subset = ['response_text', 'op_gender'], inplace = True)\n",
    "        temp_df['op_gender'] = np.where(temp_df.op_gender == 'W', 1, 0)\n",
    "        temp_df = bootstrap_sample(temp_df, sample_size)\n",
    "        temp_df['source'] = filename\n",
    "        df_lst.append(temp_df)\n",
    "    # concat df's into one df\n",
    "    concat_df = pd.concat(df_lst, ignore_index = True, copy = False)\n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train, Validation, Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create splitting function\n",
    "def split_df(df, filepath = filepath, random_state = random_state,\n",
    "            train_size = 0.6, val_size = 0.2, test_size = 0.2):\n",
    "    \"\"\"\n",
    "    Take extracted and split DataFrame and write it to a shared location.\n",
    "    Returns all split DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - df: a DataFrame to be written to the shared location.\n",
    "    - filepath: path to where split data files will reside.\n",
    "    - random_state: a seed integer.\n",
    "    - train_size: size of train set proportional to overall df.\n",
    "    - val_size: size of validation set proportional to overall df.\n",
    "    - test_size: size of test set proportional to overall df.\n",
    "\n",
    "    Returns:\n",
    "    - All of the split df's.\n",
    "    \"\"\"\n",
    "    if train_size + val_size + test_size != 1.0:\n",
    "        raise ValueError(\"Train, test, and validation splits must sum to 1.\")\n",
    "    X = df.loc[:, df.columns != 'op_gender']\n",
    "    y = df['op_gender']\n",
    "    X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
    "        X, y, test_size = test_size, random_state = random_state, \n",
    "        stratify = X['source'])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_temp, y_train_temp, \n",
    "        test_size = val_size/(train_size + val_size),\n",
    "        random_state = random_state)\n",
    "    X_train.to_csv(filepath + 'X_train')\n",
    "    X_test.to_csv(filepath + 'X_test')\n",
    "    X_val.to_csv(filepath + 'X_val')\n",
    "    y_train.to_csv(filepath + 'y_train')\n",
    "    y_test.to_csv(filepath + 'y_test')\n",
    "    y_val.to_csv(filepath + 'y_val')\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Text Preprocessing\n",
    "- Remove stopwords\n",
    "- Lowercase\n",
    "- Remove punctuation\n",
    "- Tokenize (using TweetTokenizer)\n",
    "- Remove links\n",
    "- Lemmatize\n",
    "- If POS tagging, we will also remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as to not confuse the POS tagger, we will need to remove emojis and tags\n",
    "# below function and regex was posted by toshi456 on stackoverflow\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'', text)\n",
    "\n",
    "# similarly we want to remove user tags\n",
    "#  as these will only apply to specific responses\n",
    "def rm_userid(text):\n",
    "    return re.sub(r'@[^\\s]+', '', text)\n",
    "\n",
    "# similarly we want to remove links\n",
    "#  as these will only apply to specific responses\n",
    "def rm_link(text):\n",
    "    return re.sub(r'https?://\\S+', '', text)\n",
    "\n",
    "# create class that lemmatizes tweet tokens\n",
    "# this will be used when creating the term matrix\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.tt = TweetTokenizer(preserve_case=False, reduce_len=True,\n",
    "                                 strip_handles=True, match_phone_numbers=False)\n",
    "    def __call__(self, docs):\n",
    "        return [self.wnl.lemmatize(t) for t in self.tt.tokenize(rm_link(docs))]\n",
    "    \n",
    "# creates a term matrix\n",
    "def train_vectorizer(text_data, vectorizer=CountVectorizer, tokenizer=LemmaTokenizer(),\n",
    "                     ngram_range_lower = 1, ngram_range_upper = 1, min_df = None, max_df = 1):\n",
    "    \"\"\"\n",
    "    Trains a vectorizer on the provided text data and returns the vectorizer instance,\n",
    "    the document-term matrix, and the feature names.\n",
    "\n",
    "    Parameters:\n",
    "    - text_data: List of text documents to be vectorized.\n",
    "    - vectorizer: Vectorizer class to be used for text vectorization. Defaults to CountVectorizer.\n",
    "    - tokenizer: Tokenizer class to be used for tokenizing the text documents. Defaults to TweetTokenizer.\n",
    "    - ngram_range_lower: What's the minimum length of n-grams we want.\n",
    "    - ngram_range_upper: What's the maximum length of n-grams we want.\n",
    "    - min_df: Minimum data frequency.\n",
    "    - max_df: Maximum data frequency.\n",
    "\n",
    "    Returns:\n",
    "    - instance: The trained vectorizer instance.\n",
    "    - matrix: The document-term matrix resulting from fitting the vectorizer on `text_data`.\n",
    "    - features: An array of feature names generated by the vectorizer.\n",
    "    \"\"\"\n",
    "    # Initialize the vectorizer with specified configurations\n",
    "    instance = vectorizer(\n",
    "        strip_accents=None,  # Do not strip accents\n",
    "        lowercase=True,  # Do not convert characters to lowercase\n",
    "        tokenizer=tokenizer,  # Use the tokenize method of the tokenizer instance\n",
    "        token_pattern=None,  # Since a tokenizer is provided, token_pattern is not used\n",
    "        stop_words=list(lemma_stop_words),  # Remove stop_words but keep pronouns\n",
    "        ngram_range=(ngram_range_lower, ngram_range_upper),  # Consider only single words (1-grams)\n",
    "        min_df=min_df,  # Minimum document frequency for filtering terms\n",
    "        max_df=max_df,  # Maximum document frequency for filtering terms\n",
    "        max_features=None  # No limit on the number of features\n",
    "    )\n",
    "\n",
    "    # Fit the vectorizer on the provided text data and transform the data into a matrix\n",
    "    vector = instance.fit(text_data)\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tweet tokenizer\n",
    "tt = TweetTokenizer(preserve_case = False, reduce_len = True,\n",
    "                    strip_handles = True, match_phone_numbers = False)\n",
    "\n",
    "# create POS/dep matrices\n",
    "def pos_matrix(responses):\n",
    "    \"\"\"\n",
    "    Accepts a response series to turn into a POS matrix and a dependency matrix\n",
    "    \"\"\"\n",
    "    response_lst = [' '.join(tt.tokenize(deEmojify(rm_link(rm_userid(doc))))) for doc in responses]\n",
    "    response_lst = list(nlp.pipe(response_lst))\n",
    "    pos_cnt_lst = []\n",
    "    dep_cnt_lst = []\n",
    "    for doc in response_lst:\n",
    "        pos_cnt = Counter(tok.pos_ for tok in doc)\n",
    "        dep_cnt = Counter(tok.dep_ for tok in doc)\n",
    "        pos_cnt_lst.append(pos_cnt)\n",
    "        dep_cnt_lst.append(dep_cnt)\n",
    "    df_pos_cnts = pd.DataFrame(pos_cnt_lst).fillna(0)\n",
    "    df_dep_cnts = pd.DataFrame(dep_cnt_lst).fillna(0)\n",
    "    return df_pos_cnts, df_dep_cnts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df's in dict iterable\n",
    "df_dict = {}\n",
    "for sample in os.listdir(filepath + 'samples/'):\n",
    "    name = sample.replace('.csv', '')\n",
    "    temp_df = pd.read_csv(filepath + 'samples/' + sample)[['source', ' response_text', ' op_gender']]\n",
    "    temp_df = temp_df.rename(columns = {' response_text': 'response_text', ' op_gender': 'op_gender'}).dropna()\n",
    "    df_dict[name] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iterables\n",
    "vectorizers = [CountVectorizer, TfidfVectorizer]\n",
    "ngram_lower_lst = np.arange(1, 3)\n",
    "ngram_upper_lst = np.arange(1, 4)\n",
    "min_df_lst = [1, 2, 3, 4, 5]\n",
    "sample_path = filepath + 'samples/' \n",
    "sample_nms = os.listdir(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over params and fit vectorizers\n",
    "v_dict = {}\n",
    "for d in df_dict.keys():\n",
    "    if 'train' in d:\n",
    "        for v in vectorizers:\n",
    "            if v == CountVectorizer: v_n = 'CntV'\n",
    "            elif v == TfidfVectorizer: v_n = 'Tfidf'\n",
    "            for nl in ngram_lower_lst:\n",
    "                nl_n = f'nl{nl}'\n",
    "                for nu in ngram_upper_lst:\n",
    "                    nu_n = f'nu{nu}'\n",
    "                    for f in min_df_lst:\n",
    "                        f_n = f'f{f}'\n",
    "                        name = d + '_' + v_n + '_' + nl_n + '_' + nu_n + '_' + f_n\n",
    "                        v_dict[name] = train_vectorizer(text_data = df_dict[d]['response_text'],\n",
    "                                                        vectorizer = v, \n",
    "                                                        tokenizer = LemmaTokenizer(),\n",
    "                                                        ngram_range_lower = nl, \n",
    "                                                        ngram_range_upper = nu,\n",
    "                                                        min_df = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train vectorizers on train set\n",
    "tfid_1m = TfidfVectorizer().fit(train_1m.response_text)\n",
    "tfid_3m = TfidfVectorizer().fit(train_3m.response_text)\n",
    "tfid_5m = TfidfVectorizer().fit(train_5m.response_text)\n",
    "cnt_1m  = CountVectorizer().fit(train_1m.response_text)\n",
    "cnt_3m  = CountVectorizer().fit(train_3m.response_text)\n",
    "cnt_5m  = CountVectorizer().fit(train_5m.response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create storage location\n",
    "if os.path.exists(filepath + 'matrices/') == False:\n",
    "    os.mkdir(filepath + 'matrices/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iterator function\n",
    "def v_param_iter(name, text_data, vectorizers, ngram_lower_lst,\n",
    "                 ngram_upper_lst, min_df_lst):\n",
    "    for v in vectorizers:\n",
    "        for nl in ngram_lower_lst:\n",
    "            for nu in ngram_upper_lst:\n",
    "                for f in min_df_lst:\n",
    "                    tfidf_dict[tfidf_1m] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_nm in sample_path:\n",
    "    # load df, rename columns, create filename\n",
    "    df_dict = {}\n",
    "    temp_df = pd.read_csv(sample_path + sample_nm)[['source', ' response_text', ' op_gender']]\n",
    "    temp_df['response_text'] = temp_df[' response_text']\n",
    "    temp_df['op_gender'] = temp_df[' op_gender']\n",
    "    name = sample_nm.replace('.csv', '')\n",
    "    df_dict[name] = temp_df\n",
    "    # fit vectorizer if train set\n",
    "    tfidf_dict = {}\n",
    "    cnt_dict = {}\n",
    "    if 'one' in name and 'train' in name:\n",
    "        for nl in ngram_lower_lst:\n",
    "            for nu in ngram_upper_lst:\n",
    "                for f in min_df_lst:\n",
    "                    tfidf_dict[tfidf_1m] = \n",
    "    \n",
    "    # loop over iterables and create files\n",
    "    for nl in ngram_lower_lst:\n",
    "        for nu in ngram_upper_lst:\n",
    "            for f in min_df_lst:\n",
    "                if 'one' in name:\n",
    "                    instance, matrix, features = train_vectorizer()\n",
    "                elif 'three' in name:\n",
    "                    \n",
    "                elif 'five' in name:\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNidRxLM7XD+bc6y30m1bDg",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
