{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603b1b7e",
   "metadata": {},
   "source": [
    "# BGMM\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6237c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f8329bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/datallah/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "\n",
    "### sklearn dependencies\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import sparse\n",
    "\n",
    "### text preprocessing dependencies\n",
    "import nltk\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "### gensim dependencies\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcabb29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/datallah/datallah-jaymefis-gibsonce/'\n",
    "random_state = 42\n",
    "stop = {'a', 'about', 'above', 'after', 'again', 'against', 'ain',\n",
    "        'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\",\n",
    "        'as', 'at', 'be', 'because', 'been', 'before', 'being',\n",
    "        'below', 'between', 'both', 'but', 'by', 'can', 'couldn',\n",
    "        \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does',\n",
    "        'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during',\n",
    "        'each', 'few', 'for', 'from', 'further', 'had', 'hadn',\n",
    "        \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\",\n",
    "        'having', 'here', 'how', 'i', 'if', 'in', 'into', 'is', 'isn',\n",
    "        \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm',\n",
    "        'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn',\n",
    "        \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor',\n",
    "        'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or',\n",
    "        'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "        're', 's', 'same', 'shan', \"shan't\", 'should', \"should've\",\n",
    "        'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than',\n",
    "        'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves',\n",
    "        'then', 'there', 'these', 'they', 'this', 'those', 'through',\n",
    "        'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn',\n",
    "        \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where',\n",
    "        'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won',\n",
    "        \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\",\n",
    "        \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1929c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'one'\n",
    "train = pd.read_csv(filepath + f'samples/train_{size}_million.csv').rename(\n",
    "    columns = {' response_text': 'response_text', ' op_gender': 'op_gender'}).dropna()\n",
    "val   = pd.read_csv(filepath + f'samples/validate_{size}_million.csv').rename(\n",
    "    columns = {' response_text': 'response_text', ' op_gender': 'op_gender'}).dropna()\n",
    "test  = pd.read_csv(filepath + f'samples/test_{size}_million.csv').rename(\n",
    "    columns = {' response_text': 'response_text', ' op_gender': 'op_gender'}).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42ef8a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[train.source == 'TED'].response_text\n",
    "y_train = train[train.source == 'TED'].op_gender\n",
    "X_val = val[val.source == 'TED'].response_text\n",
    "y_val = val[val.source == 'TED'].op_gender\n",
    "X_test  = test[test.source == 'TED'].response_text\n",
    "y_test  = test[test.source == 'TED'].op_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26a70393",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c933f637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139982,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded0923",
   "metadata": {},
   "source": [
    "## Load TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0394fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize functions\n",
    "# remove links as these will only apply to specific responses\n",
    "def rm_link(text):\n",
    "    return re.sub(r'https?://\\S+', '', text)\n",
    "\n",
    "# remove punctuation\n",
    "def rm_punct(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# create class that lemmatizes tweet tokens\n",
    "# this will be used when creating the term matrix\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.tt = TweetTokenizer(preserve_case=False, reduce_len=True,\n",
    "                                 strip_handles=True, match_phone_numbers=False)\n",
    "    def __call__(self, docs):\n",
    "        return [self.wnl.lemmatize(t) for t in self.tt.tokenize(rm_link(rm_punct(docs))) if t not in stop]\n",
    "# creates a term matrix\n",
    "def train_vectorizer(text_data, vectorizer=CountVectorizer, tokenizer=LemmaTokenizer(),\n",
    "                     ngram_range_lower = 1, ngram_range_upper = 1, min_df = 1):\n",
    "    \"\"\"\n",
    "    Trains a vectorizer on the provided text data and returns the vectorizer instance,\n",
    "    the document-term matrix, and the feature names.\n",
    "\n",
    "    Parameters:\n",
    "    - text_data: List of text documents to be vectorized.\n",
    "    - vectorizer: Vectorizer class to be used for text vectorization. Defaults to CountVectorizer.\n",
    "    - tokenizer: Tokenizer class to be used for tokenizing the text documents. Defaults to TweetTokenizer.\n",
    "    - ngram_range_lower: What's the minimum length of n-grams we want.\n",
    "    - ngram_range_upper: What's the maximum length of n-grams we want.\n",
    "    - min_df: Minimum data frequency.\n",
    "    - max_df: Maximum data frequency.\n",
    "\n",
    "    Returns:\n",
    "    - instance: The trained vectorizer instance.\n",
    "    - matrix: The document-term matrix resulting from fitting the vectorizer on `text_data`.\n",
    "    - features: An array of feature names generated by the vectorizer.\n",
    "    \"\"\"\n",
    "    # Initialize the vectorizer with specified configurations\n",
    "    instance = vectorizer(\n",
    "        strip_accents=None,  # Do not strip accents\n",
    "        lowercase=True,  # Do not convert characters to lowercase\n",
    "        tokenizer=tokenizer,  # Use the tokenize method of the tokenizer instance\n",
    "        token_pattern=None,  # Since a tokenizer is provided, token_pattern is not used\n",
    "        ngram_range=(ngram_range_lower, ngram_range_upper),  # Consider only single words (1-grams)\n",
    "        min_df=min_df,  # Minimum document frequency for filtering terms\n",
    "        max_features=None  # No limit on the number of features\n",
    "    )\n",
    "\n",
    "    # Fit the vectorizer on the provided text data and transform the data into a matrix\n",
    "    vector = instance.fit(text_data)\n",
    "\n",
    "    return vector, instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41919272",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v, instance = train_vectorizer(text_data = X_train,\n",
    "                           vectorizer = TfidfVectorizer,\n",
    "                           ngram_range_lower = 1,\n",
    "                           ngram_range_upper = 3,\n",
    "                           min_df = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55c0a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = '|\\n|'.join(instance.get_feature_names())\n",
    "file = open('/home/datallah/datallah-jaymefis-gibsonce/bgmm/features.txt', \"w\")\n",
    "file.write(features)\n",
    "file.close()  # to change file access modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55074f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming X_train\n",
      "Transforming X_val\n",
      "Transforming X_test\n"
     ]
    }
   ],
   "source": [
    "# normalize and truncate\n",
    "\n",
    "print(\"Transforming X_train\")\n",
    "tfidf_m = tfidf_v.transform(X_train)\n",
    "sparse.save_npz('/home/datallah/datallah-jaymefis-gibsonce/bgmm/tfidf_m.npz', tfidf_m)\n",
    "del X_train\n",
    "# truncate\n",
    "svd = TruncatedSVD(n_components = 100, random_state = random_state)\n",
    "svd.fit(normalize(tfidf_m))\n",
    "tfidf_m = svd.transform(normalize(tfidf_m))\n",
    "np.save('/home/datallah/datallah-jaymefis-gibsonce/bgmm/tfidf_trunc.npy', tfidf_m)\n",
    "\n",
    "print(\"Transforming X_val\")\n",
    "tfidf_m_val = tfidf_v.transform(X_val)\n",
    "del X_val\n",
    "sparse.save_npz('/home/datallah/datallah-jaymefis-gibsonce/bgmm/tfidf_m_val.npz', tfidf_m_val)\n",
    "tfidf_m_val = svd.transform(normalize(tfidf_m_val))\n",
    "np.save('/home/datallah/datallah-jaymefis-gibsonce/bgmm/tfidf_trunc_val.npy', tfidf_m_val)\n",
    "\n",
    "print(\"Transforming X_test\")\n",
    "tfidf_m_test = tfidf_v.transform(X_test)\n",
    "del X_test\n",
    "sparse.save_npz('/home/datallah/datallah-jaymefis-gibsonce/bgmm/tfidf_m_test.npz', tfidf_m_test)\n",
    "tfidf_m_test = svd.transform(normalize(tfidf_m_test))\n",
    "np.save('/home/datallah/datallah-jaymefis-gibsonce/bgmm/tfidf_trunc_test.npy', tfidf_m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "351efa8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139982, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_m.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb6770",
   "metadata": {},
   "source": [
    "## Train Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e703f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:02<01:12,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 1, Log likelihood: 196.5713518830757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 2/30 [01:36<26:15, 56.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 2, Log likelihood: 243.11340103156027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 3/30 [05:31<1:02:02, 137.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 3, Log likelihood: 260.99605489375665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 4/30 [08:36<1:07:47, 156.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 4, Log likelihood: 290.68748356450806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 5/30 [14:30<1:34:57, 227.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 5, Log likelihood: 302.53934624201463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 6/30 [18:57<1:36:22, 240.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 6, Log likelihood: 311.82450073233014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 7/30 [25:00<1:47:41, 280.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 7, Log likelihood: 319.21092295241766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 8/30 [28:35<1:35:18, 259.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 8, Log likelihood: 316.05980845011544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 9/30 [36:06<1:51:53, 319.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 9, Log likelihood: 324.16009071553844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 10/30 [43:32<1:59:36, 358.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 10, Log likelihood: 329.4315077583327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 11/30 [49:25<1:53:00, 356.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 11, Log likelihood: 329.75458349761567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 12/30 [1:01:46<2:22:09, 473.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 12, Log likelihood: 335.2015776342215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 13/30 [1:20:33<3:10:19, 671.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 13, Log likelihood: 338.5252807338983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 14/30 [1:48:10<4:18:27, 969.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 14, Log likelihood: 340.07432639411326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 15/30 [2:07:35<4:17:01, 1028.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 15, Log likelihood: 344.4102559634466\n"
     ]
    }
   ],
   "source": [
    "# init variables\n",
    "best_model = None\n",
    "best_log_likelihood = -np.inf\n",
    "log_lik_lst = []\n",
    "time_start_lst = []\n",
    "time_end_lst = []\n",
    "max_components = np.arange(1, 30 + 1)\n",
    "breaker = 0\n",
    "\n",
    "# iterate over different numbers of components\n",
    "for n_components in tqdm.tqdm(max_components):\n",
    "    time_start_lst.append(datetime.now())\n",
    "    # Initialize and fit the BGMM with current number of components\n",
    "    bgmm = BayesianGaussianMixture(n_components = n_components,\n",
    "                                   random_state = random_state, \n",
    "                                   max_iter = 1000,\n",
    "                                   warm_start = True)\n",
    "    bgmm.fit(tfidf_m)\n",
    "    \n",
    "    # Calculate log likelihood of the current model\n",
    "    log_likelihood = bgmm.score(tfidf_m_val)\n",
    "    log_lik_lst.append(log_likelihood)\n",
    "    print(f\"Number of components: {n_components}, Log likelihood: {log_likelihood}\")\n",
    "    \n",
    "    time_end_lst.append(datetime.now())\n",
    "    # Check if log likelihood is better than the best so far\n",
    "    if log_likelihood > best_log_likelihood:\n",
    "        best_log_likelihood = log_likelihood\n",
    "        best_model = bgmm\n",
    "        breaker = 0\n",
    "    elif breaker < 3:\n",
    "        breaker ++ 1\n",
    "        # If log likelihood starts to dip, stop iterating\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af1a50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_components = np.arange(1, 30 + 1)\n",
    "log_lik_lst = [196.57135188307578, 243.11340103156016, 260.9960548937567, 290.687483564508, 302.5393462420144, 311.82450073233025, 319.2109229524175, 316.05980845011544, 324.16009071553856, 329.4315077583328, 329.7545834976157, 335.2015776342216, 338.5252807338984, 340.07432639411326, 344.4102559634466, 346.4648811376254, 349.14685280030614, 349.6416119071051, 351.64877838142047, 351.8723286693082, 353.37432998827825]\n",
    "times_str = ['00:00:00', '00:00:02', '00:01:36', '00:05:31', '00:08:36', '00:14:30', '00:18:57', '00:25:00', '00:28:35', '00:36:06', '00:43:32', '00:49:25', '01:01:46', '01:20:33', '01:48:10', '02:07:35', '02:32:11', '03:14:35', '03:42:39', '04:29:00', '04:57:43', '05:23:08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06bdd7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 94.0, 235.0, 185.0, 354.0, 267.0, 363.0, 215.0, 451.0, 446.0, 353.0, 741.0, 1127.0, 1657.0, 1165.0, 1476.0, 2544.0, 1684.0, 2781.0, 1723.0, 1525.0]\n"
     ]
    }
   ],
   "source": [
    "times = [datetime.strptime(time_str, '%H:%M:%S') for time_str in times_str]\n",
    "time_diff_lst = [(times[i + 1] - times[i]).total_seconds() for i in range(len(times) - 1)]\n",
    "print(time_diff_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "176b706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = len(log_lik_lst) - 1\n",
    "# create and write sensitivity df\n",
    "sens_df = pd.DataFrame()\n",
    "sens_df['n_componenets'] = max_components[0:i]\n",
    "sens_df['max_log_likelihood'] = log_lik_lst[0:i]\n",
    "sens_df['train_time'] = time_diff_lst[0:i]\n",
    "sens_df\n",
    "# write df\n",
    "sens_df.to_csv('/home/datallah/datallah-jaymefis-gibsonce/bgmm/sensitivity.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728bae1",
   "metadata": {},
   "source": [
    "## Train Base Model (Larger step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b39287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already trained for 1 components.\n",
      "Already trained for 5 components.\n",
      "Already trained for 13 components.\n",
      "Already trained for 3 components.\n",
      "Already trained for 15 components.\n",
      "Already trained for 7 components.\n",
      "Already trained for 21 components.\n",
      "Already trained for 17 components.\n",
      "Already trained for 23 components.\n",
      "Already trained for 25 components.\n",
      "Already trained for 29 components.\n",
      "Already trained for 31 components.\n",
      "Already trained for 33 components.\n",
      "Already trained for 35 components.\n",
      "Already trained for 9 components.\n",
      "Already trained for 11 components.\n",
      "Initialization 0\n",
      "Initialization 0\n",
      "Initialization 0\n",
      "Initialization 0\n",
      "  Iteration 10\n",
      "  Iteration 10\n",
      "  Iteration 20\n",
      "  Iteration 10\n",
      "  Iteration 10\n",
      "  Iteration 30\n",
      "  Iteration 20\n",
      "  Iteration 40\n",
      "  Iteration 20\n",
      "  Iteration 50\n",
      "  Iteration 30\n",
      "  Iteration 20\n",
      "  Iteration 60\n",
      "  Iteration 40\n",
      "  Iteration 30\n",
      "  Iteration 70\n",
      "  Iteration 30\n",
      "  Iteration 50\n",
      "  Iteration 80\n",
      "  Iteration 40\n",
      "  Iteration 40\n",
      "  Iteration 90\n",
      "  Iteration 60\n",
      "  Iteration 50\n",
      "  Iteration 100\n",
      "  Iteration 70\n",
      "  Iteration 50\n",
      "  Iteration 110\n",
      "  Iteration 60\n",
      "  Iteration 120\n",
      "  Iteration 80\n",
      "  Iteration 60\n",
      "  Iteration 130\n",
      "  Iteration 70\n",
      "  Iteration 90\n",
      "  Iteration 140\n",
      "  Iteration 70\n",
      "  Iteration 100\n",
      "  Iteration 150\n",
      "  Iteration 80\n",
      "  Iteration 110\n",
      "  Iteration 160\n",
      "  Iteration 80\n",
      "  Iteration 90\n"
     ]
    }
   ],
   "source": [
    "!python bgmm_multi_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f5605c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
