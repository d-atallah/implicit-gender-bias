{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d202520",
   "metadata": {},
   "source": [
    "# Explore BGMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab66124",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b28e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/datallah/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "### sklearn dependencies\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import sparse\n",
    "\n",
    "### text preprocessing dependencies\n",
    "import nltk\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "### gensim dependencies\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5709923",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/datallah/datallah-jaymefis-gibsonce/'\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "42d5adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 'one'\n",
    "train = pd.read_csv(filepath + f'samples/train_{size}_million.csv').rename(\n",
    "    columns = {' response_text': 'response_text', ' op_gender': 'op_gender'}).dropna()\n",
    "val   = pd.read_csv(filepath + f'samples/validate_{size}_million.csv').rename(\n",
    "    columns = {' response_text': 'response_text', ' op_gender': 'op_gender'}).dropna()\n",
    "test  = pd.read_csv(filepath + f'samples/test_{size}_million.csv').rename(\n",
    "    columns = {' response_text': 'response_text', ' op_gender': 'op_gender'}).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8cf385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[train.source == 'TED'].response_text\n",
    "y_train = train[train.source == 'TED'].op_gender\n",
    "X_val = val[val.source == 'TED'].response_text\n",
    "y_val = val[val.source == 'TED'].op_gender\n",
    "X_test  = test[test.source == 'TED'].response_text\n",
    "y_test  = test[test.source == 'TED'].op_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f04faa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e525360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_m = np.load('/home/datallah/datallah-jaymefis-gibsonce/bgmm/tfidf_trunc.npy')\n",
    "tfidf_m_val = np.load('/home/datallah/datallah-jaymefis-gibsonce/bgmm/tfidf_trunc_val.npy')\n",
    "tfidf_m_test = np.load('/home/datallah/datallah-jaymefis-gibsonce/bgmm/tfidf_trunc_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e71e463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_componenets</th>\n",
       "      <th>max_log_likelihood</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>196.571352</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>243.113401</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>260.996055</td>\n",
       "      <td>235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>290.687484</td>\n",
       "      <td>185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>302.539346</td>\n",
       "      <td>354.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>311.824501</td>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>319.210923</td>\n",
       "      <td>363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>316.059808</td>\n",
       "      <td>215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>324.160091</td>\n",
       "      <td>451.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>329.431508</td>\n",
       "      <td>446.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>329.754583</td>\n",
       "      <td>353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>335.201578</td>\n",
       "      <td>741.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>338.525281</td>\n",
       "      <td>1127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>340.074326</td>\n",
       "      <td>1657.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>344.410256</td>\n",
       "      <td>1165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>346.464881</td>\n",
       "      <td>1476.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>349.146853</td>\n",
       "      <td>2544.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>349.641612</td>\n",
       "      <td>1684.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>351.648778</td>\n",
       "      <td>2781.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>351.872329</td>\n",
       "      <td>1723.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_componenets  max_log_likelihood  train_time\n",
       "0               1          196.571352         2.0\n",
       "1               2          243.113401        94.0\n",
       "2               3          260.996055       235.0\n",
       "3               4          290.687484       185.0\n",
       "4               5          302.539346       354.0\n",
       "5               6          311.824501       267.0\n",
       "6               7          319.210923       363.0\n",
       "7               8          316.059808       215.0\n",
       "8               9          324.160091       451.0\n",
       "9              10          329.431508       446.0\n",
       "10             11          329.754583       353.0\n",
       "11             12          335.201578       741.0\n",
       "12             13          338.525281      1127.0\n",
       "13             14          340.074326      1657.0\n",
       "14             15          344.410256      1165.0\n",
       "15             16          346.464881      1476.0\n",
       "16             17          349.146853      2544.0\n",
       "17             18          349.641612      1684.0\n",
       "18             19          351.648778      2781.0\n",
       "19             20          351.872329      1723.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens_df = pd.read_csv('/home/datallah/datallah-jaymefis-gibsonce/bgmm/sensitivity.csv')\n",
    "sens_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eec7f1b",
   "metadata": {},
   "source": [
    "## Fit Model with Best Base Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9721b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f9d6618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianGaussianMixture(max_iter=1000, n_components=20, random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgmm = BayesianGaussianMixture(n_components = n_components,\n",
    "                               random_state = random_state, \n",
    "                               max_iter = 1000)\n",
    "bgmm.fit(tfidf_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3119b8b1",
   "metadata": {},
   "source": [
    "## Analyze Component Gender Makeup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb794b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20715</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62647</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94194</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109690</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67685</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        preds  label\n",
       "20715       6      1\n",
       "62647      15      0\n",
       "94194      16      1\n",
       "109690     18      0\n",
       "67685      13      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds = bgmm.predict(tfidf_m)\n",
    "train_preds_df = pd.DataFrame({'preds' : train_preds, \n",
    "                               'label' : np.where(y_train == 'W', 0, 1)})\n",
    "train_preds_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc0a5df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component</th>\n",
       "      <th>men_ratio</th>\n",
       "      <th>women_ratio</th>\n",
       "      <th>comp_cnts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.623349</td>\n",
       "      <td>0.376651</td>\n",
       "      <td>5602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.611401</td>\n",
       "      <td>0.388599</td>\n",
       "      <td>14928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.544816</td>\n",
       "      <td>0.455184</td>\n",
       "      <td>7687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.524370</td>\n",
       "      <td>0.475630</td>\n",
       "      <td>13767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.522467</td>\n",
       "      <td>0.477533</td>\n",
       "      <td>3583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.515214</td>\n",
       "      <td>0.484786</td>\n",
       "      <td>6836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.514547</td>\n",
       "      <td>0.485453</td>\n",
       "      <td>6462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.508679</td>\n",
       "      <td>0.491321</td>\n",
       "      <td>5300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.493118</td>\n",
       "      <td>0.506882</td>\n",
       "      <td>4577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.490485</td>\n",
       "      <td>0.509515</td>\n",
       "      <td>2575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.490031</td>\n",
       "      <td>0.509969</td>\n",
       "      <td>2608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.487920</td>\n",
       "      <td>0.512080</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.478936</td>\n",
       "      <td>0.521064</td>\n",
       "      <td>4510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.477757</td>\n",
       "      <td>0.522243</td>\n",
       "      <td>3192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.471334</td>\n",
       "      <td>0.528666</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.460091</td>\n",
       "      <td>0.539909</td>\n",
       "      <td>7066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.456583</td>\n",
       "      <td>0.543417</td>\n",
       "      <td>2856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.440332</td>\n",
       "      <td>0.559668</td>\n",
       "      <td>22424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.409780</td>\n",
       "      <td>0.590220</td>\n",
       "      <td>6135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.388673</td>\n",
       "      <td>0.611327</td>\n",
       "      <td>5403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       component  men_ratio  women_ratio  comp_cnts\n",
       "preds                                              \n",
       "1              1   0.623349     0.376651       5602\n",
       "14            14   0.611401     0.388599      14928\n",
       "16            16   0.544816     0.455184       7687\n",
       "9              9   0.524370     0.475630      13767\n",
       "19            19   0.522467     0.477533       3583\n",
       "13            13   0.515214     0.484786       6836\n",
       "5              5   0.514547     0.485453       6462\n",
       "11            11   0.508679     0.491321       5300\n",
       "18            18   0.493118     0.506882       4577\n",
       "17            17   0.490485     0.509515       2575\n",
       "15            15   0.490031     0.509969       2608\n",
       "8              8   0.487920     0.512080      12500\n",
       "6              6   0.478936     0.521064       4510\n",
       "2              2   0.477757     0.522243       3192\n",
       "7              7   0.471334     0.528666       1971\n",
       "12            12   0.460091     0.539909       7066\n",
       "10            10   0.456583     0.543417       2856\n",
       "0              0   0.440332     0.559668      22424\n",
       "4              4   0.409780     0.590220       6135\n",
       "3              3   0.388673     0.611327       5403"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components = np.sort(train_preds_df.preds.unique())\n",
    "men_ratio = train_preds_df.groupby('preds').apply(lambda x: sum(x.label)/len(x))\n",
    "cnts = train_preds_df.groupby('preds')['label'].count()\n",
    "grpd = pd.DataFrame({'component'   : components,\n",
    "                     'men_ratio'   : men_ratio,\n",
    "                     'women_ratio' : 1 - men_ratio,\n",
    "                     'comp_cnts'   : cnts})\n",
    "grpd.sort_values(by = 'men_ratio', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd4f11d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 3]\n"
     ]
    }
   ],
   "source": [
    "sig_comp_df = grpd[(grpd.men_ratio >= 0.6) | (grpd.women_ratio >= 0.6)].sort_values(by = 'men_ratio', ascending = False)\n",
    "sig_comps = list(sig_comp_df.component.unique())\n",
    "print(sig_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c08b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_probs = bgmm.predict_proba(tfidf_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49333088",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_docs = {}\n",
    "n_top_docs = sig_comp_df.comp_cnts.min()\n",
    "for comp in sig_comps:\n",
    "    # Get the indices of the top n documents for the current component\n",
    "    top_docs_idx = list(np.argsort(posterior_probs[:, comp])[::-1][:n_top_docs])\n",
    "    # Retrieve the top n documents for the current component\n",
    "    top_docs = [X_train.iloc[idx] for idx in top_docs_idx]\n",
    "    sig_docs[comp] = top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3216ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for component 1:\n",
      "1: OH YES!  That is a great perspective and attitude. Promoting the commensalism is a great way to strengthen us. Targeting specific bacteria is wonderful.   I am curious if there is also a a friend receptor to know when symbiotic bacteria are present to work together with other groups of bacteria. That would be like the an organ detecting sugar and releasing chemicals to help other cells process the sugar.  Is any of this communications or virulence blocking valid for Viruses?\n",
      "2: By complete coincidence I started watching this talk by Nathan Myhrvold on Ted.com and as it started an episode of Charlie Rose came on TV with Nathan Myhrvold speaking for an hour....which was by far more interesting and very much worth watching. I believe Myhrvolds point here on this Ted Talk was thats it is ok to follow your varied interests in life. I would suggest that most people are afraid to do so mostly cause of societal pressures.\n",
      "3: Very inspirational talk. Does anyone know the name/composer of that first really fast snare piece that EG plays? Thanks.\n",
      "4: Wow. Thats so cool. A triumph of passion over pragmatism.\n",
      "5: Now know why is who is in and who is out\n",
      "6: OK Go makes fantastic videos. (Just wish they worked more on making fantastic music.)  We should be showing the music video to fifth graders to inspire the next generation of engineers.  Anyway\n",
      "7: I think that a point that Mr. Rutan was trying to make is to not ask the question of who will pick up the torch? (ref William Pounds below) but to declare instead\n",
      "8: This is a very confusing message.   Mr. Gerzema says\n",
      "9: Very nice Mr Stevenson...this is call for society but we are still not so unite I think.All we need is change that comes inside of us.Couple years ago was a movie Life of David Gale which gives a insight what youve been talking about\n",
      "10: Unfortunately\n",
      "\n",
      "\n",
      "Examples for component 14:\n",
      "1: i dont understand why some educated people dont believe in god because of their studies. Im a god loving lutheran christian that has studied Higher level physics and standard level Chemistry in the IB programme and i still dont see any evidence against the existance of god. The bible states that the world is about 6000years old but experiments show that the world is many billion years old. A simple explanation for this is that time is relative as proven in physicsby relativity. Gods 6000years could easily be our billions of years. About evolution\n",
      "2: This man is advocating adherence to a political system of self-interest. It really is lamentable. I bet he is a trillionaire too\n",
      "3: The first question should be what are rating agencies and why do people care about them. The speaker says that investors look at ratings before they make investment decisions\n",
      "4: This is the worst film I have seen in here.   -We do invent new chemicals. In fact. We have added several to the periodic system. -We do make things that are unnatural. Anyone heared about the worry dolls made of skin and hear tissue. All lab made. Is a mice with a human ear growing on its back really a natural creation? Ask your self this question. Have the mice become more human by getting a ear\n",
      "5: Just to make sure I got this right... When the NSA deputy director intentionally alienated half the audience by saying Go Dallas Cowboys he not only implied that he doesnt fundamentally think that the whole issue is more important than a sporting event -- he also was saying f#@k you to us all\n",
      "6: Pat you have argued pretty consistently over this forum. Flooding the comments board doesnt mean you have a stronger argument. What is actually extremely nauseating is your unwillingness to actually state what your political philosophy is ? As I suggested before your emphasis on stats and in the inability to compare etc is irrelevant becasue you have failed to make clear that you are actually saying is that you dont give a toss about poor people. Instead some kind of perverse Individual rights ... freedom mixture which fails to acknowledge reality . Well I am not one to give you ethical or even ethical arguments because I would doubt you would understand them let along attempt to do so . Instead  i thought i would put it in terms you can understand. You may have heard of a gentleman called Karl Marx. Karl Marx suggesed that when the gap between the wealthiest and the poorest got wide enough you would have social revolution. It honestly doesnt matter what you think because if this occurs \n",
      "7: I cried a lot during watching this video. Im a Korean and I had a lot of problems about self-confidence. I couldnt even meet people because I was so afraid. Even I got good grades in school\n",
      "8: A guy comes in a tries to clear the air using common sense\n",
      "9: Did he mention that his book is a best-seller?\n",
      "10: Its about highlighting simplicity with the depth of your gadgets. I reckon he explains it better than most. Just like the books - http://amzn.com/Switching-Mac-Missing-Mountain-Manuals/dp/1449330290/ref=sr_1_4\n",
      "\n",
      "\n",
      "Examples for component 3:\n",
      "1: Loved the talk. The first screen showed the paper bag vs. plastic\n",
      "2: i was moved by the synchronization of your body to your words.\n",
      "3: Well done..... And Thank you\n",
      "4: Where the hell is the rest of the talk??? She just stopped leaving the story unfinished >:(\n",
      "5: shed tears after watching this.  hope is indeed alive especially when acts of kindness abounds.\n",
      "6: Words too preachy or\n",
      "7: Man\n",
      "8: Amazing presentation\n",
      "9: Just beautiful\n",
      "10: Beautiful visuals\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example doc\n",
    "for key in sig_comps:\n",
    "    print(f'Examples for component {str(key)}:')\n",
    "    for i in range(10):\n",
    "        print(f'{str(i + 1)}: {sig_docs[key][i]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed8374",
   "metadata": {},
   "source": [
    "## Find Significant Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34a72884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tf-idf\n",
    "full_train_tfidf = normalize(sparse.load_npz('/home/datallah/datallah-jaymefis-gibsonce/bgmm/tfidf_m.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3cf7350c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '0 1', '000', '000 000', '01']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('/home/datallah/datallah-jaymefis-gibsonce/bgmm/features.txt', \"r\")\n",
    "features = file.read().split('|\\n|')\n",
    "file.close()\n",
    "features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6daaca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_vec = {}\n",
    "n_top_docs = sig_comp_df.comp_cnts.min()\n",
    "for comp in sig_comps:\n",
    "    # Get the indices of the top n documents for the current component\n",
    "    top_docs_idx = list(np.argsort(posterior_probs[:, comp])[::-1][:n_top_docs])\n",
    "    # Retrieve the top n documents for the current component\n",
    "    top_docs = [full_train_tfidf[idx] for idx in top_docs_idx]\n",
    "    sig_vec[comp] = top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d0587485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant terms for component 1:\n",
      "['first', 'ok', 'mr', 'yes', 'know', 'cool', 'he', 'funny', 'wow', 'technology', 'true', 'talk', 'people', 'opinion', 'really', 'like', 'think', 'oh', 'his', 'would', 'unfortunately', 'amazing', 'one', 'see', 'guy', 'dont', 'interesting', 'thing', 'video', 'god', 'love', 'hello', 'thanks', 'u', 'make', 'world', 'time', 'say', 'idea', 'way', 'good', 'nice', 'agree', 'need', 'thought', 'want', 'much', 'work', 'great', 'hi']\n",
      "Significant terms for component 14:\n",
      "['he', 'his', 'god', 'talk', 'im', 'technology', 'actually', 'people', 'video', 'great', 'comment', 'like', 'think', 'dont', 'one', 'say', 'man', 'ted', 'guy', 'know', 'question', 'make', 'u', 'time', 'great talk', 'thing', 'would', 'good', 'point', 'said', 'book', 'get', 'right', 'many', 'sorry', 'world', 'year', 'way', 'yeah', 'problem', 'want', 'need', 'agree', 'life', 'first', 'see', 'watching', 'she', 'could', 'believe']\n",
      "Significant terms for component 3:\n",
      "['beautiful', 'talk', 'amazing', 'true', 'inspiring', 'powerful', 'story', 'she', 'thank', 'watching', 'hey', 'absolutely', 'word', 'great', 'many', 'sharing', 'ted', 'man', 'video', 'speech', 'speaker', 'truly', 'thanks', 'inspirational', 'time', 'amazing talk', 'her', 'best', 'loved', 'watched', 'really', 'ted talk', 'thank sharing', 'absolutely amazing', 'job', 'presentation', 'touching', 'watching video', 'well', 'thanks sharing', 'ive', 'first', 'beautiful talk', 'great story', 'moving', 'great job', 'seen', 'amazing speech', 'ever', 'inspiring speech']\n"
     ]
    }
   ],
   "source": [
    "sig_terms = {}\n",
    "# example doc\n",
    "for key in sig_comps:\n",
    "    print(f'Significant terms for component {str(key)}:')\n",
    "    # find significant terms by average weight\n",
    "    avg_tfidf_scores = sparse.csr_matrix.mean(np.array(sig_vec[key]), axis = 0)\n",
    "    top_term_indx = np.argsort(avg_tfidf_scores.toarray()[0])[::-1]\n",
    "    n_top_terms = 50\n",
    "    top_terms = [features[idx] for idx in top_term_indx[:n_top_terms]]\n",
    "    sig_terms[key] = top_terms\n",
    "    \n",
    "    print(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "130fad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant (non-redundant) terms for component 1:\n",
      "['ok', 'mr', 'yes', 'cool', 'funny', 'wow', 'opinion', 'oh', 'unfortunately', 'interesting', 'love', 'hello', 'idea', 'nice', 'thought', 'much', 'work', 'hi']\n",
      "Significant (non-redundant) terms for component 14:\n",
      "['im', 'actually', 'comment', 'question', 'great talk', 'point', 'said', 'book', 'get', 'right', 'sorry', 'year', 'yeah', 'problem', 'life', 'could', 'believe']\n",
      "Significant (non-redundant) terms for component 3:\n",
      "['beautiful', 'inspiring', 'powerful', 'story', 'thank', 'hey', 'absolutely', 'word', 'sharing', 'speech', 'speaker', 'truly', 'inspirational', 'amazing talk', 'her', 'best', 'loved', 'watched', 'ted talk', 'thank sharing', 'absolutely amazing', 'job', 'presentation', 'touching', 'watching video', 'well', 'thanks sharing', 'ive', 'beautiful talk', 'great story', 'moving', 'great job', 'seen', 'amazing speech', 'ever', 'inspiring speech']\n"
     ]
    }
   ],
   "source": [
    "# remove redundant values to exclude special values\n",
    "top_terms = [term for lst in sig_terms.values() for term in lst]\n",
    "term_counts = Counter(top_terms)\n",
    "exclude_terms = {term for term, count in term_counts.items() if count > 1}\n",
    "for key in sig_comps:\n",
    "    print(f'Significant (non-redundant) terms for component {str(key)}:')\n",
    "    temp_lst = [term for term in sig_terms[key] if term not in exclude_terms]\n",
    "    print(temp_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad8060",
   "metadata": {},
   "source": [
    "## Analyze Component Gender Makeup (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fb340c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16446</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12278</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23243</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       preds  label\n",
       "16446      5      1\n",
       "3314      12      0\n",
       "12278     11      0\n",
       "5166       0      0\n",
       "23243     19      0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = bgmm.predict(tfidf_m_test)\n",
    "test_preds_df = pd.DataFrame({'preds' : test_preds, \n",
    "                              'label' : np.where(y_test == 'W', 0, 1)})\n",
    "test_preds_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b8d0ca63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component</th>\n",
       "      <th>men_ratio</th>\n",
       "      <th>women_ratio</th>\n",
       "      <th>comp_cnts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.615449</td>\n",
       "      <td>0.384551</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.600064</td>\n",
       "      <td>0.399936</td>\n",
       "      <td>3133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.545932</td>\n",
       "      <td>0.454068</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.532365</td>\n",
       "      <td>0.467635</td>\n",
       "      <td>1653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.522601</td>\n",
       "      <td>0.477399</td>\n",
       "      <td>2876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.512857</td>\n",
       "      <td>0.487143</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>1470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.509044</td>\n",
       "      <td>0.490956</td>\n",
       "      <td>1161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.504149</td>\n",
       "      <td>0.495851</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.502582</td>\n",
       "      <td>0.497418</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.501656</td>\n",
       "      <td>0.498344</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.488363</td>\n",
       "      <td>0.511637</td>\n",
       "      <td>2621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.485993</td>\n",
       "      <td>0.514007</td>\n",
       "      <td>1535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.451667</td>\n",
       "      <td>0.548333</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.450102</td>\n",
       "      <td>0.549898</td>\n",
       "      <td>982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.448556</td>\n",
       "      <td>0.551444</td>\n",
       "      <td>4918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.433556</td>\n",
       "      <td>0.566444</td>\n",
       "      <td>1347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.415525</td>\n",
       "      <td>0.584475</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.395073</td>\n",
       "      <td>0.604927</td>\n",
       "      <td>1096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       component  men_ratio  women_ratio  comp_cnts\n",
       "preds                                              \n",
       "1              1   0.615449     0.384551       1191\n",
       "14            14   0.600064     0.399936       3133\n",
       "19            19   0.545932     0.454068        762\n",
       "16            16   0.532365     0.467635       1653\n",
       "9              9   0.522601     0.477399       2876\n",
       "5              5   0.512857     0.487143       1400\n",
       "13            13   0.510204     0.489796       1470\n",
       "11            11   0.509044     0.490956       1161\n",
       "18            18   0.504149     0.495851        964\n",
       "15            15   0.502582     0.497418        581\n",
       "17            17   0.501656     0.498344        604\n",
       "8              8   0.488363     0.511637       2621\n",
       "12            12   0.485993     0.514007       1535\n",
       "2              2   0.457143     0.542857        665\n",
       "10            10   0.451667     0.548333        600\n",
       "6              6   0.450102     0.549898        982\n",
       "0              0   0.448556     0.551444       4918\n",
       "4              4   0.433556     0.566444       1347\n",
       "7              7   0.415525     0.584475        438\n",
       "3              3   0.395073     0.604927       1096"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components = np.sort(test_preds_df.preds.unique())\n",
    "men_ratio = test_preds_df.groupby('preds').apply(lambda x: sum(x.label)/len(x))\n",
    "cnts = test_preds_df.groupby('preds')['label'].count()\n",
    "grpd = pd.DataFrame({'component'   : components,\n",
    "                     'men_ratio'   : men_ratio,\n",
    "                     'women_ratio' : 1 - men_ratio,\n",
    "                     'comp_cnts'   : cnts})\n",
    "grpd.sort_values(by = 'men_ratio', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1fdf5167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 3]\n"
     ]
    }
   ],
   "source": [
    "sig_comp_df = grpd[(grpd.men_ratio >= 0.6) | (grpd.women_ratio >= 0.6)].sort_values(by = 'men_ratio', ascending = False)\n",
    "sig_comps = list(sig_comp_df.component.unique())\n",
    "print(sig_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "31fe9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_probs = bgmm.predict_proba(tfidf_m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7f42ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_docs = {}\n",
    "n_top_docs = sig_comp_df.comp_cnts.min()\n",
    "for comp in sig_comps:\n",
    "    # Get the indices of the top n documents for the current component\n",
    "    top_docs_idx = list(np.argsort(posterior_probs[:, comp])[::-1][:n_top_docs])\n",
    "    # Retrieve the top n documents for the current component\n",
    "    top_docs = [X_test.iloc[idx] for idx in top_docs_idx]\n",
    "    sig_docs[comp] = top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e446132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for component 1:\n",
      "1: ok lets clear things up 1.Hell is not a place of fire and flames.       how do we know this? The bible uses imagery very often because that was the way people spoke.  Mathew 25:30 describes hell as a dark place’ Revelations 20:14 says hell is a place of fire are they contradictory? NO Hebrews 12:29 says God is a consuming fire\n",
      "2: In my opinion\n",
      "3: To share in the joy of that womans first steps in years was fantastic! Yesterday we saw a brief clip of a monkey controlling a robot arm with its mind.It is only a matter of time before these technologies combine to create amazing new devices that will  enrich our lives.\n",
      "4: Yes we need to increase law enforcement in the world to stop the violence.  It has worked out so well here in the U.S.A.  https://www.youtube.com/watch?v=eiq4Cht49o8\n",
      "5: This is wonderful technology that is likely to revolutionize manufacturing. Very soon we may see 3D printers all around the world...... that are likely to be made in China!\n",
      "6: Unfortunately\n",
      "7: Hello! I am a student in the Liberty University Masters of Public Health program. I was interested in your presentation because obesity and dieting is what originally sparked my interest in public health. I thought your talk was unique because you did not just point out the flaws with dieting\n",
      "8: I agree with Guy Stevens. Hey\n",
      "9: I gave up 3/4 of the way through.  Why is there a fascination with time measured by human lives.  The point is that there are organisms that will continue as long as the environmental requirements for its propagation continue regardless of any other factor or measurement.  These organisms seem to get by with very little and they can be sustained in it;s limited echo system.  that would be worth discussion.  But Age is the Wow.... Wow it is old which means what exactly?  I see some kind of reverence paid to something because it is old.  It seems to me that a cosmological view of time would be more relevant in 2010.  Perhaps then some scientists would pull their heads out of their man made global warming contrived hole and look at the greater climate cycles of our planet.  Walter Wegos comment about a frozen 5000 year old tadpole has it.\n",
      "10: Yes - stop using petroleum.     There are ways - <a href=http://www.go-offgrid.com>Go-Offgrid.com</a>\n",
      "\n",
      "\n",
      "Examples for component 14:\n",
      "1: God that was horrible\n",
      "2: This speaker reminds me of what I am discovering in my study of the aging process &#8211; that simplicity and wisdom seem to grow with age. He seems to be saying that an innate need for simplicity go with an ongoing recontextualization and that the process of recontextualizing is itself pretty complex. He also reminds me that &#8220;retirement&#8221; comes from the word &#8220;retreat.&#8221; And\n",
      "3: Probably my favourite Ted Talk. Make sure everything you do\n",
      "4: i dont think violence or non-violence has anything to do with it actually. change is news\n",
      "5: I see some complaints in here that the story is not addressing the importance in the faith of God as recounted in the Bible. While\n",
      "6: I am really sorry that you had to go through what you had to go through. You are very courageous! I admire how you are turning your personal suffering to help others. It is very honorable !  I hope your sincere words will wake up the compassion in the hearts that need it and help them stop wasting their time and energy on Gossip.\n",
      "7: 4:34 says a man can beat his wife. 23:5-6 & 70:29-30 say a man can have sex with his female slaves. 65:4 says a man can consummate marriage with a girl too young to have started her periods.  Ive also heard reports from women that they have been inappropriately fondled by unknown men whilst circling the Kaaba. Frankly I dont give a damn whether or not you can see women in burger king.\n",
      "8: I agree with Browns statement that vulnerability is a part of human life. I noticed how she combined ethos with humor in the beginning of her talk and qualified herself as a social researcher while also keeping her audience interested with humorous stories and comments. She then goes on to explore the irony of human shame and vulnerability\n",
      "9: Shes right about being wrong.  Ive been reading Nobel-laureate Daniel Kahnemans Thinking\n",
      "10: Thank You Jocelyne Bloch!   I have Multiple Sclerosis (MS) with 9-13 lesions. I’m extremely happy to about this for the first time\n",
      "\n",
      "\n",
      "Examples for component 3:\n",
      "1: the best definition of compassion was her compassionate story: the ILIAD  the war between troy and the greeks.  when  hector kills the best friend of achilles\n",
      "2: Very touching and inspiring.\n",
      "3: Thank you for your beautiful and pawerfol talk\n",
      "4: Your speak is really inspirational\n",
      "5: Thank you Susan for this amazing talk!\n",
      "6: A True Genius...\n",
      "7: True\n",
      "8: How confident she is! After listening to this speech\n",
      "9: I loved this talk on so many levels. First and foremost the intro. As someone of Indian origin\n",
      "10: Even though we can all be watching the same movie\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example doc\n",
    "for key in sig_comps:\n",
    "    print(f'Examples for component {str(key)}:')\n",
    "    for i in range(10):\n",
    "        print(f'{str(i + 1)}: {sig_docs[key][i]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef709b99",
   "metadata": {},
   "source": [
    "## Find Significant Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a35ce8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tf-idf\n",
    "full_test_tfidf = normalize(sparse.load_npz('/home/datallah/datallah-jaymefis-gibsonce/bgmm/tfidf_m_test.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9624cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('/home/datallah/datallah-jaymefis-gibsonce/bgmm/features_test.txt', \"r\")\n",
    "# features = file.read().split('|\\n|')\n",
    "# file.close()\n",
    "# features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4b4d9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_vec = {}\n",
    "n_top_docs = sig_comp_df.comp_cnts.min()\n",
    "for comp in sig_comps:\n",
    "    # Get the indices of the top n documents for the current component\n",
    "    top_docs_idx = list(np.argsort(posterior_probs[:, comp])[::-1][:n_top_docs])\n",
    "    # Retrieve the top n documents for the current component\n",
    "    top_docs = [full_test_tfidf[idx] for idx in top_docs_idx]\n",
    "    sig_vec[comp] = top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "25119608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant terms for component 1:\n",
      "['first', 'ok', 'mr', 'opinion', 'cool', 'yes', 'he', 'wow', 'funny', 'know', 'oh', 'unfortunately', 'his', 'people', 'think', 'like', 'would', 'technology', 'talk', 'true', 'god', 'really', 'world', 'time', 'one', 'way', 'thing', 'love', 'see', 'interesting', 'say', 'amazing', 'hello', 'idea', 'make', 'need', 'u', 'video', 'guy', 'dont', 'good', 'want', 'work', 'actually', 'human', 'hey', 'right', 'much', 'something', 'use']\n",
      "Significant terms for component 14:\n",
      "['he', 'his', 'god', 'talk', 'im', 'video', 'dont', 'technology', 'think', 'great', 'actually', 'comment', 'people', 'say', 'like', 'ted', 'one', 'guy', 'u', 'make', 'said', 'time', 'get', 'know', 'great talk', 'thing', 'would', 'question', 'need', 'want', 'sorry', 'agree', 'book', 'good', 'point', 'right', 'year', 'first', 'human', 'man', 'many', 'world', 'see', 'life', 'game', 'much', 'go', 'thank', 'way', 'watching']\n",
      "Significant terms for component 3:\n",
      "['beautiful', 'amazing', 'talk', 'inspiring', 'true', 'story', 'hey', 'she', 'thank', 'word', 'sharing', 'powerful', 'truly', 'ted', 'watching', 'video', 'absolutely', 'man', 'great', 'amazing talk', 'thanks', 'speech', 'time', 'many', 'touching', 'her', 'speaker', 'presentation', 'watched', 'ted talk', 'really', 'loved', 'thank sharing', 'inspirational', 'truly inspiring', 'best', 'thanks sharing', 'ive', 'first', 'job', 'well', 'seen', 'watching video', 'ever', 'heard', 'great ted', 'really inspiring', 'amazing speech', 'inspiring talk', 'brave']\n"
     ]
    }
   ],
   "source": [
    "sig_terms = {}\n",
    "# example doc\n",
    "for key in sig_comps:\n",
    "    print(f'Significant terms for component {str(key)}:')\n",
    "    # find significant terms by average weight\n",
    "    avg_tfidf_scores = sparse.csr_matrix.mean(np.array(sig_vec[key]), axis = 0)\n",
    "    top_term_indx = np.argsort(avg_tfidf_scores.toarray()[0])[::-1]\n",
    "    n_top_terms = 50\n",
    "    top_terms = [features[idx] for idx in top_term_indx[:n_top_terms]]\n",
    "    sig_terms[key] = top_terms\n",
    "    \n",
    "    print(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e15b155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant (non-redundant) terms for component 1:\n",
      "['ok', 'mr', 'opinion', 'cool', 'yes', 'wow', 'funny', 'oh', 'unfortunately', 'love', 'interesting', 'hello', 'idea', 'work', 'something', 'use']\n",
      "Significant (non-redundant) terms for component 14:\n",
      "['im', 'comment', 'said', 'get', 'great talk', 'question', 'sorry', 'agree', 'book', 'point', 'year', 'life', 'game', 'go']\n",
      "Significant (non-redundant) terms for component 3:\n",
      "['beautiful', 'inspiring', 'story', 'she', 'word', 'sharing', 'powerful', 'truly', 'absolutely', 'amazing talk', 'thanks', 'speech', 'touching', 'her', 'speaker', 'presentation', 'watched', 'ted talk', 'loved', 'thank sharing', 'inspirational', 'truly inspiring', 'best', 'thanks sharing', 'ive', 'job', 'well', 'seen', 'watching video', 'ever', 'heard', 'great ted', 'really inspiring', 'amazing speech', 'inspiring talk', 'brave']\n"
     ]
    }
   ],
   "source": [
    "# remove redundant values to exclude special values\n",
    "top_terms = [term for lst in sig_terms.values() for term in lst]\n",
    "term_counts = Counter(top_terms)\n",
    "exclude_terms = {term for term, count in term_counts.items() if count > 1}\n",
    "for key in sig_comps:\n",
    "    print(f'Significant (non-redundant) terms for component {str(key)}:')\n",
    "    temp_lst = [term for term in sig_terms[key] if term not in exclude_terms]\n",
    "    print(temp_lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
