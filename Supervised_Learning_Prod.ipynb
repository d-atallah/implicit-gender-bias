{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-atallah/implicit_gender_bias/blob/main/Supervised_Learning_Prod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHf_jOR9jOca"
      },
      "source": [
        "# Import, Download, & Variable Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6WzZ3_ujTwL",
        "outputId": "5155469d-6a35-4835-d3bf-b982720909af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'implicit_gender_bias' already exists and is not an empty directory.\r\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/gibsonce/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/gibsonce/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/gibsonce/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import & download statements\n",
        "# General Statements\n",
        "!git clone https://github.com/d-atallah/implicit_gender_bias.git\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import joblib\n",
        "from implicit_gender_bias import config as cf\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Feature selection & Model tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD,PCA, NMF\n",
        "from sklearn.metrics import confusion_matrix,precision_score, recall_score, f1_score, accuracy_score, roc_curve, roc_auc_score, log_loss, make_scorer, average_precision_score\n",
        "\n",
        "# Model options\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# NLTK resources\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "porter = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPZ-eni9oS-A",
        "outputId": "7601f13f-a6c7-4ce5-e631-48888d597a14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (1,4,6,7,10,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "# Variables\n",
        "folder_path = '/home/gibsonce/datallah-jaymefis-gibsonce/'\n",
        "\n",
        "# Inputs\n",
        "responses_combined = pd.read_csv(folder_path+'responses_combined.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezs_NSDuTD7n"
      },
      "source": [
        "### Read Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsJ5DrLhTD7o"
      },
      "outputs": [],
      "source": [
        "# Read in the processed response DataFrame from the CSV file\n",
        "all_data = pd.read_csv(folder_path+'all_data.csv')\n",
        "\n",
        "# Separate the data into individual variables\n",
        "responses_combined = all_data[['response_text', 'op_gender_binary', 'source']]\n",
        "X_train = all_data['X_train']\n",
        "y_train = all_data['y_train']\n",
        "X_test = all_data['X_test']\n",
        "y_test = all_data['y_test']\n",
        "#X_train_preprocessed = all_data['X_train_preprocessed']\n",
        "#X_test_preprocessed = all_data['X_test_preprocessed']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zRF7xFVjBKo"
      },
      "source": [
        "## Define Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zLj7yI_jJcQ"
      },
      "outputs": [],
      "source": [
        "# Evaluate a model\n",
        "def model_eval(model, X_test, y_test, y_pred):\n",
        "    \"\"\"\n",
        "    Evaluates a specified model using accuracy, precision, recall, F-1 score, AUC, log-Loss, and a confusion matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained model to be evaluated.\n",
        "    - X_test (list or array): Test set features.\n",
        "    - y_test (list or array): True labels.\n",
        "    - y_pred (list or array): Predicted labels.\n",
        "\n",
        "    Returns:\n",
        "    - metrics_df (pd.DataFrame): DataFrame containing the metrics and scores.\n",
        "    - confusion_df (pd.DataFrame): DataFrame containing a confusion matrix.\n",
        "    \"\"\"\n",
        "    # Initialize dataframes\n",
        "    metrics_df = pd.DataFrame(columns=['Metric', 'Score'])\n",
        "    confusion_df = pd.DataFrame(columns=['Actual Positive', 'Actual Negative', 'Predicted Positive', 'Predicted Negative'])\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    metrics_df = pd.concat([metrics_df, pd.DataFrame({'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "                                                      'Score': [accuracy, precision, recall, f1]})])\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "    metrics_df = pd.concat([metrics_df, pd.DataFrame({'Metric': ['AUC'],\n",
        "                                                      'Score': [auc]})])\n",
        "\n",
        "    logloss = log_loss(y_test, model.predict_proba(X_test))\n",
        "    metrics_df = pd.concat([metrics_df, pd.DataFrame({'Metric': ['Log-Loss'],\n",
        "                                                      'Score': [logloss]})])\n",
        "\n",
        "    # Reset index\n",
        "    metrics_df = metrics_df.reset_index(drop=True)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    confusion_df = pd.DataFrame(cm, columns=['Predicted Positive', 'Predicted Negative'], index=['Actual Positive', 'Actual Negative'])\n",
        "\n",
        "    # Print dataframes\n",
        "    print(\"Metrics:\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_df)\n",
        "\n",
        "    return metrics_df, confusion_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pscLi2HiU1CL"
      },
      "outputs": [],
      "source": [
        "stop_words = {'a',\n",
        " 'about',\n",
        " 'above',\n",
        " 'after',\n",
        " 'again',\n",
        " 'against',\n",
        " 'ain',\n",
        " 'all',\n",
        " 'am',\n",
        " 'an',\n",
        " 'and',\n",
        " 'any',\n",
        " 'are',\n",
        " 'aren',\n",
        " \"aren't\",\n",
        " 'as',\n",
        " 'at',\n",
        " 'be',\n",
        " 'because',\n",
        " 'been',\n",
        " 'before',\n",
        " 'being',\n",
        " 'below',\n",
        " 'between',\n",
        " 'both',\n",
        " 'but',\n",
        " 'by',\n",
        " 'can',\n",
        " 'couldn',\n",
        " \"couldn't\",\n",
        " 'd',\n",
        " 'did',\n",
        " 'didn',\n",
        " \"didn't\",\n",
        " 'do',\n",
        " 'does',\n",
        " 'doesn',\n",
        " \"doesn't\",\n",
        " 'doing',\n",
        " 'don',\n",
        " \"don't\",\n",
        " 'down',\n",
        " 'during',\n",
        " 'each',\n",
        " 'few',\n",
        " 'for',\n",
        " 'from',\n",
        " 'further',\n",
        " 'had',\n",
        " 'hadn',\n",
        " \"hadn't\",\n",
        " 'has',\n",
        " 'hasn',\n",
        " \"hasn't\",\n",
        " 'have',\n",
        " 'haven',\n",
        " \"haven't\",\n",
        " 'having',\n",
        " #'he',\n",
        " #'her',\n",
        " 'here',\n",
        " #'hers',\n",
        " #'herself',\n",
        " #'him',\n",
        " #'himself',\n",
        " #'his',\n",
        " 'how',\n",
        " 'i',\n",
        " 'if',\n",
        " 'in',\n",
        " 'into',\n",
        " 'is',\n",
        " 'isn',\n",
        " \"isn't\",\n",
        " 'it',\n",
        " \"it's\",\n",
        " 'its',\n",
        " 'itself',\n",
        " 'just',\n",
        " 'll',\n",
        " 'm',\n",
        " 'ma',\n",
        " 'me',\n",
        " 'mightn',\n",
        " \"mightn't\",\n",
        " 'more',\n",
        " 'most',\n",
        " 'mustn',\n",
        " \"mustn't\",\n",
        " 'my',\n",
        " 'myself',\n",
        " 'needn',\n",
        " \"needn't\",\n",
        " 'no',\n",
        " 'nor',\n",
        " 'not',\n",
        " 'now',\n",
        " 'o',\n",
        " 'of',\n",
        " 'off',\n",
        " 'on',\n",
        " 'once',\n",
        " 'only',\n",
        " 'or',\n",
        " 'other',\n",
        " 'our',\n",
        " 'ours',\n",
        " 'ourselves',\n",
        " 'out',\n",
        " 'over',\n",
        " 'own',\n",
        " 're',\n",
        " 's',\n",
        " 'same',\n",
        " 'shan',\n",
        " \"shan't\",\n",
        " #'she',\n",
        " #\"she's\",\n",
        " 'should',\n",
        " \"should've\",\n",
        " 'shouldn',\n",
        " \"shouldn't\",\n",
        " 'so',\n",
        " 'some',\n",
        " 'such',\n",
        " 't',\n",
        " 'than',\n",
        " 'that',\n",
        " \"that'll\",\n",
        " 'the',\n",
        " 'their',\n",
        " 'theirs',\n",
        " 'them',\n",
        " 'themselves',\n",
        " 'then',\n",
        " 'there',\n",
        " 'these',\n",
        " 'they',\n",
        " 'this',\n",
        " 'those',\n",
        " 'through',\n",
        " 'to',\n",
        " 'too',\n",
        " 'under',\n",
        " 'until',\n",
        " 'up',\n",
        " 've',\n",
        " 'very',\n",
        " 'was',\n",
        " 'wasn',\n",
        " \"wasn't\",\n",
        " 'we',\n",
        " 'were',\n",
        " 'weren',\n",
        " \"weren't\",\n",
        " 'what',\n",
        " 'when',\n",
        " 'where',\n",
        " 'which',\n",
        " 'while',\n",
        " 'who',\n",
        " 'whom',\n",
        " 'why',\n",
        " 'will',\n",
        " 'with',\n",
        " 'won',\n",
        " \"won't\",\n",
        " 'wouldn',\n",
        " \"wouldn't\",\n",
        " 'y',\n",
        " 'you',\n",
        " \"you'd\",\n",
        " \"you'll\",\n",
        " \"you're\",\n",
        " \"you've\",\n",
        " 'your',\n",
        " 'yours',\n",
        " 'yourself',\n",
        " 'yourselves'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1jh0q60RcvF"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Applies text preprocessing to a given text, including:\n",
        "    - Removing special characters and digits\n",
        "    - Converting to lowercase\n",
        "    - Tokenization and removing stopwords\n",
        "    - Lemmatization and stemming\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): Input text to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "    - processed_text (str): Preprocessed text after applying the specified steps.\n",
        "    \"\"\"\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization and removing stopwords\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    #tokens = [porter.stem(word) for word in tokens]\n",
        "\n",
        "    # Rejoin tokens into a processed text\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ9IeKzIkjZx"
      },
      "outputs": [],
      "source": [
        "def model_rank(model_list, model_str, metric):\n",
        "    \"\"\"\n",
        "    Finds the model with the best score based on a specified metric.\n",
        "\n",
        "    Parameters:\n",
        "    - models_list (list): List of dictionaries, each representing a model's details.\n",
        "    - model_str (list): List of model names corresponding to models_list.\n",
        "    - metric (str): Metric to rank the models by (e.g., 'Accuracy', 'F1-Score').\n",
        "\n",
        "    Returns:\n",
        "    - all_models (pd.DataFrame): DataFrame with metric scores and model names.\n",
        "    - models_by_metric (pd.DataFrame): DataFrame filtered by the specified metric and sorted in descending order.\n",
        "    \"\"\"\n",
        "    all_models = [model_dict['metrics'].assign(Model=model_name) for model_dict, model_name in zip(model_list, model_str)]\n",
        "\n",
        "    # Concatenate the DataFrames in the list\n",
        "    all_models = pd.concat(all_models, ignore_index=True)\n",
        "\n",
        "\n",
        "    # Sort the DataFrame by the specified metric in descending order\n",
        "    models_by_metric = all_models[all_models['Metric'] == metric].sort_values(by='Score', ascending=False)\n",
        "\n",
        "    return all_models, models_by_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFjcFKYITD7q"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess\n",
        "def preprocess_batch(data, batch_index):\n",
        "    processed_data = list(map(preprocess_text, data))\n",
        "    return processed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OZmlCPsrWD6"
      },
      "source": [
        "# Train, Validate, Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hlRU05nrUOB"
      },
      "outputs": [],
      "source": [
        "# All responses combined\n",
        "# Set train-test split variables\n",
        "X = responses_combined['response_text']\n",
        "y = responses_combined['op_gender_binary']\n",
        "\n",
        "# Perform stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=responses_combined['source']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR_VM33mTD7r"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jSHq7s_TD7r",
        "outputId": "17130cff-2746-41b4-d4e0-f22ab5e3f39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1...\n",
            "Batch 1 processed in 71.67 seconds.\n",
            "Total time elapsed: 1.19 minutes.\n",
            "Processing batch 2...\n",
            "Batch 2 processed in 71.56 seconds.\n",
            "Total time elapsed: 2.39 minutes.\n",
            "Processing batch 3...\n",
            "Batch 3 processed in 70.82 seconds.\n",
            "Total time elapsed: 3.57 minutes.\n",
            "Processing batch 4...\n",
            "Batch 4 processed in 71.27 seconds.\n",
            "Total time elapsed: 4.76 minutes.\n",
            "Processing batch 5...\n",
            "Batch 5 processed in 70.83 seconds.\n",
            "Total time elapsed: 5.94 minutes.\n",
            "Processing batch 6...\n",
            "Batch 6 processed in 71.52 seconds.\n",
            "Total time elapsed: 7.13 minutes.\n",
            "Processing batch 7...\n",
            "Batch 7 processed in 71.07 seconds.\n",
            "Total time elapsed: 8.31 minutes.\n",
            "Processing batch 8...\n",
            "Batch 8 processed in 71.50 seconds.\n",
            "Total time elapsed: 9.50 minutes.\n",
            "Processing batch 9...\n",
            "Batch 9 processed in 70.80 seconds.\n",
            "Total time elapsed: 10.68 minutes.\n",
            "Processing batch 10...\n",
            "Batch 10 processed in 70.82 seconds.\n",
            "Total time elapsed: 11.86 minutes.\n",
            "Processing batch 11...\n",
            "Batch 11 processed in 71.35 seconds.\n",
            "Total time elapsed: 13.05 minutes.\n",
            "Processing batch 12...\n",
            "Batch 12 processed in 70.93 seconds.\n",
            "Total time elapsed: 14.24 minutes.\n",
            "Processing batch 13...\n",
            "Batch 13 processed in 71.33 seconds.\n",
            "Total time elapsed: 15.42 minutes.\n",
            "Processing batch 14...\n",
            "Batch 14 processed in 71.02 seconds.\n",
            "Total time elapsed: 16.61 minutes.\n",
            "Processing batch 15...\n",
            "Batch 15 processed in 71.58 seconds.\n",
            "Total time elapsed: 17.80 minutes.\n",
            "Processing batch 16...\n",
            "Batch 16 processed in 70.68 seconds.\n",
            "Total time elapsed: 18.98 minutes.\n",
            "Processing batch 17...\n",
            "Batch 17 processed in 71.46 seconds.\n",
            "Total time elapsed: 20.17 minutes.\n",
            "Processing batch 18...\n",
            "Batch 18 processed in 70.85 seconds.\n",
            "Total time elapsed: 21.35 minutes.\n",
            "Processing batch 19...\n",
            "Batch 19 processed in 71.85 seconds.\n",
            "Total time elapsed: 22.55 minutes.\n",
            "Processing batch 20...\n",
            "Batch 20 processed in 82.02 seconds.\n",
            "Total time elapsed: 23.92 minutes.\n",
            "Processing batch 21...\n",
            "Batch 21 processed in 71.25 seconds.\n",
            "Total time elapsed: 25.10 minutes.\n",
            "Processing batch 22...\n",
            "Batch 22 processed in 70.99 seconds.\n",
            "Total time elapsed: 26.29 minutes.\n",
            "Processing batch 23...\n",
            "Batch 23 processed in 71.43 seconds.\n",
            "Total time elapsed: 27.48 minutes.\n",
            "Processing batch 24...\n",
            "Batch 24 processed in 71.24 seconds.\n",
            "Total time elapsed: 28.66 minutes.\n",
            "Processing batch 25...\n",
            "Batch 25 processed in 71.63 seconds.\n",
            "Total time elapsed: 29.86 minutes.\n",
            "Processing batch 26...\n",
            "Batch 26 processed in 71.51 seconds.\n",
            "Total time elapsed: 31.05 minutes.\n",
            "Processing batch 27...\n",
            "Batch 27 processed in 71.68 seconds.\n",
            "Total time elapsed: 32.24 minutes.\n",
            "Processing batch 28...\n",
            "Batch 28 processed in 71.17 seconds.\n",
            "Total time elapsed: 33.43 minutes.\n",
            "Processing batch 29...\n",
            "Batch 29 processed in 71.41 seconds.\n",
            "Total time elapsed: 34.62 minutes.\n",
            "Processing batch 30...\n",
            "Batch 30 processed in 71.70 seconds.\n",
            "Total time elapsed: 35.82 minutes.\n",
            "Processing batch 31...\n",
            "Batch 31 processed in 71.53 seconds.\n",
            "Total time elapsed: 37.01 minutes.\n",
            "Processing batch 32...\n",
            "Batch 32 processed in 71.41 seconds.\n",
            "Total time elapsed: 38.20 minutes.\n",
            "Processing batch 33...\n",
            "Batch 33 processed in 71.64 seconds.\n",
            "Total time elapsed: 39.39 minutes.\n",
            "Processing batch 34...\n",
            "Batch 34 processed in 71.32 seconds.\n",
            "Total time elapsed: 40.58 minutes.\n",
            "Processing batch 35...\n",
            "Batch 35 processed in 71.71 seconds.\n",
            "Total time elapsed: 41.78 minutes.\n",
            "Processing batch 36...\n",
            "Batch 36 processed in 71.39 seconds.\n",
            "Total time elapsed: 42.97 minutes.\n",
            "Processing batch 37...\n",
            "Batch 37 processed in 71.68 seconds.\n",
            "Total time elapsed: 44.16 minutes.\n",
            "Processing batch 38...\n",
            "Batch 38 processed in 71.72 seconds.\n",
            "Total time elapsed: 45.36 minutes.\n",
            "Processing batch 39...\n",
            "Batch 39 processed in 71.81 seconds.\n",
            "Total time elapsed: 46.55 minutes.\n",
            "Processing batch 40...\n",
            "Batch 40 processed in 71.40 seconds.\n",
            "Total time elapsed: 47.74 minutes.\n",
            "Processing batch 41...\n",
            "Batch 41 processed in 71.77 seconds.\n",
            "Total time elapsed: 48.94 minutes.\n",
            "Processing batch 42...\n",
            "Batch 42 processed in 71.31 seconds.\n",
            "Total time elapsed: 50.13 minutes.\n",
            "Processing batch 43...\n",
            "Batch 43 processed in 27.23 seconds.\n",
            "Total time elapsed: 50.58 minutes.\n"
          ]
        }
      ],
      "source": [
        "# Specify the batch size\n",
        "batch_size = 500000\n",
        "# Create batches\n",
        "X_train_batches = [X_train[i:i + batch_size] for i in range(0, len(X_train), batch_size)]\n",
        "# Preprocess data storage\n",
        "X_train_preprocessed = []\n",
        "# Initialize total time elapsed\n",
        "total_start_time = time.time()\n",
        "\n",
        "for batch_index, batch in enumerate(X_train_batches):\n",
        "    print(f\"Processing batch {batch_index + 1}...\")\n",
        "    # Start time for each batch\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Run preprocessing\n",
        "    X_train_preprocessed.extend(preprocess_batch(batch, batch_index))\n",
        "\n",
        "    # End time for each batch\n",
        "    end_time = time.time()\n",
        "    total_end_time = time.time()\n",
        "\n",
        "    # Calculate elapsed time for the batch\n",
        "    elapsed_time = end_time - start_time\n",
        "    total_elapsed_time = total_end_time - total_start_time\n",
        "    print(f\"Batch {batch_index + 1} processed in {elapsed_time/60:.2f} seconds.\")\n",
        "    print(f\"Total time elapsed: {total_elapsed_time/60:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6asNa6CVTD7r",
        "outputId": "03c3e37f-586d-4055-fc88-627897f2cd06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1...\n",
            "Batch 1 processed in 71.34 seconds.\n",
            "Total time elapsed: 1.19 minutes.\n",
            "Processing batch 2...\n",
            "Batch 2 processed in 71.37 seconds.\n",
            "Total time elapsed: 2.38 minutes.\n",
            "Processing batch 3...\n",
            "Batch 3 processed in 71.38 seconds.\n",
            "Total time elapsed: 3.57 minutes.\n",
            "Processing batch 4...\n",
            "Batch 4 processed in 71.93 seconds.\n",
            "Total time elapsed: 4.77 minutes.\n",
            "Processing batch 5...\n",
            "Batch 5 processed in 71.30 seconds.\n",
            "Total time elapsed: 5.96 minutes.\n",
            "Processing batch 6...\n",
            "Batch 6 processed in 72.16 seconds.\n",
            "Total time elapsed: 7.16 minutes.\n",
            "Processing batch 7...\n",
            "Batch 7 processed in 71.12 seconds.\n",
            "Total time elapsed: 8.34 minutes.\n",
            "Processing batch 8...\n",
            "Batch 8 processed in 71.08 seconds.\n",
            "Total time elapsed: 9.53 minutes.\n",
            "Processing batch 9...\n",
            "Batch 9 processed in 71.76 seconds.\n",
            "Total time elapsed: 10.72 minutes.\n",
            "Processing batch 10...\n",
            "Batch 10 processed in 71.20 seconds.\n",
            "Total time elapsed: 11.91 minutes.\n",
            "Processing batch 11...\n",
            "Batch 11 processed in 42.38 seconds.\n",
            "Total time elapsed: 12.62 minutes.\n"
          ]
        }
      ],
      "source": [
        "# Specify the batch size\n",
        "batch_size = 500000\n",
        "# Create batches\n",
        "X_test_batches = [X_test[i:i + batch_size] for i in range(0, len(X_test), batch_size)]\n",
        "# Preprocess data storage\n",
        "X_test_preprocessed = []\n",
        "# Initialize total time elapsed\n",
        "total_start_time = time.time()\n",
        "\n",
        "for batch_index, batch in enumerate(X_test_batches):\n",
        "    print(f\"Processing batch {batch_index + 1}...\")\n",
        "\n",
        "    # Start time for each batch\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Run preprocessing\n",
        "    X_test_preprocessed.extend(preprocess_batch(batch, batch_index))\n",
        "\n",
        "    # End time for each batch\n",
        "    end_time = time.time()\n",
        "    total_end_time = time.time()\n",
        "\n",
        "    # Calculate elapsed time for the batch\n",
        "    elapsed_time = end_time - start_time\n",
        "    total_elapsed_time = total_end_time - total_start_time\n",
        "    print(f\"Batch {batch_index + 1} processed in {elapsed_time/60:.2f} seconds.\")\n",
        "    print(f\"Total time elapsed: {total_elapsed_time/60:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ezww-io3TD7s"
      },
      "outputs": [],
      "source": [
        "# Convert lists to DataFrames\n",
        "df_X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=['text'])\n",
        "df_X_test_preprocessed = pd.DataFrame(X_test_preprocessed, columns=['text'])\n",
        "df_y_train = pd.DataFrame({'op_gender_binary': y_train})\n",
        "df_y_test = pd.DataFrame({'op_gender_binary': y_test})\n",
        "\n",
        "# Save DataFrames to CSV files\n",
        "df_X_train_preprocessed.to_csv(folder_path + 'X_train_preprocessed.csv', index=False)\n",
        "df_X_test_preprocessed.to_csv(folder_path + 'X_test_preprocessed.csv', index=False)\n",
        "df_y_train.to_csv(folder_path + 'y_train.csv', index=False)\n",
        "df_y_test.to_csv(folder_path + 'y_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TLh5IitPSwQ"
      },
      "source": [
        "Check for class imbalance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOundgQTOvwp",
        "outputId": "6431852b-7612-46b3-f7bd-8415424034f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0    12321744\n",
              "0.0     8868747\n",
              "Name: op_gender_binary, dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_distribution = pd.Series(y_train).value_counts()\n",
        "class_distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE1JVfSWTD7s",
        "outputId": "3be0efbd-f542-4853-d6fa-f5baeccf9a14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21190491"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train_preprocessed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsUhDyhs-EK"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptMJ_W-6TD7s"
      },
      "outputs": [],
      "source": [
        "def model_testing(X_train, y_train, X_test, y_test, model_type, vectorizer, ngram, params):\n",
        "    \"\"\"\n",
        "    Runs a specified model and dimensionality reduction method with tuned hyperparameters\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (array-like): Training set features, preprocessed.\n",
        "    - y_train (array-like): Training set labels.\n",
        "    - X_test (array-like): Test set features, preprocessed.\n",
        "    - y_test (array-like): Test set labels.\n",
        "    - model_type (str): Type of model to test. Choose from 'log' (Logistic Regression), 'xgb' (XGBoost), 'knn' (k-Nearest Neighbors), 'svm' (Support Vector Machine).\n",
        "    - vectorizer (str): Type of vectorizer to test. Choose from 'count' (Count Vecotizer) or 'tfidf' (TF-IDF Vecotizer).\n",
        "    - ngram (int): Feature representation to test. Choose 1 for unigrams, 2 for bigrams, and so on.\n",
        "    - params (dict): Hyperparameter grid for the specified model and dimensionality reduction method.\n",
        "\n",
        "    Returns:\n",
        "    - selected_model: Trained model with the best hyperparameters.\n",
        "    - X_train_ (array-like): Vectorized training set features.\n",
        "    - X_test_ (array-like): Vectorized test set features.\n",
        "    \"\"\"\n",
        "    if vectorizer == 'count':\n",
        "        vect = CountVectorizer(ngram_range=(ngram, ngram))\n",
        "        X_train_ = vect.fit_transform(X_train)\n",
        "        X_test_ = vect.transform(X_test)\n",
        "    elif vectorizer == 'tfidf':\n",
        "        vect = TfidfVectorizer(ngram_range=(ngram, ngram))\n",
        "        X_train_ = vect.fit_transform(X_train)\n",
        "        X_test_ = vect.transform(X_test)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid vector type. Use 'count' or 'tfidf'.\")\n",
        "\n",
        "    if model_type == 'log':\n",
        "        model = LogisticRegression(max_iter=1000, random_state=42, **params)\n",
        "    elif model_type == 'xgb':\n",
        "        model = XGBClassifier(random_state=42, **params)\n",
        "    elif model_type == 'knn':\n",
        "        model = KNeighborsClassifier(**params)\n",
        "    elif model_type == 'svm':\n",
        "        model = SVC(probability=True, **params)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type. Use 'xgb', 'svm', 'knn', or 'log'.\")\n",
        "\n",
        "    # Pipeline with dimensionality reduction method and model to test\n",
        "    pipeline = make_pipeline(\n",
        "        TruncatedSVD(random_state=42),\n",
        "        model\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    pipeline.fit(X_train_, y_train)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_test_pred = pipeline.predict(X_test_)\n",
        "    metrics_val_df, confusion_matrix = model_eval(pipeline, X_test_, y_test, y_test_pred)\n",
        "\n",
        "    return pipeline, X_train_, X_test_, metrics_val_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CheB7IbxTD7t"
      },
      "source": [
        "### XGB Final Model:\n",
        "*   Vectorization: TF-IDF\n",
        "*   Feature Representation: Unigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "a5yEd2jITD7t"
      },
      "outputs": [],
      "source": [
        "# Define variables\n",
        "model = 'xgb'\n",
        "vectorization = 'tfidf'\n",
        "ngram = 1\n",
        "params = {'xgbclassifier__subsample': 0.8, 'xgbclassifier__n_estimators': 150, 'xgbclassifier__max_depth': 9, 'xgbclassifier__learning_rate': 0.05, 'xgbclassifier__colsample_bytree': 0.5, 'truncatedsvd__n_components': 150}\n",
        "\n",
        "\n",
        "# Run model search\n",
        "model, train, test, metrics = model_testing(X_train_preprocessed, y_train, X_test_preprocessed, y_test, model, vectorization, ngram, params)\n",
        "\n",
        "# Save results to dictionary\n",
        "xgb_tfidf_1 = {\n",
        "    'model': model,\n",
        "    'params': params,\n",
        "    'X_train': train,\n",
        "    'X_test': test,\n",
        "    'metrics': metrics\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "322iMKuvi26V"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FLV_drUIqJ4"
      },
      "source": [
        "### Logistic Regression Model Method:\n",
        "*   Vectorization: Count\n",
        "*   Feature Representation: Unigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHFIZlGSIp1d",
        "outputId": "7ef00ef2-8473-4df9-8f76-25748d75d13f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameters: {'logisticregression__solver': 'saga', 'logisticregression__penalty': 'l1', 'logisticregression__C': 0.1}\n",
            "Metrics:\n",
            "      Metric     Score\n",
            "0   Accuracy  0.525630\n",
            "1  Precision  0.521610\n",
            "2     Recall  0.890585\n",
            "3   F1-Score  0.657895\n",
            "4        AUC  0.516530\n",
            "5   Log-Loss  0.692039\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Positive  Predicted Negative\n",
            "Actual Positive                 160                 963\n",
            "Actual Negative                 129                1050\n"
          ]
        }
      ],
      "source": [
        "# Define variables\n",
        "model = 'log'\n",
        "vectorization = 'count'\n",
        "ngram = 1\n",
        "params = {'logisticregression__solver': 'saga', 'logisticregression__penalty': 'l1', 'logisticregression__C': 0.1}\n",
        "\n",
        "# Run model search\n",
        "model, train, test, metrics = model_testing(X_train_preprocessed, y_train, X_test_preprocessed, y_test, model, vectorization, ngram, params)\n",
        "\n",
        "# Save results to dictionary\n",
        "log_count_1 = {\n",
        "    'model': model,\n",
        "    'params': params,\n",
        "    'X_train': train,\n",
        "    'X_test': test,\n",
        "    'metrics': metrics\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whj969mudljU"
      },
      "source": [
        "## Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIoOQC9U3ifv"
      },
      "source": [
        "### Support Vector Machine Model Method:\n",
        "*   Vectorization: Count\n",
        "*   Feature Representation: Unigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrfJfLNO3rKy",
        "outputId": "c5b08ffb-17a4-44e9-c4a1-33331bfdb129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameters: {'svc__kernel': 'rbf', 'svc__gamma': 'scale', 'svc__C': 10}\n",
            "Metrics:\n",
            "      Metric     Score\n",
            "0   Accuracy  0.526499\n",
            "1  Precision  0.524930\n",
            "2     Recall  0.794741\n",
            "3   F1-Score  0.632254\n",
            "4        AUC  0.519811\n",
            "5   Log-Loss  0.692200\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Positive  Predicted Negative\n",
            "Actual Positive                 275                 848\n",
            "Actual Negative                 242                 937\n"
          ]
        }
      ],
      "source": [
        "# Define variables\n",
        "model = 'svm'\n",
        "vectorization = 'count'\n",
        "ngram = 1\n",
        "params = {'svc__kernel': 'rbf', 'svc__gamma': 'scale', 'svc__C': 10}\n",
        "\n",
        "# Run model search\n",
        "model, train, test, metrics = model_testing(X_train_preprocessed, y_train, X_test_preprocessed, y_test, model, vectorization, ngram, params)\n",
        "\n",
        "# Save results to dictionary\n",
        "svm_count_1 = {\n",
        "    'model': model,\n",
        "    'params': params,\n",
        "    'X_train': train,\n",
        "    'X_test': test,\n",
        "    'metrics': metrics\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nihU1Me_eeO1"
      },
      "source": [
        "## K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPiFUZllfQaC"
      },
      "source": [
        "### K-Nearest Neighbors Model Method:\n",
        "*   Vectorization: TF-IDF\n",
        "*   Feature Representation: Bigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_4uzFrwfUkB",
        "outputId": "63838bc6-d200-4dbc-c6b4-f9fd8cc636a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameters: {'kneighborsclassifier__weights': 'distance', 'kneighborsclassifier__p': 1, 'kneighborsclassifier__n_neighbors': 3}\n",
            "Metrics:\n",
            "      Metric     Score\n",
            "0   Accuracy  0.551694\n",
            "1  Precision  0.550034\n",
            "2     Recall  0.685327\n",
            "3   F1-Score  0.610272\n",
            "4        AUC  0.548362\n",
            "5   Log-Loss  7.951702\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Positive  Predicted Negative\n",
            "Actual Positive                 462                 661\n",
            "Actual Negative                 371                 808\n"
          ]
        }
      ],
      "source": [
        "# Define variables\n",
        "model = 'knn'\n",
        "vectorization = 'tfidf'\n",
        "ngram = 2\n",
        "params = {'kneighborsclassifier__weights': 'distance', 'kneighborsclassifier__p': 1, 'kneighborsclassifier__n_neighbors': 3}\n",
        "\n",
        "# Run model search\n",
        "model, train, test, metrics = model_testing(X_train_preprocessed, y_train, X_test_preprocessed, y_test, model, vectorization, ngram, params)\n",
        "\n",
        "# Save results to dictionary\n",
        "knn_tfidf_2 = {\n",
        "    'model': model,\n",
        "    'params': params,\n",
        "    'X_train': train,\n",
        "    'X_test': test,\n",
        "    'metrics': metrics\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bBQ798Ix0Wl"
      },
      "source": [
        "# Model Ranking Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pwqEjyuFklr9",
        "outputId": "42cea569-4499-40a9-dd6f-ee0088da2254"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Score</th>\n",
              "      <th>Model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.571551</td>\n",
              "      <td>xgb_tfidf_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.570937</td>\n",
              "      <td>xgb_count_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.570937</td>\n",
              "      <td>xgb_count_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.549021</td>\n",
              "      <td>xgb_count_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.548362</td>\n",
              "      <td>knn_tfidf_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.545513</td>\n",
              "      <td>knn_count_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.541344</td>\n",
              "      <td>knn_count_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.519811</td>\n",
              "      <td>svm_count_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.519561</td>\n",
              "      <td>knn_tfidf_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.516530</td>\n",
              "      <td>log_count_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.514178</td>\n",
              "      <td>log_tfidf_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.509251</td>\n",
              "      <td>svm_tfidf_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.505934</td>\n",
              "      <td>svm_count_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.504790</td>\n",
              "      <td>svm_tfidf_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.502500</td>\n",
              "      <td>log_count_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>AUC</td>\n",
              "      <td>0.496097</td>\n",
              "      <td>log_tfidf_2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Metric     Score        Model\n",
              "10    AUC  0.571551  xgb_tfidf_1\n",
              "4     AUC  0.570937  xgb_count_1\n",
              "16    AUC  0.570937  xgb_count_1\n",
              "22    AUC  0.549021  xgb_count_2\n",
              "94    AUC  0.548362  knn_tfidf_2\n",
              "76    AUC  0.545513  knn_count_1\n",
              "82    AUC  0.541344  knn_count_2\n",
              "52    AUC  0.519811  svm_count_1\n",
              "88    AUC  0.519561  knn_tfidf_1\n",
              "28    AUC  0.516530  log_count_1\n",
              "40    AUC  0.514178  log_tfidf_1\n",
              "64    AUC  0.509251  svm_tfidf_1\n",
              "58    AUC  0.505934  svm_count_2\n",
              "70    AUC  0.504790  svm_tfidf_2\n",
              "34    AUC  0.502500  log_count_2\n",
              "46    AUC  0.496097  log_tfidf_2"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model Names (Need string values for dataframe column)\n",
        "model_list = [xgb_count_1, xgb_tfidf_1, xgb_count_1, xgb_count_2, log_count_1, log_count_2, log_tfidf_1, log_tfidf_2, svm_count_1, svm_count_2, svm_tfidf_1, svm_tfidf_2, knn_count_1, knn_count_2, knn_tfidf_1, knn_tfidf_2]\n",
        "model_str = ['xgb_count_1', 'xgb_tfidf_1', 'xgb_count_1', 'xgb_count_2', 'log_count_1', 'log_count_2', 'log_tfidf_1', 'log_tfidf_2', 'svm_count_1', 'svm_count_2', 'svm_tfidf_1', 'svm_tfidf_2', 'knn_count_1', 'knn_count_2', 'knn_tfidf_1', 'knn_tfidf_2']\n",
        "\n",
        "# Specify the metric to rank the models by\n",
        "all_models, models_by_metric = model_rank(model_list, model_str, 'AUC')\n",
        "models_by_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JnYQNQaTD7v"
      },
      "outputs": [],
      "source": [
        "all_models.to_csv(folder_path+test_num+'all_models.csv', index=False)\n",
        "models_by_metric.to_csv(folder_path+test_num+'models_by_metric.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attu5aSwi0cS"
      },
      "source": [
        "# Save the vectorizer and associated data\n",
        "joblib.dump(vectorizer_tfidf_bi,folder_path+'tfidf_vectorizer_bi.pkl')\n",
        "joblib.dump(X_train_vtfidf_bi, folder_path+'X_train_vtfidf_bi.pkl')\n",
        "joblib.dump(X_validation_vtfidf_bi, folder_path+'X_validation_vtfidf_bi.pkl')\n",
        "joblib.dump(X_test_vtfidf_bi, folder_path+'X_test_vtfidf_bi.pkl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sdKBq7ETihSJ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}