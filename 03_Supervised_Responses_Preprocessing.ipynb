{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-atallah/implicit_gender_bias/blob/main/03_Supervised_Responses_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHf_jOR9jOca"
      },
      "source": [
        "# Import, Download, & Variable Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6WzZ3_ujTwL",
        "outputId": "5155469d-6a35-4835-d3bf-b982720909af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/gibsonce/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/gibsonce/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/gibsonce/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import & download statements\n",
        "# General Statements\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import joblib\n",
        "from implicit_gender_bias import config as cf\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Feature selection & Model tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold, cross_validate\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD,PCA, NMF\n",
        "from sklearn.metrics import confusion_matrix,precision_score, recall_score, f1_score, accuracy_score, roc_curve, roc_auc_score, log_loss, make_scorer, average_precision_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# NLTK resources\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "porter = PorterStemmer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPZ-eni9oS-A",
        "outputId": "7601f13f-a6c7-4ce5-e631-48888d597a14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (1,4,6,7,10,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "# Variables\n",
        "folder_path = '/home/gibsonce/datallah-jaymefis-gibsonce/'\n",
        "\n",
        "# Inputs\n",
        "responses_combined = pd.read_csv(folder_path+'responses_combined.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zRF7xFVjBKo"
      },
      "source": [
        "## Define Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pscLi2HiU1CL"
      },
      "outputs": [],
      "source": [
        "stop_words = {'a',\n",
        " 'about',\n",
        " 'above',\n",
        " 'after',\n",
        " 'again',\n",
        " 'against',\n",
        " 'ain',\n",
        " 'all',\n",
        " 'am',\n",
        " 'an',\n",
        " 'and',\n",
        " 'any',\n",
        " 'are',\n",
        " 'aren',\n",
        " \"aren't\",\n",
        " 'as',\n",
        " 'at',\n",
        " 'be',\n",
        " 'because',\n",
        " 'been',\n",
        " 'before',\n",
        " 'being',\n",
        " 'below',\n",
        " 'between',\n",
        " 'both',\n",
        " 'but',\n",
        " 'by',\n",
        " 'can',\n",
        " 'couldn',\n",
        " \"couldn't\",\n",
        " 'd',\n",
        " 'did',\n",
        " 'didn',\n",
        " \"didn't\",\n",
        " 'do',\n",
        " 'does',\n",
        " 'doesn',\n",
        " \"doesn't\",\n",
        " 'doing',\n",
        " 'don',\n",
        " \"don't\",\n",
        " 'down',\n",
        " 'during',\n",
        " 'each',\n",
        " 'few',\n",
        " 'for',\n",
        " 'from',\n",
        " 'further',\n",
        " 'had',\n",
        " 'hadn',\n",
        " \"hadn't\",\n",
        " 'has',\n",
        " 'hasn',\n",
        " \"hasn't\",\n",
        " 'have',\n",
        " 'haven',\n",
        " \"haven't\",\n",
        " 'having',\n",
        " #'he',\n",
        " #'her',\n",
        " 'here',\n",
        " #'hers',\n",
        " #'herself',\n",
        " #'him',\n",
        " #'himself',\n",
        " #'his',\n",
        " 'how',\n",
        " 'i',\n",
        " 'if',\n",
        " 'in',\n",
        " 'into',\n",
        " 'is',\n",
        " 'isn',\n",
        " \"isn't\",\n",
        " 'it',\n",
        " \"it's\",\n",
        " 'its',\n",
        " 'itself',\n",
        " 'just',\n",
        " 'll',\n",
        " 'm',\n",
        " 'ma',\n",
        " 'me',\n",
        " 'mightn',\n",
        " \"mightn't\",\n",
        " 'more',\n",
        " 'most',\n",
        " 'mustn',\n",
        " \"mustn't\",\n",
        " 'my',\n",
        " 'myself',\n",
        " 'needn',\n",
        " \"needn't\",\n",
        " 'no',\n",
        " 'nor',\n",
        " 'not',\n",
        " 'now',\n",
        " 'o',\n",
        " 'of',\n",
        " 'off',\n",
        " 'on',\n",
        " 'once',\n",
        " 'only',\n",
        " 'or',\n",
        " 'other',\n",
        " 'our',\n",
        " 'ours',\n",
        " 'ourselves',\n",
        " 'out',\n",
        " 'over',\n",
        " 'own',\n",
        " 're',\n",
        " 's',\n",
        " 'same',\n",
        " 'shan',\n",
        " \"shan't\",\n",
        " #'she',\n",
        " #\"she's\",\n",
        " 'should',\n",
        " \"should've\",\n",
        " 'shouldn',\n",
        " \"shouldn't\",\n",
        " 'so',\n",
        " 'some',\n",
        " 'such',\n",
        " 't',\n",
        " 'than',\n",
        " 'that',\n",
        " \"that'll\",\n",
        " 'the',\n",
        " 'their',\n",
        " 'theirs',\n",
        " 'them',\n",
        " 'themselves',\n",
        " 'then',\n",
        " 'there',\n",
        " 'these',\n",
        " 'they',\n",
        " 'this',\n",
        " 'those',\n",
        " 'through',\n",
        " 'to',\n",
        " 'too',\n",
        " 'under',\n",
        " 'until',\n",
        " 'up',\n",
        " 've',\n",
        " 'very',\n",
        " 'was',\n",
        " 'wasn',\n",
        " \"wasn't\",\n",
        " 'we',\n",
        " 'were',\n",
        " 'weren',\n",
        " \"weren't\",\n",
        " 'what',\n",
        " 'when',\n",
        " 'where',\n",
        " 'which',\n",
        " 'while',\n",
        " 'who',\n",
        " 'whom',\n",
        " 'why',\n",
        " 'will',\n",
        " 'with',\n",
        " 'won',\n",
        " \"won't\",\n",
        " 'wouldn',\n",
        " \"wouldn't\",\n",
        " 'y',\n",
        " 'you',\n",
        " \"you'd\",\n",
        " \"you'll\",\n",
        " \"you're\",\n",
        " \"you've\",\n",
        " 'your',\n",
        " 'yours',\n",
        " 'yourself',\n",
        " 'yourselves'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1jh0q60RcvF"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Applies text preprocessing to a given text, including:\n",
        "    - Removing special characters and digits\n",
        "    - Converting to lowercase\n",
        "    - Tokenization and removing stopwords\n",
        "    - Lemmatization\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): Input text to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "    - processed_text (str): Preprocessed text after applying the specified steps.\n",
        "    \"\"\"\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization and removing stopwords\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Rejoin tokens into a processed text\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OZmlCPsrWD6"
      },
      "source": [
        "# Train, Validate, Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkPBkYtXB66h"
      },
      "outputs": [],
      "source": [
        "# Identify the data source with the minimum number of rows\n",
        "min_source = responses_combined['source'].value_counts().idxmin()\n",
        "\n",
        "# Determine the minimum number of rows for any data source\n",
        "min_rows = responses_combined['source'].value_counts().min()\n",
        "\n",
        "# Create a new DataFrame to store the balanced data\n",
        "balanced_data = pd.DataFrame()\n",
        "\n",
        "# Iterate through each unique data source\n",
        "for source in responses_combined['source'].unique():\n",
        "    # If the current data source has more rows than the minimum, randomly sample to match the minimum\n",
        "    if source != min_source:\n",
        "        subset = responses_combined[responses_combined['source'] == source].sample(n=min_rows, replace=True)\n",
        "    else:\n",
        "        # If the current data source has fewer rows than the minimum, use all rows\n",
        "        subset = responses_combined[responses_combined['source'] == source]\n",
        "\n",
        "    # Append the subset to the balanced_data DataFrame\n",
        "    balanced_data = balanced_data.append(subset)\n",
        "\n",
        "# Shuffle the balanced_data DataFrame to ensure randomness\n",
        "balanced_data = balanced_data.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJru8yo5B66i"
      },
      "outputs": [],
      "source": [
        "# Sample data\n",
        "# Set train-test split variables\n",
        "X = balanced_data['response_text']\n",
        "y = balanced_data['op_gender_binary']\n",
        "\n",
        "# Perform stratified train-test split for the training data\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=balanced_data['source']\n",
        ")\n",
        "\n",
        "# Perform stratified sampling on the training data\n",
        "sample_size = int(0.4 * len(X_train_full))\n",
        "X_train, _, y_train, _ = train_test_split(\n",
        "    X_train_full, y_train_full, train_size=sample_size, random_state=42, stratify=y_train_full\n",
        ")\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority_class = 1.0\n",
        "minority_class = 0.0\n",
        "\n",
        "majority_data = X_train[y_train == majority_class]\n",
        "minority_data = X_train[y_train == minority_class]\n",
        "\n",
        "# Upsample the minority class\n",
        "minority_upsampled = resample(minority_data, replace=True, n_samples=len(majority_data), random_state=42)\n",
        "\n",
        "# Combine the upsampled minority class with the majority class\n",
        "X_train_balanced = pd.concat([majority_data, minority_upsampled])\n",
        "y_train_balanced = np.concatenate([np.full(len(majority_data), majority_class), np.full(len(minority_upsampled), minority_class)])\n",
        "\n",
        "# Convert y_train_balanced to a pandas Series\n",
        "y_train_balanced_series = pd.Series(y_train_balanced, index=X_train_balanced.index)\n",
        "\n",
        "# Shuffle the balanced data\n",
        "shuffle_indices = np.arange(len(X_train_balanced))\n",
        "np.random.shuffle(shuffle_indices)\n",
        "\n",
        "X_train = X_train_balanced.iloc[shuffle_indices]\n",
        "y_train = y_train_balanced_series.iloc[shuffle_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb46LJIsB66i",
        "outputId": "76aa12fd-b5c3-431f-9f30-f9b34289f057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0    193356\n",
            "1.0    193356\n",
            "dtype: int64 386712\n"
          ]
        }
      ],
      "source": [
        "print(y_train.value_counts(),len(X_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX3Ey5eXB66j"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwkSOBLFB66j"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing to each set (X_train, X_validation, X_test)\n",
        "X_train_preprocessed = X_train.apply(preprocess_text)\n",
        "X_test_preprocessed = X_test.apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAH8PNK6B66j"
      },
      "source": [
        "## Write Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhgL9RYRB66k"
      },
      "outputs": [],
      "source": [
        "X_train_preprocessed.to_pickle(folder_path+'X_train_preprocessed.pkl')\n",
        "X_test_preprocessed.to_pickle(folder_path+'X_test_preprocessed.pkl')\n",
        "y_train.to_pickle(folder_path+'y_train.pkl')\n",
        "y_test.to_pickle(folder_path+'y_test.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llb1Ava-B66k"
      },
      "source": [
        "## Unused code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilrdxQJsB66k"
      },
      "source": [
        "X = balanced_data['response_text']\n",
        "y = balanced_data['op_gender_binary']\n",
        "\n",
        "# Set train-test split variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=responses_combined['source']\n",
        ")\n",
        "\n",
        "# Separate majority and minority classes (if needed)\n",
        "majority_class = 1.0\n",
        "minority_class = 0.0\n",
        "\n",
        "majority_data = X_train[y_train == majority_class]\n",
        "minority_data = X_train[y_train == minority_class]\n",
        "\n",
        "# Upsample the minority class (if needed)\n",
        "minority_upsampled = resample(minority_data, replace=True, n_samples=len(majority_data), random_state=42)\n",
        "\n",
        "# Combine the upsampled minority class with the majority class (if needed)\n",
        "X_train_balanced = pd.concat([majority_data, minority_upsampled])\n",
        "y_train_balanced = np.concatenate([np.full(len(majority_data), majority_class), np.full(len(minority_upsampled), minority_class)])\n",
        "\n",
        "# Convert y_train_balanced to a pandas Series (if needed)\n",
        "y_train_balanced_series = pd.Series(y_train_balanced, index=X_train_balanced.index)\n",
        "\n",
        "# Shuffle the balanced data (if needed)\n",
        "shuffle_indices = np.arange(len(X_train_balanced))\n",
        "np.random.shuffle(shuffle_indices)\n",
        "\n",
        "X_train = X_train_balanced.iloc[shuffle_indices]\n",
        "y_train = y_train_balanced_series.iloc[shuffle_indices]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sdKBq7ETihSJ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}