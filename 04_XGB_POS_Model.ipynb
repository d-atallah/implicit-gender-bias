{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-atallah/implicit_gender_bias/blob/main/04_XGB_POS_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHf_jOR9jOca"
      },
      "source": [
        "# Import, Download, & Variable Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6WzZ3_ujTwL",
        "outputId": "172843ab-7d0e-42d7-dce9-ee13902a825b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import & download statements\n",
        "# General Statements\n",
        "#!git clone https://github.com/d-atallah/implicit_gender_bias.git\n",
        "#! pip install joblib\n",
        "#! pip install shap\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import joblib\n",
        "#from implicit_gender_bias import config as cf\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import spacy\n",
        "import scipy\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "#import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Feature selection & Model tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold, cross_validate\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD,PCA, NMF\n",
        "from sklearn.metrics import confusion_matrix,precision_score, recall_score, f1_score, accuracy_score, roc_curve, roc_auc_score, log_loss, make_scorer, average_precision_score\n",
        "\n",
        "# Model options\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# NLTK resources\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "porter = PorterStemmer()\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H15EoL6AdRRY",
        "outputId": "d5096ce9-fa80-443d-8f43-1bb08dadb834"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FJueqg02GhL"
      },
      "source": [
        "## Read Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pPZ-eni9oS-A"
      },
      "outputs": [],
      "source": [
        "# Variables\n",
        "folder_path = '/content/drive/MyDrive/Supervised Learning Notebooks/'#'/home/gibsonce/datallah-jaymefis-gibsonce/'\n",
        "\n",
        "# Load DataFrames from pkl files\n",
        "X_train = pd.read_pickle(folder_path + 'X_train_preprocessed.pkl')\n",
        "X_test = pd.read_pickle(folder_path + 'X_test_preprocessed.pkl')\n",
        "y_train = pd.read_pickle(folder_path + 'y_train.pkl')\n",
        "y_test = pd.read_pickle(folder_path + 'y_test.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2fsNgrmU2GhM"
      },
      "outputs": [],
      "source": [
        "non_nan_indices_train = ~X_train.isnull()\n",
        "non_nan_indices_test = ~X_test.isnull()\n",
        "\n",
        "# Filter y_train and y_test using the non-NaN indices\n",
        "y_train = y_train[non_nan_indices_train]\n",
        "y_test = y_test[non_nan_indices_test]\n",
        "\n",
        "# Filter X_train and X_test to remove NaN records\n",
        "X_train = X_train[non_nan_indices_train]\n",
        "X_test = X_test[non_nan_indices_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zRF7xFVjBKo"
      },
      "source": [
        "## Define Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_categorize(text):\n",
        "    # Tokenize and process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "    word_features = ' '.join([token.text for token in doc])\n",
        "    pos_tags = ' '.join([token.pos_ for token in doc])\n",
        "\n",
        "    return word_features, pos_tags"
      ],
      "metadata": {
        "id": "hH4eqRosn932"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def model_testing(X_train, y_train, X_test, y_test, params, model_type = 'XGB'):\n",
        "    \"\"\"\n",
        "    Runs a specified model and dimensionality reduction method with tuned hyperparameters\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (array-like): Training set features, preprocessed.\n",
        "    - y_train (array-like): Training set labels.\n",
        "    - X_test (array-like): Test set features, preprocessed.\n",
        "    - y_test (array-like): Test set labels.\n",
        "    - params (dict): Hyperparameter grid for the specified model and dimensionality reduction method.\n",
        "\n",
        "    Returns:\n",
        "    - Pipeline: Trained and fit pipeline with the best hyperparameters.\n",
        "    - X_train_combined (array-like): Preprocessed  and vectorized training set features with POS tagging.\n",
        "    - X_test_combined (array-like): Preprocessed  and vectorized test set features with POS tagging.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    X_train_word_features, X_train_pos_tags = zip(*map(tokenize_and_categorize, X_train))\n",
        "    X_test_word_features, X_test_pos_tags = zip(*map(tokenize_and_categorize, X_test))\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"POS tagging completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "    # Vectorize the word features\n",
        "    word_features_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "    X_train_word_features_ = word_features_vectorizer.fit_transform(X_train_word_features)\n",
        "    X_test_word_features_ = word_features_vectorizer.transform(X_test_word_features)\n",
        "\n",
        "    # Vectorize the parts of speech tags\n",
        "    pos_tags_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "    X_train_pos_tags_ = pos_tags_vectorizer.fit_transform(X_train_pos_tags)\n",
        "    X_test_pos_tags_ = pos_tags_vectorizer.transform(X_test_pos_tags)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Vectorization completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "    # Combine the vectorized word features and parts of speech tags\n",
        "    X_train_combined = scipy.sparse.hstack([X_train_word_features_, X_train_pos_tags_])\n",
        "    X_test_combined = scipy.sparse.hstack([X_test_word_features_, X_test_pos_tags_])\n",
        "\n",
        "    model = XGBClassifier(random_state=42, **params.get('xgbclassifier', {}))\n",
        "    model.fit(X_train_combined, y_train)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Pipeline fitting completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "    explainer = None#shap.Explainer(model)\n",
        "    feature_importances = model.feature_importances_\n",
        "    feature_names = word_features_vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Clone the original vectorizer and fit it to the misclassified samples\n",
        "    misclassified_indices = np.where(y_test != model.predict(X_test_combined))[0]\n",
        "    misclassified_samples = X_test_combined[misclassified_indices]\n",
        "    misclassified_features = word_features_vectorizer.inverse_transform(misclassified_samples)\n",
        "\n",
        "    # Combine misclassified feature names into a bag of words\n",
        "    misclassified_bow = [' '.join(features) for features in misclassified_features]\n",
        "\n",
        "    # Create feature importances\n",
        "    feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Feature analysis completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "    # Save the trained pipeline\n",
        "    joblib.dump(model, f'{folder_path}{model_type}_pipeline.pkl')\n",
        "    joblib.dump(X_train_combined, f'{folder_path}{model_type}_X_train.pkl')\n",
        "    joblib.dump(X_test_combined, f'{folder_path}{model_type}_X_test.pkl')\n",
        "    joblib.dump(feature_importance_dict, f'{folder_path}{model_type}_features.pkl')\n",
        "    joblib.dump(misclassified_bow, f'{folder_path}{model_type}_misclassified_bow.pkl')\n",
        "\n",
        "\n",
        "    if explainer:\n",
        "      joblib.dump(explainer, f'{folder_path}{model_type}_shap.pkl')\n",
        "\n",
        "    print('Write to pkl file completed.')\n",
        "\n",
        "    return model, X_train_combined, X_test_combined, explainer, feature_importance_dict, misclassified_bow\n"
      ],
      "metadata": {
        "id": "-jXkmBj8u9b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def model_testing(X_train, y_train, X_test, y_test, params, model_type = 'XGB'):\n",
        "    \"\"\"\n",
        "    Runs a specified model and dimensionality reduction method with tuned hyperparameters\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (array-like): Training set features, preprocessed.\n",
        "    - y_train (array-like): Training set labels.\n",
        "    - X_test (array-like): Test set features, preprocessed.\n",
        "    - y_test (array-like): Test set labels.\n",
        "    - params (dict): Hyperparameter grid for the specified model and dimensionality reduction method.\n",
        "\n",
        "    Returns:\n",
        "    - Pipeline: Trained and fit pipeline with the best hyperparameters.\n",
        "    - X_train_combined (array-like): Preprocessed  and vectorized training set features with POS tagging.\n",
        "    - X_test_combined (array-like): Preprocessed  and vectorized test set features with POS tagging.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    X_train_word_features, X_train_pos_tags = zip(*map(tokenize_and_categorize, X_train))\n",
        "    X_test_word_features, X_test_pos_tags = zip(*map(tokenize_and_categorize, X_test))\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"POS tagging completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "    # Vectorize the word features\n",
        "    word_features_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "    X_train_word_features_ = word_features_vectorizer.fit_transform(X_train_word_features)\n",
        "    X_test_word_features_ = word_features_vectorizer.transform(X_test_word_features)\n",
        "\n",
        "    # Vectorize the parts of speech tags\n",
        "    pos_tags_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "    X_train_pos_tags_ = pos_tags_vectorizer.fit_transform(X_train_pos_tags)\n",
        "    X_test_pos_tags_ = pos_tags_vectorizer.transform(X_test_pos_tags)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Vectorization completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "    # Combine the vectorized word features and parts of speech tags\n",
        "    X_train_combined = scipy.sparse.hstack([X_train_word_features_, X_train_pos_tags_])\n",
        "    X_test_combined = scipy.sparse.hstack([X_test_word_features_, X_test_pos_tags_])\n",
        "\n",
        "    model = XGBClassifier(random_state=42, **params.get('xgbclassifier', {}))\n",
        "    model.fit(X_train_combined, y_train)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Pipeline fitting completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "    explainer = None#shap.Explainer(model)\n",
        "\n",
        "    # Create feature importances\n",
        "    feature_importances = model.feature_importances_\n",
        "    word_features = (word_features_vectorizer.get_feature_names_out())\n",
        "    word_importance_dict = dict(zip(word_features, feature_importances))\n",
        "\n",
        "\n",
        "    # Clone the original vectorizer and fit it to the misclassified samples\n",
        "    misclassified_indices = np.where(y_test != model.predict(X_test_combined))[0]\n",
        "    misclassified_samples = X_test_combined[misclassified_indices]\n",
        "    misclassified_features = (\n",
        "        word_features_vectorizer.inverse_transform(misclassified_samples[:, :X_test_word_features_.shape[1]])\n",
        "        + pos_tags_vectorizer.inverse_transform(misclassified_samples[:, X_test_word_features_.shape[1]:])\n",
        "    )\n",
        "\n",
        "    # Combine misclassified feature names into a bag of words\n",
        "    misclassified_bow = [' '.join(features) for features in misclassified_features]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Feature analysis completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "    # Save the trained pipeline\n",
        "    joblib.dump(model, f'{folder_path}{model_type}_pipeline.pkl')\n",
        "    joblib.dump(X_train_combined, f'{folder_path}{model_type}_X_train.pkl')\n",
        "    joblib.dump(X_test_combined, f'{folder_path}{model_type}_X_test.pkl')\n",
        "    #joblib.dump(feature_importance_dict, f'{folder_path}{model_type}_features.pkl')\n",
        "    joblib.dump(misclassified_bow, f'{folder_path}{model_type}_misclassified_bow.pkl')\n",
        "\n",
        "\n",
        "    if explainer:\n",
        "      joblib.dump(explainer, f'{folder_path}{model_type}_shap.pkl')\n",
        "\n",
        "    print('Write to pkl file completed.')\n",
        "\n",
        "    return model, X_train_combined, X_test_combined, explainer, word_importance_dict, misclassified_bow\n"
      ],
      "metadata": {
        "id": "e09hNyy-kkyu"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsUhDyhs-EK"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0WQx9XkK2GhO",
        "outputId": "822cc128-c179-439b-cb89-04cc69ca1869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagging completed. Time elapsed: 1.91 minutes.\n",
            "Vectorization completed. Time elapsed: 1.92 minutes.\n",
            "Pipeline fitting completed. Time elapsed: 2.14 minutes.\n",
            "Feature analysis completed. Time elapsed: 2.15 minutes.\n",
            "Write to pkl file completed.\n"
          ]
        }
      ],
      "source": [
        "# Define variables\n",
        "model_type = 'xgb'\n",
        "params = {\n",
        "    'xgbclassifier': {'subsample': 0.8, 'n_estimators': 150, 'max_depth': 9, 'learning_rate': 0.05, 'colsample_bytree': 0.5},\n",
        "}\n",
        "\n",
        "# Run model\n",
        "model, train, test, explainer, word_importance_dict, misclassified_bow = model_testing(X_train, y_train, X_test, y_test, params , model_type)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_importance_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP9VSIP82jpj",
        "outputId": "ab289817-d401-4d2b-e6a2-628149730f7c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'aaaaaaaa': 0.0,\n",
              " 'aaaaaaaaaaaaa': 0.0,\n",
              " 'aaaaw': 0.0,\n",
              " 'aaah': 0.0,\n",
              " 'aand': 0.0,\n",
              " 'aaron': 0.0,\n",
              " 'ab': 0.0,\n",
              " 'abad': 0.0,\n",
              " 'abalicious': 0.0,\n",
              " 'abalone': 0.0,\n",
              " 'abandon': 0.0,\n",
              " 'abandoned': 0.0,\n",
              " 'abaya': 0.0,\n",
              " 'abbia': 0.0,\n",
              " 'abc': 0.0,\n",
              " 'abdul': 0.0,\n",
              " 'abe': 0.0,\n",
              " 'aberdeen': 0.0,\n",
              " 'abide': 0.0,\n",
              " 'ability': 0.0,\n",
              " 'abillity': 0.0,\n",
              " 'abject': 0.0,\n",
              " 'ablas': 0.0,\n",
              " 'ablation': 0.0,\n",
              " 'able': 0.0019955356,\n",
              " 'ably': 0.0,\n",
              " 'abnormal': 0.0,\n",
              " 'aboard': 0.0,\n",
              " 'abolish': 0.0,\n",
              " 'abolishing': 0.0,\n",
              " 'abominable': 0.0,\n",
              " 'abortion': 0.0,\n",
              " 'abounds': 0.0,\n",
              " 'abouti': 0.0,\n",
              " 'aboutno': 0.0,\n",
              " 'aboutshouldnt': 0.0,\n",
              " 'aboutwhich': 0.0,\n",
              " 'abraham': 0.0,\n",
              " 'abras': 0.0,\n",
              " 'abrazo': 0.0,\n",
              " 'abrazos': 0.0,\n",
              " 'abroad': 0.0,\n",
              " 'absentee': 0.0,\n",
              " 'absolute': 0.0,\n",
              " 'absolutely': 0.0025113393,\n",
              " 'absolutly': 0.0,\n",
              " 'absolved': 0.0,\n",
              " 'absorb': 0.0,\n",
              " 'abstain': 0.0,\n",
              " 'abstention': 0.0,\n",
              " 'absurd': 0.0,\n",
              " 'abt': 0.0,\n",
              " 'abundance': 0.0,\n",
              " 'abuse': 0.0,\n",
              " 'abused': 0.0,\n",
              " 'abuser': 0.0,\n",
              " 'abusing': 0.0,\n",
              " 'abvious': 0.0,\n",
              " 'abyss': 0.0,\n",
              " 'abzug': 0.0,\n",
              " 'ac': 0.0,\n",
              " 'aca': 0.0,\n",
              " 'acabarloooo': 0.0,\n",
              " 'academic': 0.0,\n",
              " 'acai': 0.0,\n",
              " 'acaobamacare': 0.0,\n",
              " 'acaso': 0.0,\n",
              " 'accelerator': 0.0,\n",
              " 'accent': 0.0,\n",
              " 'accept': 0.0,\n",
              " 'acceptable': 0.0,\n",
              " 'accepted': 0.0,\n",
              " 'accepting': 0.0,\n",
              " 'accepts': 0.0,\n",
              " 'accesible': 0.0,\n",
              " 'access': 0.0,\n",
              " 'accessible': 0.0,\n",
              " 'accident': 0.0,\n",
              " 'accidental': 0.0,\n",
              " 'accidentally': 0.0,\n",
              " 'accomplice': 0.0,\n",
              " 'accomplish': 0.0,\n",
              " 'accomplished': 0.0,\n",
              " 'accomplishing': 0.0,\n",
              " 'accomplishment': 0.0,\n",
              " 'accord': 0.0,\n",
              " 'according': 0.0,\n",
              " 'account': 0.0,\n",
              " 'accountability': 0.0,\n",
              " 'accountable': 0.0,\n",
              " 'accounted': 0.0,\n",
              " 'accumulate': 0.0,\n",
              " 'accuracy': 0.0,\n",
              " 'accurate': 0.0,\n",
              " 'accurately': 0.0,\n",
              " 'accusation': 0.0,\n",
              " 'accuse': 0.0,\n",
              " 'accused': 0.0,\n",
              " 'accusing': 0.0,\n",
              " 'acd': 0.0,\n",
              " 'acerca': 0.0,\n",
              " 'acercamiento': 0.0,\n",
              " 'acerenza': 0.0,\n",
              " 'ache': 0.0,\n",
              " 'achieve': 0.0,\n",
              " 'achieved': 0.0,\n",
              " 'achievement': 0.0,\n",
              " 'achieving': 0.0,\n",
              " 'achillevs': 0.0,\n",
              " 'aching': 0.0,\n",
              " 'acid': 0.0,\n",
              " 'ack': 0.0,\n",
              " 'ackbar': 0.0,\n",
              " 'acknowledge': 0.0,\n",
              " 'acknowledged': 0.0,\n",
              " 'acknowledging': 0.0,\n",
              " 'acknowledgment': 0.0,\n",
              " 'acl': 0.0,\n",
              " 'acordabrasil': 0.0,\n",
              " 'acoustic': 0.0,\n",
              " 'acquaintance': 0.0,\n",
              " 'acquiesce': 0.0,\n",
              " 'acquired': 0.0,\n",
              " 'acquisition': 0.0,\n",
              " 'acronym': 0.0,\n",
              " 'across': 0.0,\n",
              " 'act': 0.0,\n",
              " 'acted': 0.0,\n",
              " 'acting': 0.0,\n",
              " 'action': 0.0,\n",
              " 'actionspeople': 0.0,\n",
              " 'activate': 0.0,\n",
              " 'activated': 0.0,\n",
              " 'activation': 0.0,\n",
              " 'active': 0.0,\n",
              " 'actively': 0.0,\n",
              " 'activism': 0.0,\n",
              " 'activist': 0.0,\n",
              " 'activity': 0.0,\n",
              " 'actor': 0.0,\n",
              " 'actress': 0.0,\n",
              " 'actressmodel': 0.0,\n",
              " 'actresswhat': 0.0,\n",
              " 'actual': 0.0028251265,\n",
              " 'actually': 0.0037856689,\n",
              " 'actualy': 0.0,\n",
              " 'acu': 0.0,\n",
              " 'acueste': 0.0,\n",
              " 'acumen': 0.0,\n",
              " 'acute': 0.0,\n",
              " 'ad': 0.0,\n",
              " 'adam': 0.0029616342,\n",
              " 'adami': 0.0,\n",
              " 'adani': 0.0,\n",
              " 'adapt': 0.0,\n",
              " 'adaptation': 0.0,\n",
              " 'adapted': 0.0,\n",
              " 'adapter': 0.0,\n",
              " 'adaptive': 0.0,\n",
              " 'adat': 0.0,\n",
              " 'adbut': 0.0,\n",
              " 'add': 0.0,\n",
              " 'added': 0.0,\n",
              " 'adderall': 0.0,\n",
              " 'addict': 0.0,\n",
              " 'addicted': 0.0,\n",
              " 'addicting': 0.0,\n",
              " 'addiction': 0.0,\n",
              " 'addictive': 0.0,\n",
              " 'adding': 0.0,\n",
              " 'addison': 0.0,\n",
              " 'addition': 0.0,\n",
              " 'additional': 0.0,\n",
              " 'address': 0.0,\n",
              " 'addressed': 0.0,\n",
              " 'addressing': 0.0,\n",
              " 'adequate': 0.0,\n",
              " 'adhere': 0.0,\n",
              " 'adhering': 0.0,\n",
              " 'adichie': 0.0,\n",
              " 'adirondacks': 0.0,\n",
              " 'adis': 0.0,\n",
              " 'adjust': 0.0,\n",
              " 'adjustable': 0.0,\n",
              " 'adjustment': 0.0,\n",
              " 'adjutant': 0.0,\n",
              " 'adk': 0.0,\n",
              " 'admin': 0.0,\n",
              " 'administration': 0.0,\n",
              " 'administrator': 0.0,\n",
              " 'admins': 0.0,\n",
              " 'admirable': 0.0,\n",
              " 'admirador': 0.0,\n",
              " 'admiral': 0.0,\n",
              " 'admiration': 0.0,\n",
              " 'admire': 0.0,\n",
              " 'admired': 0.0,\n",
              " 'admiring': 0.0,\n",
              " 'admission': 0.0,\n",
              " 'admit': 0.0,\n",
              " 'admittance': 0.0,\n",
              " 'admitted': 0.0,\n",
              " 'admittedly': 0.0,\n",
              " 'admitting': 0.0,\n",
              " 'adn': 0.0,\n",
              " 'adolescent': 0.0,\n",
              " 'adopt': 0.0,\n",
              " 'adopted': 0.0,\n",
              " 'adopting': 0.0,\n",
              " 'adoption': 0.0,\n",
              " 'adorable': 0.0,\n",
              " 'adorablegod': 0.0,\n",
              " 'adore': 0.0,\n",
              " 'adress': 0.0,\n",
              " 'adrian': 0.0,\n",
              " 'adult': 0.0,\n",
              " 'adulthood': 0.0,\n",
              " 'advance': 0.0,\n",
              " 'advanced': 0.0,\n",
              " 'advantage': 0.0,\n",
              " 'adventure': 0.0,\n",
              " 'adventurous': 0.0,\n",
              " 'advert': 0.0,\n",
              " 'advertise': 0.0,\n",
              " 'advertisement': 0.0,\n",
              " 'advertising': 0.0,\n",
              " 'advetising': 0.0,\n",
              " 'advice': 0.0,\n",
              " 'advise': 0.0,\n",
              " 'advises': 0.0,\n",
              " 'advisor': 0.0,\n",
              " 'advocacy': 0.0,\n",
              " 'advocate': 0.0,\n",
              " 'advocated': 0.0,\n",
              " 'advocatesorry': 0.0,\n",
              " 'advocating': 0.0,\n",
              " 'ae': 0.0,\n",
              " 'aer': 0.0,\n",
              " 'aerobiccardio': 0.0,\n",
              " 'aery': 0.0,\n",
              " 'aesthetic': 0.0,\n",
              " 'aetheryte': 0.0,\n",
              " 'aethiest': 0.0,\n",
              " 'af': 0.0,\n",
              " 'afar': 0.0,\n",
              " 'afb': 0.0,\n",
              " 'afcw': 0.0,\n",
              " 'affect': 0.0,\n",
              " 'affected': 0.0,\n",
              " 'affecting': 0.0,\n",
              " 'affection': 0.0,\n",
              " 'affiord': 0.0,\n",
              " 'affliction': 0.0,\n",
              " 'afford': 0.0,\n",
              " 'affordable': 0.0,\n",
              " 'afforded': 0.0,\n",
              " 'afghanistan': 0.0,\n",
              " 'aficionado': 0.0,\n",
              " 'afix': 0.0,\n",
              " 'afks': 0.0,\n",
              " 'afloat': 0.0,\n",
              " 'afoot': 0.0,\n",
              " 'afortunado': 0.0,\n",
              " 'afraid': 0.0,\n",
              " 'africa': 0.0,\n",
              " 'african': 0.0,\n",
              " 'afterall': 0.0,\n",
              " 'afternoon': 0.0,\n",
              " 'afterwards': 0.0,\n",
              " 'ag': 0.0,\n",
              " 'againcrist': 0.0,\n",
              " 'againi': 0.0,\n",
              " 'againthen': 0.0,\n",
              " 'agazaryan': 0.0,\n",
              " 'age': 0.0,\n",
              " 'aged': 0.0,\n",
              " 'agency': 0.0,\n",
              " 'agenda': 0.0,\n",
              " 'agent': 0.0,\n",
              " 'aggressive': 0.0,\n",
              " 'aggro': 0.0,\n",
              " 'agh': 0.0,\n",
              " 'agile': 0.0,\n",
              " 'aging': 0.0,\n",
              " 'ago': 0.003155829,\n",
              " 'agobottom': 0.0,\n",
              " 'agonisingly': 0.0,\n",
              " 'agora': 0.0,\n",
              " 'agoso': 0.0,\n",
              " 'agree': 0.0,\n",
              " 'agreed': 0.0,\n",
              " 'agreeing': 0.0,\n",
              " 'agreement': 0.0,\n",
              " 'agreen': 0.0,\n",
              " 'agrees': 0.0,\n",
              " 'agreewhich': 0.0,\n",
              " 'agreeyoure': 0.0,\n",
              " 'agricultural': 0.0,\n",
              " 'agriculture': 0.0,\n",
              " 'agsaforgcoulddonaldtrumpstwitterfeedbeweaponized': 0.0,\n",
              " 'ah': 0.0,\n",
              " 'aha': 0.0,\n",
              " 'ahah': 0.0,\n",
              " 'ahaha': 0.0,\n",
              " 'ahahahahaha': 0.0,\n",
              " 'ahashahaha': 0.0,\n",
              " 'ahead': 0.0,\n",
              " 'ahem': 0.0,\n",
              " 'ahh': 0.0,\n",
              " 'ahha': 0.0,\n",
              " 'ahhh': 0.0,\n",
              " 'ahhhh': 0.0,\n",
              " 'ahhing': 0.0,\n",
              " 'ahhthanks': 0.0,\n",
              " 'ahi': 0.0,\n",
              " 'ahjust': 0.0,\n",
              " 'ahole': 0.0,\n",
              " 'ahthe': 0.0,\n",
              " 'ai': 0.0,\n",
              " 'aid': 0.0,\n",
              " 'aide': 0.0,\n",
              " 'aiding': 0.0,\n",
              " 'aiea': 0.0,\n",
              " 'aiko': 0.0,\n",
              " 'aileen': 0.0,\n",
              " 'ailment': 0.0,\n",
              " 'aim': 0.0,\n",
              " 'aime': 0.0,\n",
              " 'aimed': 0.0,\n",
              " 'aimee': 0.0,\n",
              " 'aiming': 0.0,\n",
              " 'aimlessly': 0.0,\n",
              " 'aipac': 0.0,\n",
              " 'air': 0.0,\n",
              " 'aired': 0.0,\n",
              " 'airing': 0.0,\n",
              " 'airliner': 0.0,\n",
              " 'airplane': 0.0,\n",
              " 'airport': 0.0,\n",
              " 'airtight': 0.0,\n",
              " 'airwave': 0.0,\n",
              " 'aisle': 0.0,\n",
              " 'ak': 0.0,\n",
              " 'aka': 0.0,\n",
              " 'akin': 0.0,\n",
              " 'akitakaorita': 0.0,\n",
              " 'aktivate': 0.0,\n",
              " 'al': 0.0,\n",
              " 'ala': 0.0,\n",
              " 'alabama': 0.0,\n",
              " 'alan': 0.0,\n",
              " 'alans': 0.0,\n",
              " 'alarm': 0.0,\n",
              " 'alarming': 0.0,\n",
              " 'alaska': 0.0,\n",
              " 'alaskan': 0.0,\n",
              " 'alawsat': 0.0,\n",
              " 'alba': 0.0,\n",
              " 'albany': 0.0,\n",
              " 'albeit': 0.0,\n",
              " 'albert': 0.0,\n",
              " 'alberta': 0.0,\n",
              " 'albertson': 0.0,\n",
              " 'album': 0.0,\n",
              " 'albuquerque': 0.0,\n",
              " 'alchemy': 0.0,\n",
              " 'alcohol': 0.0,\n",
              " 'aldrin': 0.0,\n",
              " 'ale': 0.0,\n",
              " 'alec': 0.0,\n",
              " 'alert': 0.0,\n",
              " 'alex': 0.0,\n",
              " 'alexandra': 0.0,\n",
              " 'alfonse': 0.0,\n",
              " 'algae': 0.0,\n",
              " 'algore': 0.0,\n",
              " 'alguna': 0.0,\n",
              " 'alhasani': 0.0,\n",
              " 'ali': 0.0,\n",
              " 'alice': 0.0,\n",
              " 'alicia': 0.0,\n",
              " 'alien': 0.0,\n",
              " 'alienating': 0.0,\n",
              " 'alike': 0.0,\n",
              " 'alimony': 0.0,\n",
              " 'aline': 0.0,\n",
              " 'alisha': 0.0,\n",
              " 'alistair': 0.0,\n",
              " 'alittle': 0.0,\n",
              " 'alive': 0.0,\n",
              " 'all': 0.0,\n",
              " 'allah': 0.0,\n",
              " 'allahu': 0.0,\n",
              " 'allan': 0.0,\n",
              " 'allbut': 0.0,\n",
              " 'allconsequently': 0.0,\n",
              " 'alle': 0.0,\n",
              " 'allen': 0.0,\n",
              " 'alleviated': 0.0,\n",
              " 'allez': 0.0,\n",
              " 'allfound': 0.0,\n",
              " 'allha': 0.0,\n",
              " 'allhttpnewscnetcomantisopapipalawmakerswantinternetbillofrights': 0.0,\n",
              " 'alliance': 0.0,\n",
              " 'allireland': 0.0,\n",
              " 'alliteration': 0.0,\n",
              " 'allman': 0.0,\n",
              " 'allmost': 0.0,\n",
              " 'allnba': 0.0,\n",
              " 'allnot': 0.0,\n",
              " 'allonsy': 0.0,\n",
              " 'allotted': 0.0,\n",
              " 'allow': 0.0034763683,\n",
              " 'allowed': 0.0,\n",
              " 'allowedthis': 0.0,\n",
              " 'allowedwer': 0.0,\n",
              " 'allowing': 0.0,\n",
              " 'allows': 0.0,\n",
              " 'allstars': 0.0,\n",
              " 'allsu': 0.0,\n",
              " 'allthreeforgot': 0.0,\n",
              " 'alltogether': 0.0,\n",
              " 'allucoelacanths': 0.0,\n",
              " 'allusion': 0.0,\n",
              " 'ally': 0.0,\n",
              " 'almond': 0.0,\n",
              " 'almost': 0.0,\n",
              " 'almuadon': 0.0,\n",
              " 'aloha': 0.0,\n",
              " 'alone': 0.0,\n",
              " 'along': 0.0,\n",
              " 'alongside': 0.0,\n",
              " 'alonzo': 0.0,\n",
              " 'alonzoheal': 0.0,\n",
              " 'alos': 0.0,\n",
              " 'alot': 0.0,\n",
              " 'alows': 0.0,\n",
              " 'alpha': 0.0,\n",
              " 'alphabet': 0.0,\n",
              " 'already': 0.0,\n",
              " 'alreadyi': 0.0,\n",
              " 'alright': 0.0,\n",
              " 'alrightpost': 0.0,\n",
              " 'alsarkhi': 0.0,\n",
              " 'alsharq': 0.0,\n",
              " 'also': 0.0021897627,\n",
              " 'alsoget': 0.0,\n",
              " 'alt': 0.0,\n",
              " 'alta': 0.0,\n",
              " 'alter': 0.0,\n",
              " 'altered': 0.0,\n",
              " 'alternate': 0.0,\n",
              " 'alternative': 0.0,\n",
              " 'alternatively': 0.0,\n",
              " 'altfired': 0.0,\n",
              " 'although': 0.0016049679,\n",
              " 'altogether': 0.0,\n",
              " 'alton': 0.0,\n",
              " 'altruism': 0.0,\n",
              " 'alumnus': 0.0,\n",
              " 'alvin': 0.0,\n",
              " 'alway': 0.0,\n",
              " 'alwaya': 0.0,\n",
              " 'alwayas': 0.0,\n",
              " 'always': 0.0016355505,\n",
              " 'ama': 0.0,\n",
              " 'amaaaazing': 0.0,\n",
              " 'amada': 0.0,\n",
              " 'amanda': 0.0,\n",
              " 'amash': 0.0,\n",
              " 'amashs': 0.0,\n",
              " 'amasing': 0.0,\n",
              " 'amateur': 0.0,\n",
              " 'amaze': 0.0,\n",
              " 'amazed': 0.0,\n",
              " 'amazement': 0.0,\n",
              " 'amazes': 0.0,\n",
              " 'amazing': 0.00157754,\n",
              " 'amazingly': 0.0,\n",
              " 'amazingthings': 0.0,\n",
              " 'amazon': 0.0,\n",
              " 'ambassador': 0.0,\n",
              " 'amber': 0.0,\n",
              " 'ambled': 0.0,\n",
              " 'ambrosius': 0.0,\n",
              " 'ambulance': 0.0,\n",
              " 'amc': 0.0,\n",
              " 'amen': 0.002567951,\n",
              " 'amenable': 0.0,\n",
              " 'amend': 0.0,\n",
              " 'amendment': 0.0,\n",
              " 'amends': 0.0,\n",
              " 'amercia': 0.0,\n",
              " 'ameren': 0.0,\n",
              " 'america': 0.0,\n",
              " 'american': 0.0,\n",
              " 'americanot': 0.0,\n",
              " 'amg': 0.0,\n",
              " 'ami': 0.0,\n",
              " 'amie': 0.0,\n",
              " 'amiga': 0.0,\n",
              " 'amigas': 0.0,\n",
              " 'amigo': 0.0,\n",
              " 'amiibo': 0.0,\n",
              " 'amiigo': 0.0,\n",
              " 'amis': 0.0,\n",
              " 'ammo': 0.0,\n",
              " 'amnesty': 0.0,\n",
              " 'amomg': 0.0,\n",
              " 'among': 0.0,\n",
              " 'amor': 0.0,\n",
              " 'amorangelitos': 0.0,\n",
              " 'amorosa': 0.0,\n",
              " 'amortization': 0.0,\n",
              " 'amount': 0.0,\n",
              " 'amp': 0.0,\n",
              " 'amped': 0.0,\n",
              " 'ample': 0.0,\n",
              " 'amputation': 0.0,\n",
              " 'amputee': 0.0,\n",
              " 'amtrack': 0.0,\n",
              " 'amulet': 0.0,\n",
              " 'amuse': 0.0,\n",
              " 'amusing': 0.0,\n",
              " 'amy': 0.0032844758,\n",
              " 'amys': 0.0,\n",
              " 'amzing': 0.0,\n",
              " 'an': 0.0,\n",
              " 'anagram': 0.0,\n",
              " 'analize': 0.0,\n",
              " 'analog': 0.0,\n",
              " 'analogous': 0.0,\n",
              " 'analogue': 0.0,\n",
              " 'analogy': 0.0,\n",
              " 'analyse': 0.0,\n",
              " 'analysis': 0.0,\n",
              " 'analyze': 0.0,\n",
              " 'analyzing': 0.0,\n",
              " 'anarchy': 0.0,\n",
              " 'anastasia': 0.0,\n",
              " 'anbd': 0.0,\n",
              " 'ancestor': 0.0,\n",
              " 'anchio': 0.0,\n",
              " 'anchoring': 0.0,\n",
              " 'ancient': 0.0,\n",
              " 'andarchaic': 0.0,\n",
              " 'andbroke': 0.0,\n",
              " 'andcommit': 0.0,\n",
              " 'anddat': 0.0,\n",
              " 'andele': 0.0,\n",
              " 'anderson': 0.0,\n",
              " 'andes': 0.0,\n",
              " 'andheri': 0.0,\n",
              " 'andor': 0.0,\n",
              " 'andre': 0.0,\n",
              " 'andrew': 0.0,\n",
              " 'andrewstone': 0.0,\n",
              " 'andriculture': 0.0,\n",
              " 'andrno': 0.0,\n",
              " 'androgynous': 0.0,\n",
              " 'android': 0.0,\n",
              " 'androidrobot': 0.0,\n",
              " 'anduneducated': 0.0,\n",
              " 'andy': 0.0,\n",
              " 'anecdote': 0.0,\n",
              " 'anesthetic': 0.0,\n",
              " 'angel': 0.0,\n",
              " 'angela': 0.0,\n",
              " 'angeles': 0.0,\n",
              " 'angelica': 0.0,\n",
              " 'angelsgood': 0.0,\n",
              " 'anger': 0.0,\n",
              " 'angie': 0.0,\n",
              " 'angle': 0.0,\n",
              " 'angrily': 0.0,\n",
              " 'angry': 0.0,\n",
              " 'angst': 0.0,\n",
              " 'angus': 0.0,\n",
              " 'ani': 0.0,\n",
              " 'anik': 0.0,\n",
              " 'animal': 0.0,\n",
              " 'animalsit': 0.0,\n",
              " 'animated': 0.0,\n",
              " 'animation': 0.0,\n",
              " 'animaux': 0.0,\n",
              " 'anime': 0.0,\n",
              " 'anita': 0.0,\n",
              " 'anj': 0.0,\n",
              " 'ankle': 0.0,\n",
              " 'ann': 0.0,\n",
              " 'anna': 0.0,\n",
              " 'annabut': 0.0,\n",
              " 'annapolis': 0.0,\n",
              " 'annapresident': 0.0,\n",
              " 'anne': 0.0,\n",
              " 'annette': 0.0,\n",
              " 'annie': 0.0,\n",
              " 'annihilate': 0.0,\n",
              " 'annihilated': 0.0,\n",
              " 'anniversary': 0.0,\n",
              " 'announce': 0.0,\n",
              " 'announced': 0.0,\n",
              " 'announcement': 0.0,\n",
              " 'announcer': 0.0,\n",
              " 'annoyance': 0.0,\n",
              " 'annoyed': 0.0,\n",
              " 'annoying': 0.0,\n",
              " 'annoys': 0.0,\n",
              " 'annual': 0.0,\n",
              " 'ano': 0.0,\n",
              " 'anoiting': 0.0,\n",
              " 'anology': 0.0,\n",
              " 'anonymous': 0.0,\n",
              " 'anonymouse': 0.0,\n",
              " 'another': 0.0029792208,\n",
              " 'anotherand': 0.0,\n",
              " 'answer': 0.0,\n",
              " 'answered': 0.0,\n",
              " 'answering': 0.0,\n",
              " 'ant': 0.0,\n",
              " 'antagonist': 0.0,\n",
              " 'antarctica': 0.0,\n",
              " 'ante': 0.0,\n",
              " 'anthem': 0.0,\n",
              " 'anthony': 0.0,\n",
              " 'anthropology': 0.0,\n",
              " 'anti': 0.0,\n",
              " 'antibody': 0.0,\n",
              " 'antic': 0.0,\n",
              " 'anticipate': 0.0,\n",
              " 'anticipation': 0.0,\n",
              " 'anticlimactic': 0.0,\n",
              " 'anticontribution': 0.0,\n",
              " 'antifun': 0.0,\n",
              " 'antigun': 0.0,\n",
              " 'antihealth': 0.0,\n",
              " 'antiinflammatories': 0.0,\n",
              " 'antijew': 0.0,\n",
              " 'antimissile': 0.0,\n",
              " 'antique': 0.0,\n",
              " 'antisemite': 0.0,\n",
              " 'antisemitism': 0.0,\n",
              " 'antisocial': 0.0,\n",
              " 'antisopa': 0.0,\n",
              " 'antiwoman': 0.0,\n",
              " 'antmall': 0.0,\n",
              " 'antman': 0.0,\n",
              " 'antonio': 0.0,\n",
              " 'anxiety': 0.0,\n",
              " 'anxious': 0.0,\n",
              " 'anxiously': 0.0,\n",
              " 'anybody': 0.0,\n",
              " 'anybodys': 0.0,\n",
              " 'anyhow': 0.0,\n",
              " 'anyhowyou': 0.0,\n",
              " 'anymore': 0.0,\n",
              " 'anymorehope': 0.0,\n",
              " 'anyone': 0.0,\n",
              " 'anything': 0.0037707465,\n",
              " 'anytime': 0.0,\n",
              " 'anyway': 0.0,\n",
              " 'anyways': 0.0,\n",
              " 'anywayz': 0.0,\n",
              " 'anywhere': 0.0,\n",
              " 'ao': 0.0,\n",
              " 'ap': 0.0,\n",
              " 'apalled': 0.0,\n",
              " 'aparently': 0.0,\n",
              " 'apart': 0.0,\n",
              " 'apartheid': 0.0,\n",
              " 'apartment': 0.0,\n",
              " 'apathetic': 0.0,\n",
              " 'ape': 0.0,\n",
              " 'apesar': 0.0,\n",
              " 'aphorism': 0.0,\n",
              " 'apical': 0.0,\n",
              " 'aplications': 0.0,\n",
              " 'apocalypse': 0.0,\n",
              " 'apollarte': 0.0,\n",
              " 'apollo': 0.0,\n",
              " 'apologetic': 0.0,\n",
              " 'apologia': 0.0,\n",
              " 'apologise': 0.0,\n",
              " 'apologist': 0.0,\n",
              " 'apologize': 0.0,\n",
              " 'apologizes': 0.0,\n",
              " 'apology': 0.0,\n",
              " 'apoyar': 0.0,\n",
              " 'apoyo': 0.0,\n",
              " 'app': 0.0,\n",
              " 'appalachian': 0.0,\n",
              " 'appalled': 0.0,\n",
              " 'appalling': 0.0,\n",
              " 'apparent': 0.0,\n",
              " 'apparently': 0.0,\n",
              " 'appeal': 0.0,\n",
              " 'appealed': 0.0,\n",
              " 'appealing': 0.0,\n",
              " 'appear': 0.0,\n",
              " 'appearance': 0.0,\n",
              " 'appears': 0.0,\n",
              " 'appease': 0.0,\n",
              " 'appendage': 0.0,\n",
              " 'appetiser': 0.0,\n",
              " 'appetite': 0.0,\n",
              " 'applaud': 0.0,\n",
              " 'applauded': 0.0,\n",
              " 'applause': 0.0,\n",
              " 'apple': 0.0,\n",
              " 'applebees': 0.0,\n",
              " 'applicable': 0.0,\n",
              " 'application': 0.0,\n",
              " 'applied': 0.0,\n",
              " 'apply': 0.0,\n",
              " 'applying': 0.0,\n",
              " 'appoint': 0.0,\n",
              " 'appointed': 0.0,\n",
              " 'appointing': 0.0,\n",
              " 'appointment': 0.0,\n",
              " 'appolgies': 0.0,\n",
              " 'appraisal': 0.0,\n",
              " 'appreciable': 0.0,\n",
              " 'appreciate': 0.0035024176,\n",
              " 'appreciated': 0.0,\n",
              " 'appreciatedlets': 0.0,\n",
              " 'appreciates': 0.0,\n",
              " 'appreciation': 0.0,\n",
              " 'appreicate': 0.0,\n",
              " 'approach': 0.0,\n",
              " 'approachable': 0.0,\n",
              " 'approached': 0.0,\n",
              " 'appropriate': 0.0,\n",
              " 'approval': 0.0,\n",
              " 'approvaland': 0.0,\n",
              " 'approve': 0.0,\n",
              " 'approved': 0.0,\n",
              " 'approving': 0.0,\n",
              " 'approximately': 0.0,\n",
              " 'apps': 0.0,\n",
              " 'appt': 0.0,\n",
              " 'apreciate': 0.0,\n",
              " 'april': 0.0,\n",
              " 'aprove': 0.0,\n",
              " 'aprovechense': 0.0,\n",
              " 'aptitude': 0.0,\n",
              " 'aptly': 0.0,\n",
              " 'aqib': 0.0,\n",
              " 'aquaponics': 0.0,\n",
              " 'aqui': 0.0,\n",
              " 'aquin': 0.0,\n",
              " 'aquired': 0.0,\n",
              " 'ar': 0.0,\n",
              " 'arab': 0.0,\n",
              " 'arabic': 0.0,\n",
              " 'arabo': 0.0,\n",
              " 'arakk': 0.0,\n",
              " 'arbeit': 0.0,\n",
              " 'arbol': 0.0,\n",
              " 'arbor': 0.0,\n",
              " 'arc': 0.0,\n",
              " 'arcane': 0.0,\n",
              " 'arch': 0.0,\n",
              " 'archery': 0.0,\n",
              " 'archieve': 0.0,\n",
              " 'architect': 0.0,\n",
              " 'architectural': 0.0,\n",
              " 'architecture': 0.0,\n",
              " 'archive': 0.0,\n",
              " 'archived': 0.0,\n",
              " 'ardent': 0.0,\n",
              " 'are': 0.0,\n",
              " 'area': 0.0,\n",
              " 'areily': 0.0,\n",
              " 'arena': 0.0,\n",
              " 'arenga': 0.0,\n",
              " 'areright': 0.0,\n",
              " 'argentina': 0.0,\n",
              " 'arguably': 0.0,\n",
              " 'argue': 0.0,\n",
              " 'arguing': 0.0,\n",
              " 'argument': 0.0,\n",
              " 'aria': 0.0,\n",
              " 'arises': 0.0,\n",
              " 'ariund': 0.0,\n",
              " 'arizona': 0.0,\n",
              " 'arizonan': 0.0,\n",
              " 'arkadalar': 0.0,\n",
              " 'arkansas': 0.0,\n",
              " 'arlington': 0.0,\n",
              " 'arm': 0.0,\n",
              " 'armageddon': 0.0,\n",
              " 'armament': 0.0,\n",
              " 'armani': 0.0,\n",
              " 'armarkatcom': 0.0,\n",
              " 'armatraj': 0.0,\n",
              " 'armband': 0.0,\n",
              " 'armed': 0.0,\n",
              " 'arming': 0.0,\n",
              " 'armor': 0.0,\n",
              " 'armoured': 0.0,\n",
              " 'armsmirin': 0.0,\n",
              " 'armstongs': 0.0,\n",
              " 'armstrong': 0.0,\n",
              " 'army': 0.0,\n",
              " 'arnold': 0.0,\n",
              " 'arnt': 0.0,\n",
              " 'aronberg': 0.0,\n",
              " 'around': 0.003019748,\n",
              " 'aroundexcept': 0.0,\n",
              " 'aroundkick': 0.0,\n",
              " 'arrange': 0.0,\n",
              " 'arranged': 0.0,\n",
              " 'arrangement': 0.0,\n",
              " 'arranging': 0.0,\n",
              " 'arreglar': 0.0,\n",
              " 'arrest': 0.0,\n",
              " 'arrested': 0.0,\n",
              " 'arrival': 0.0,\n",
              " 'arrive': 0.0,\n",
              " 'arrived': 0.0,\n",
              " 'arrives': 0.0,\n",
              " 'arriving': 0.0,\n",
              " 'arrogant': 0.0,\n",
              " 'arrow': 0.0,\n",
              " 'arsenal': 0.0,\n",
              " 'arshedwirelessyahoocom': 0.0,\n",
              " 'art': 0.0,\n",
              " 'artculo': 0.0,\n",
              " 'arteaga': 0.0,\n",
              " 'article': 0.0,\n",
              " 'articlei': 0.0,\n",
              " 'articulate': 0.0,\n",
              " 'articulated': 0.0,\n",
              " 'artifact': 0.0,\n",
              " 'artificial': 0.0,\n",
              " 'artillery': 0.0,\n",
              " 'artist': 0.0,\n",
              " 'artistic': 0.0,\n",
              " 'artisticactivism': 0.0,\n",
              " 'artistry': 0.0,\n",
              " 'artit': 0.0,\n",
              " 'artwork': 0.0,\n",
              " 'arty': 0.0,\n",
              " 'as': 0.0,\n",
              " 'asa': 0.0,\n",
              " 'asad': 0.0,\n",
              " 'asana': 0.0,\n",
              " 'asap': 0.0,\n",
              " 'asbestos': 0.0,\n",
              " 'ascension': 0.0,\n",
              " 'ascent': 0.0,\n",
              " 'ascribe': 0.0,\n",
              " 'asdhsghjkasdhjkdhjkasdhas': 0.0,\n",
              " 'aseguro': 0.0,\n",
              " 'asfouri': 0.0,\n",
              " 'asgardian': 0.0,\n",
              " 'asgasdgjasdfkwwqfwqf': 0.0,\n",
              " 'ash': 0.0,\n",
              " 'ashamed': 0.0,\n",
              " 'ashland': 0.0,\n",
              " 'ashlee': 0.0,\n",
              " 'ashley': 0.0,\n",
              " 'ashwin': 0.0,\n",
              " 'asi': 0.0,\n",
              " 'asia': 0.0,\n",
              " 'asian': 0.0,\n",
              " 'asianamerican': 0.0,\n",
              " 'aside': 0.0,\n",
              " 'asign': 0.0,\n",
              " 'asimov': 0.0,\n",
              " 'asistive': 0.0,\n",
              " 'asju': 0.0,\n",
              " 'ask': 0.0026640228,\n",
              " 'asked': 0.0,\n",
              " 'asking': 0.0,\n",
              " 'asks': 0.0,\n",
              " 'asos': 0.0,\n",
              " 'aspect': 0.0,\n",
              " 'asperger': 0.0,\n",
              " 'aspergers': 0.0,\n",
              " 'aspergians': 0.0,\n",
              " 'aspetta': 0.0,\n",
              " 'asphalt': 0.0,\n",
              " 'aspire': 0.0,\n",
              " 'aspires': 0.0,\n",
              " 'aspiring': 0.0,\n",
              " 'ass': 0.0,\n",
              " 'assange': 0.0,\n",
              " 'assassin': 0.0,\n",
              " 'assault': 0.0,\n",
              " 'assembly': 0.0,\n",
              " 'assert': 0.0,\n",
              " 'assertion': 0.0,\n",
              " 'assessment': 0.0,\n",
              " 'asset': 0.0,\n",
              " 'asshole': 0.0,\n",
              " 'assimilate': 0.0,\n",
              " 'assist': 0.0,\n",
              " 'assistance': 0.0,\n",
              " 'assistant': 0.0,\n",
              " 'assisted': 0.0,\n",
              " 'assisting': 0.0,\n",
              " 'associate': 0.0,\n",
              " 'associated': 0.0,\n",
              " 'association': 0.0,\n",
              " 'asst': 0.0,\n",
              " 'assume': 0.0,\n",
              " 'assumed': 0.0,\n",
              " 'assuming': 0.0,\n",
              " 'assumption': 0.0,\n",
              " 'assurance': 0.0,\n",
              " 'assure': 0.0,\n",
              " 'assuredly': 0.0,\n",
              " 'asthanks': 0.0,\n",
              " 'asthma': 0.0,\n",
              " 'aston': 0.0,\n",
              " 'astonished': 0.0,\n",
              " 'astonishing': 0.0,\n",
              " 'astounded': 0.0,\n",
              " 'astros': 0.0,\n",
              " 'astute': 0.0,\n",
              " 'asu': 0.0,\n",
              " 'aswell': 0.0,\n",
              " 'aswhen': 0.0,\n",
              " 'asymmetric': 0.0,\n",
              " 'atcha': 0.0,\n",
              " 'ate': 0.0,\n",
              " 'atf': 0.0,\n",
              " 'atheism': 0.0,\n",
              " 'atheist': 0.0,\n",
              " 'athen': 0.0,\n",
              " 'athlete': 0.0,\n",
              " 'athletic': 0.0,\n",
              " 'athletics': 0.0,\n",
              " 'ati': 0.0,\n",
              " 'atk': 0.0,\n",
              " 'atl': 0.0,\n",
              " 'atlanta': 0.0,\n",
              " 'atlantic': 0.0,\n",
              " 'atleast': 0.0,\n",
              " 'atm': 0.0,\n",
              " 'atmosphere': 0.0,\n",
              " 'atom': 0.0,\n",
              " 'atomic': 0.0,\n",
              " 'atoreta': 0.0,\n",
              " 'atrial': 0.0,\n",
              " 'atrocity': 0.0,\n",
              " 'att': 0.0,\n",
              " 'atta': 0.0,\n",
              " 'attach': 0.0,\n",
              " 'attachment': 0.0,\n",
              " 'attack': 0.0,\n",
              " 'attacking': 0.0,\n",
              " 'attainable': 0.0,\n",
              " 'attemp': 0.0,\n",
              " 'attempt': 0.0,\n",
              " 'attempted': 0.0,\n",
              " 'attempting': 0.0,\n",
              " 'attenborough': 0.0,\n",
              " 'attend': 0.0,\n",
              " 'attended': 0.0,\n",
              " 'attending': 0.0,\n",
              " 'attention': 0.0028467425,\n",
              " 'attire': 0.0,\n",
              " 'attitude': 0.0,\n",
              " 'attorney': 0.0,\n",
              " 'attract': 0.0,\n",
              " 'attracted': 0.0,\n",
              " 'attracting': 0.0,\n",
              " 'attraction': 0.0,\n",
              " 'attractive': 0.0,\n",
              " 'attracts': 0.0,\n",
              " 'attribute': 0.0,\n",
              " 'attributed': 0.0,\n",
              " 'attributing': 0.0,\n",
              " 'attunement': 0.0,\n",
              " 'au': 0.0,\n",
              " 'auction': 0.0,\n",
              " 'aucune': 0.0,\n",
              " 'audience': 0.0,\n",
              " 'audio': 0.0,\n",
              " 'audit': 0.0,\n",
              " 'auditing': 0.0,\n",
              " 'audition': 0.0,\n",
              " 'audrey': 0.0,\n",
              " 'aug': 0.0,\n",
              " 'augmentation': 0.0,\n",
              " 'auguri': 0.0,\n",
              " 'august': 0.0,\n",
              " 'augusta': 0.0,\n",
              " 'aunt': 0.0,\n",
              " 'auntie': 0.0,\n",
              " 'auntlol': 0.0,\n",
              " 'auschwitz': 0.0,\n",
              " 'aussi': 0.0,\n",
              " 'aussie': 0.0,\n",
              " 'austin': 0.0,\n",
              " 'australia': 0.0,\n",
              " 'austria': 0.0,\n",
              " 'austrian': 0.0,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances = model.feature_importances_\n",
        "len(feature_importances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IijKWE5pwoH8",
        "outputId": "4ecbd5d9-00e7-407b-e2a3-7b537eb5c143"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17213"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vv-C5COA2GhP"
      },
      "outputs": [],
      "source": [
        "sorted_feature_importance = dict(sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Get the top 100 features and their importance values\n",
        "top_features = list(sorted_feature_importance.keys())[:1000]\n",
        "top_features_importance = list(sorted_feature_importance.values())[:1000]\n",
        "sorted_feature_importance"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sdKBq7ETihSJ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}