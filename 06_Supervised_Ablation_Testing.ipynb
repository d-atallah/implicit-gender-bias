{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-atallah/implicit_gender_bias/blob/main/06_Supervised_Ablation_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHf_jOR9jOca"
      },
      "source": [
        "# Import, Download, & Variable Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQD1IrVP1hrr",
        "outputId": "0c630b64-c7c2-4712-aaff-ed25466c78dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pydantic==1.8.2 in /home/gibsonce/.local/lib/python3.9/site-packages (1.8.2)\n",
            "Requirement already satisfied: typing-extensions==4.0.1 in /home/gibsonce/.local/lib/python3.9/site-packages (4.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydantic==1.8.2 typing-extensions==4.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvZ0Yn0E1j1g",
        "outputId": "77371b0a-773f-4618-c6db-1ca93eebc4b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 8.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/gibsonce/.local/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: jinja2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.8.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.26.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.20.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: setuptools in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (58.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/gibsonce/.local/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gibsonce/.local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/gibsonce/.local/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/gibsonce/.local/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.3)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/gibsonce/.local/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6WzZ3_ujTwL",
        "outputId": "b7701342-06d0-486a-f3fd-f14c125ac7a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/gibsonce/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/gibsonce/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/gibsonce/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import & download statements\n",
        "# General Statements\n",
        "#!git clone https://github.com/d-atallah/implicit_gender_bias.git\n",
        "#! pip install joblib\n",
        "#! pip install shap\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import joblib\n",
        "#from implicit_gender_bias import config as cf\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import shap\n",
        "import scipy\n",
        "\n",
        "\n",
        "# Feature selection & Model tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold, cross_validate\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD,PCA, NMF\n",
        "from sklearn.metrics import confusion_matrix,precision_score, recall_score, f1_score, accuracy_score, roc_curve, roc_auc_score, log_loss, make_scorer, average_precision_score\n",
        "\n",
        "# Model options\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# NLTK resources\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "porter = PorterStemmer()\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLgUwfU2z_4u"
      },
      "source": [
        "## Read Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPZ-eni9oS-A"
      },
      "outputs": [],
      "source": [
        "# Variables\n",
        "folder_path = '/home/gibsonce/datallah-jaymefis-gibsonce/'\n",
        "\n",
        "# Load DataFrames from pkl files\n",
        "X_train = pd.read_pickle(folder_path + 'X_train_preprocessed.pkl')\n",
        "X_test = pd.read_pickle(folder_path + 'X_test_preprocessed.pkl')\n",
        "y_train = pd.read_pickle(folder_path + 'y_train.pkl')\n",
        "y_test = pd.read_pickle(folder_path + 'y_test.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zRF7xFVjBKo"
      },
      "source": [
        "## Define Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWx_7KffOvfS"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_categorize_batch(texts):\n",
        "    docs = list(nlp.pipe(texts))\n",
        "    word_features = [' '.join([token.text for token in doc]) for doc in docs]\n",
        "    pos_tags = [' '.join([token.pos_ for token in doc]) for doc in docs]\n",
        "\n",
        "    return word_features, pos_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45e3jeJOU0rT"
      },
      "outputs": [],
      "source": [
        "def model_eval(pipeline, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates a specified model using accuracy, precision, recall, F-1 score, AUC-ROC & PR, log-Loss, and a confusion matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - pipeline (object): Fitted pipeline.\n",
        "    - X_test (list or array): Test set features.\n",
        "    - y_test (list or array): True labels.\n",
        "\n",
        "    Returns:\n",
        "    - metrics_df (pd.DataFrame): DataFrame containing the metrics and scores.\n",
        "    - confusion_df (pd.DataFrame): DataFrame containing a confusion matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Cross-validation\n",
        "    scoring = {\n",
        "        'f1': make_scorer(f1_score),\n",
        "        'pr_auc': make_scorer(average_precision_score),\n",
        "        'log_loss': make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv_results = cross_validate(pipeline, X_test, y_test, cv=cv, scoring=scoring)\n",
        "\n",
        "    # Create DataFrame to store cross-validation results\n",
        "    cv_metrics_df = pd.DataFrame({\n",
        "        'Metric': ['F1-Score', 'AUC-PR', 'Log Loss'],#\n",
        "        'CV_Mean': [np.mean(cv_results['test_f1']),\n",
        "                    np.mean(cv_results['test_pr_auc']),\n",
        "                    -np.mean(cv_results['test_log_loss'])],  # Note the negative sign for log loss\n",
        "        'CV_Std Dev': [np.std(cv_results['test_f1']),\n",
        "                       np.std(cv_results['test_pr_auc']),\n",
        "                       np.std(cv_results['test_log_loss'])\n",
        "                       ]\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Cross-validation completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "    # Print cross-validation results\n",
        "    print(\"\\nEvaluation results:\")\n",
        "    print(cv_metrics_df)\n",
        "\n",
        "\n",
        "    return cv_metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA1W2ZTYJv5L"
      },
      "outputs": [],
      "source": [
        "def model_testing(X_train, y_train, X_test, y_test, params):\n",
        "    \"\"\"\n",
        "    Runs a specified model and dimensionality reduction method with tuned hyperparameters\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (array-like): Training set features, preprocessed.\n",
        "    - y_train (array-like): Training set labels.\n",
        "    - X_test (array-like): Test set features, preprocessed.\n",
        "    - y_test (array-like): Test set labels.\n",
        "\n",
        "    Returns:\n",
        "    - Pipeline: Trained and fit pipeline with the best hyperparameters.\n",
        "    - X_train_ (array-like): Preprocessed  and vectorized training set features.\n",
        "    - X_test_ (array-like): Preprocessed  and vectorized test set features.\n",
        "    \"\"\"\n",
        "    #X_train_word_features, X_train_pos_tags = zip(*map(tokenize_and_categorize, X_train))\n",
        "    #X_test_word_features, X_test_pos_tags = zip(*map(tokenize_and_categorize, X_test))\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_batch_size = 10000\n",
        "    test_batch_size = 10000\n",
        "\n",
        "    # Initialize empty pandas Series for training data\n",
        "    X_train_ = pd.Series(dtype='object')\n",
        "    X_train_pos = pd.Series(dtype='object')\n",
        "    # Initialize empty pandas Series for testing data\n",
        "    X_test_ = pd.Series(dtype='object')\n",
        "    X_test_pos = pd.Series(dtype='object')\n",
        "\n",
        "    # Iterator for training data\n",
        "    train_iterator = (X_train.iloc[i:i+train_batch_size] for i in range(0, len(X_train), train_batch_size))\n",
        "\n",
        "    # Concatenate each batch of results for training data\n",
        "    print('Train batch start')\n",
        "    for train_batch in train_iterator:\n",
        "        X_train_word_features, X_train_pos_tags = tokenize_and_categorize_batch(train_batch)\n",
        "\n",
        "        # Concatenate to the existing Series\n",
        "        X_train_ = pd.concat([X_train_, pd.Series(X_train_word_features)])\n",
        "        X_train_pos = pd.concat([X_train_pos, pd.Series(X_train_pos_tags)])\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Train batch completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "    # Iterator for testing data\n",
        "    test_iterator = (X_test.iloc[i:i+test_batch_size] for i in range(0, len(X_test), test_batch_size))\n",
        "\n",
        "    print('Test batch start')\n",
        "    # Concatenate each batch of results for testing data\n",
        "    for test_batch in test_iterator:\n",
        "        X_test_word_features, X_test_pos_tags = tokenize_and_categorize_batch(test_batch)\n",
        "\n",
        "        # Concatenate to the existing Series\n",
        "        X_test_ = pd.concat([X_test_, pd.Series(X_test_word_features)])\n",
        "        X_test_pos = pd.concat([X_test_pos, pd.Series(X_test_pos_tags)])\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Test batch completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "    # Reset the index of training data\n",
        "    X_train_.reset_index(drop=True, inplace=True)\n",
        "    X_train_pos.reset_index(drop=True, inplace=True)\n",
        "    # Reset the index of testing data\n",
        "    X_test_.reset_index(drop=True, inplace=True)\n",
        "    X_test_pos.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    X_train = X_train_\n",
        "    X_test = X_test_\n",
        "    X_train_pos_tags = X_train_pos\n",
        "    X_test_pos_tags = X_test_pos\n",
        "\n",
        "    ablation_results = {}\n",
        "    # Iterate over unique part of speech categories\n",
        "    unique_pos_tags = [\n",
        "         'ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ','NOUN', 'NUM'\n",
        "        , 'PART', 'PRON', 'PROPN','PUNCT', 'SCONJ', 'SYM', 'VERB', 'X'#\n",
        "    ]\n",
        "    for pos_tag in unique_pos_tags:\n",
        "        print(pos_tag)\n",
        "        # Filter out the current part of speech category\n",
        "        X_train_filtered = [doc for doc, tags in zip(X_train, X_train_pos_tags) if pos_tag not in tags]\n",
        "        X_test_filtered = [doc for doc, tags in zip(X_test, X_test_pos_tags) if pos_tag not in tags]\n",
        "\n",
        "        y_train_filtered = [label for doc, label, tags in zip(X_train, y_train, X_train_pos_tags) if pos_tag not in tags]\n",
        "        y_test_filtered = [label for doc, label, tags in zip(X_test, y_test, X_test_pos_tags) if pos_tag not in tags]\n",
        "\n",
        "        # Filter the part of speech tags\n",
        "        X_train_pos_tags_filtered = [tags for tags in X_train_pos_tags if pos_tag not in tags]\n",
        "        X_test_pos_tags_filtered = [tags for tags in X_test_pos_tags if pos_tag not in tags]\n",
        "\n",
        "        # Vectorize the word features\n",
        "        word_features_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "        X_train_word_features_ = word_features_vectorizer.fit_transform(X_train_filtered)\n",
        "        X_test_word_features_ = word_features_vectorizer.transform(X_test_filtered)\n",
        "\n",
        "        # Vectorize the parts of speech tags\n",
        "        pos_tags_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "        X_train_pos_tags_ = pos_tags_vectorizer.fit_transform(X_train_pos_tags_filtered)\n",
        "        X_test_pos_tags_ = pos_tags_vectorizer.transform(X_test_pos_tags_filtered)\n",
        "\n",
        "        # Combine the vectorized word features and parts of speech tags\n",
        "        X_train_combined = scipy.sparse.hstack([X_train_word_features_, X_train_pos_tags_])\n",
        "        X_test_combined = scipy.sparse.hstack([X_test_word_features_, X_test_pos_tags_])\n",
        "\n",
        "        model = XGBClassifier(random_state=42, **params.get('xgbclassifier', {}))\n",
        "        model.fit(X_train_combined, y_train_filtered)\n",
        "\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"{pos_tag} removed model fit completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "        metrics_df = model_eval(model, X_test_combined, y_test_filtered)\n",
        "        ablation_results[pos_tag] = metrics_df\n",
        "\n",
        "    joblib.dump(ablation_results, f'{folder_path}_ablation_results.pkl')\n",
        "\n",
        "    return ablation_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsUhDyhs-EK"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgqUBL--bsI4"
      },
      "source": [
        "### Remove nulls from preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwy5nbK3z_4v"
      },
      "outputs": [],
      "source": [
        "non_nan_indices_train = ~X_train.isnull()\n",
        "non_nan_indices_test = ~X_test.isnull()\n",
        "\n",
        "# Filter y_train and y_test using the non-NaN indices\n",
        "y_train = y_train[non_nan_indices_train]\n",
        "y_test = y_test[non_nan_indices_test]\n",
        "\n",
        "# Filter X_train and X_test to remove NaN records\n",
        "X_train = X_train[non_nan_indices_train]\n",
        "X_test = X_test[non_nan_indices_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kc6_IHdbx7r"
      },
      "source": [
        "### Ablation Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG5T2udNz_4y",
        "outputId": "9723f934-00f2-40fc-e5b0-950861685ecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batch start\n",
            "Train batch completed. Time elapsed: 11.67 minutes.\n",
            "Test batch start\n",
            "Test batch completed. Time elapsed: 17.59 minutes.\n",
            "PUNCT\n",
            "PUNCT removed model fit completed. Time elapsed: 24.49 minutes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation completed. Time elapsed: 19.97 minutes.\n",
            "\n",
            "Evaluation results:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.794586    0.001437\n",
            "1    AUC-PR  0.675257    0.000775\n",
            "2  Log Loss  0.594037    0.001727\n",
            "SCONJ\n",
            "SCONJ removed model fit completed. Time elapsed: 49.91 minutes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation completed. Time elapsed: 15.23 minutes.\n",
            "\n",
            "Evaluation results:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.789836    0.000862\n",
            "1    AUC-PR  0.669149    0.001147\n",
            "2  Log Loss  0.598953    0.001029\n",
            "SYM\n",
            "SYM removed model fit completed. Time elapsed: 71.42 minutes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation completed. Time elapsed: 18.26 minutes.\n",
            "\n",
            "Evaluation results:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.794564    0.000643\n",
            "1    AUC-PR  0.675739    0.000843\n",
            "2  Log Loss  0.593839    0.001212\n",
            "VERB\n",
            "VERB removed model fit completed. Time elapsed: 90.04 minutes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation completed. Time elapsed: 0.91 minutes.\n",
            "\n",
            "Evaluation results:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.728845    0.003082\n",
            "1    AUC-PR  0.597389    0.003013\n",
            "2  Log Loss  0.636080    0.002550\n",
            "X\n",
            "X removed model fit completed. Time elapsed: 93.40 minutes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation completed. Time elapsed: 6.92 minutes.\n",
            "\n",
            "Evaluation results:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.774063    0.001053\n",
            "1    AUC-PR  0.650621    0.001096\n",
            "2  Log Loss  0.612550    0.001118\n"
          ]
        }
      ],
      "source": [
        "# Define variables\n",
        "params = {\n",
        "    'xgbclassifier': {'subsample': 0.8, 'n_estimators': 150, 'max_depth': 9, 'learning_rate': 0.05, 'colsample_bytree': 0.5},\n",
        "}\n",
        "\n",
        "# Run ablation testing\n",
        "ablation_results = model_testing(X_train, y_train, X_test, y_test, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_BfZDOVkFd3"
      },
      "outputs": [],
      "source": [
        "# Provided data\n",
        "data = [\n",
        "    (\"ADJ\", \"F1-Score\", 0.760941, 0.001806),\n",
        "    (\"ADJ\", \"AUC-PR\", 0.630041, 0.001573),\n",
        "    (\"ADJ\", \"Log Loss\", 0.624516, 0.001486),\n",
        "    (\"ADP\", \"F1-Score\", 0.787126, 0.001073),\n",
        "    (\"ADP\", \"AUC-PR\", 0.666226, 0.001381),\n",
        "    (\"ADP\", \"Log Loss\", 0.602603, 0.001933),\n",
        "    (\"ADV\", \"F1-Score\", 0.763784, 0.000713),\n",
        "    (\"ADV\", \"AUC-PR\", 0.637339, 0.000374),\n",
        "    (\"ADV\", \"Log Loss\", 0.620317, 0.000713),\n",
        "    (\"AUX\", \"F1-Score\", 0.774778, 0.001708),\n",
        "    (\"AUX\", \"AUC-PR\", 0.652002, 0.001772),\n",
        "    (\"AUX\", \"Log Loss\", 0.611575, 0.001864),\n",
        "    (\"CONJ\", \"F1-Score\", 0.789011, 0.000541),\n",
        "    (\"CONJ\", \"AUC-PR\", 0.668109, 0.000436),\n",
        "    (\"CONJ\", \"Log Loss\", 0.600078, 0.000797),\n",
        "    (\"CCONJ\", \"F1-Score\", 0.793561, 0.001265),\n",
        "    (\"CCONJ\", \"AUC-PR\", 0.674641, 0.001247),\n",
        "    (\"CCONJ\", \"Log Loss\", 0.594794, 0.001532),\n",
        "    (\"DET\", \"F1-Score\", 0.791592, 0.000687),\n",
        "    (\"DET\", \"AUC-PR\", 0.671683, 0.000878),\n",
        "    (\"DET\", \"Log Loss\", 0.597343, 0.001275),\n",
        "    (\"INTJ\", \"F1-Score\", 0.797550, 0.000983),\n",
        "    (\"INTJ\", \"AUC-PR\", 0.677920, 0.001160),\n",
        "    (\"INTJ\", \"Log Loss\", 0.593957, 0.000977),\n",
        "    (\"NOUN\", \"F1-Score\", 0.725884, 0.003907),\n",
        "    (\"NOUN\", \"AUC-PR\", 0.594031, 0.002901),\n",
        "    (\"NOUN\", \"Log Loss\", 0.635091, 0.002177),\n",
        "    (\"NUM\", \"F1-Score\", 0.787400, 0.001387),\n",
        "    (\"NUM\", \"AUC-PR\", 0.665819, 0.000999),\n",
        "    (\"NUM\", \"Log Loss\", 0.601654, 0.001036),\n",
        "    (\"PART\", \"F1-Score\", 0.784691, 0.000786),\n",
        "    (\"PART\", \"AUC-PR\", 0.662827, 0.000909),\n",
        "    (\"PART\", \"Log Loss\", 0.602660, 0.000983),\n",
        "    (\"PRON\", \"F1-Score\", 0.775020, 0.001730),\n",
        "    (\"PRON\", \"AUC-PR\", 0.647677, 0.001525),\n",
        "    (\"PRON\", \"Log Loss\", 0.616708, 0.001684),\n",
        "    (\"PROPN\", \"F1-Score\", 0.774665, 0.001458),\n",
        "    (\"PROPN\", \"AUC-PR\", 0.650410, 0.001261),\n",
        "    (\"PROPN\", \"Log Loss\", 0.621508, 0.001641),\n",
        "    (\"PUNCT\", \"F1-Score\", 0.794586, 0.001437),\n",
        "    (\"PUNCT\", \"AUC-PR\", 0.675257, 0.000775),\n",
        "    (\"PUNCT\", \"Log Loss\", 0.594037, 0.001727),\n",
        "    (\"SCONJ\", \"F1-Score\", 0.789836, 0.000862),\n",
        "    (\"SCONJ\", \"AUC-PR\", 0.669149, 0.001147),\n",
        "    (\"SCONJ\", \"Log Loss\", 0.598953, 0.001029),\n",
        "    (\"SYM\", \"F1-Score\", 0.794564, 0.000643),\n",
        "    (\"SYM\", \"AUC-PR\", 0.675739, 0.000843),\n",
        "    (\"SYM\", \"Log Loss\", 0.593839, 0.001212),\n",
        "    (\"VERB\", \"F1-Score\", 0.728845, 0.003082),\n",
        "    (\"VERB\", \"AUC-PR\", 0.597389, 0.003013),\n",
        "    (\"VERB\", \"Log Loss\", 0.636080, 0.002550),\n",
        "    (\"X\", \"F1-Score\", 0.774063, 0.001053),\n",
        "    (\"X\", \"AUC-PR\", 0.650621, 0.001096),\n",
        "    (\"X\", \"Log Loss\", 0.612550, 0.001118),\n",
        "]\n",
        "\n",
        "# Create a DataFrame from the provided data\n",
        "ablation_testing = pd.DataFrame(data, columns=[\"POS\", \"Metric\", \"CV_Mean\", \"CV_Std Dev\"])\n",
        "#joblib.dump(ablation_results, f'{folder_path}ablation_results.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oepaqDuLkFd4",
        "outputId": "4b6ce997-f399-4e0b-c0c8-6bd4b4079339"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>POS</th>\n",
              "      <th>Metric</th>\n",
              "      <th>CV_Mean</th>\n",
              "      <th>CV_Std Dev</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ADJ</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.760941</td>\n",
              "      <td>0.001806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ADP</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.787126</td>\n",
              "      <td>0.001073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ADV</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.763784</td>\n",
              "      <td>0.000713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>AUX</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.774778</td>\n",
              "      <td>0.001708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>CONJ</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.789011</td>\n",
              "      <td>0.000541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>CCONJ</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.793561</td>\n",
              "      <td>0.001265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>DET</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.791592</td>\n",
              "      <td>0.000687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>INTJ</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.797550</td>\n",
              "      <td>0.000983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>NOUN</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.725884</td>\n",
              "      <td>0.003907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>NUM</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.787400</td>\n",
              "      <td>0.001387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>PART</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.784691</td>\n",
              "      <td>0.000786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>PRON</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.775020</td>\n",
              "      <td>0.001730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>PROPN</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.774665</td>\n",
              "      <td>0.001458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>PUNCT</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.794586</td>\n",
              "      <td>0.001437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>SCONJ</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.789836</td>\n",
              "      <td>0.000862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>SYM</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.794564</td>\n",
              "      <td>0.000643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>VERB</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.728845</td>\n",
              "      <td>0.003082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>X</td>\n",
              "      <td>F1-Score</td>\n",
              "      <td>0.774063</td>\n",
              "      <td>0.001053</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      POS    Metric   CV_Mean  CV_Std Dev\n",
              "0     ADJ  F1-Score  0.760941    0.001806\n",
              "3     ADP  F1-Score  0.787126    0.001073\n",
              "6     ADV  F1-Score  0.763784    0.000713\n",
              "9     AUX  F1-Score  0.774778    0.001708\n",
              "12   CONJ  F1-Score  0.789011    0.000541\n",
              "15  CCONJ  F1-Score  0.793561    0.001265\n",
              "18    DET  F1-Score  0.791592    0.000687\n",
              "21   INTJ  F1-Score  0.797550    0.000983\n",
              "24   NOUN  F1-Score  0.725884    0.003907\n",
              "27    NUM  F1-Score  0.787400    0.001387\n",
              "30   PART  F1-Score  0.784691    0.000786\n",
              "33   PRON  F1-Score  0.775020    0.001730\n",
              "36  PROPN  F1-Score  0.774665    0.001458\n",
              "39  PUNCT  F1-Score  0.794586    0.001437\n",
              "42  SCONJ  F1-Score  0.789836    0.000862\n",
              "45    SYM  F1-Score  0.794564    0.000643\n",
              "48   VERB  F1-Score  0.728845    0.003082\n",
              "51      X  F1-Score  0.774063    0.001053"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_df = ablation_testing[ablation_testing['Metric'] == 'F1-Score']\n",
        "filtered_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sdKBq7ETihSJ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}