{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-atallah/implicit_gender_bias/blob/main/06_Supervised_Ablation_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHf_jOR9jOca"
      },
      "source": [
        "# Import, Download, & Variable Statements"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB-Lj3yZ0B9N",
        "outputId": "bcce81f0-095e-43ca-968a-31abfca5f9b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.44.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (535 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/535.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/535.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m535.7/535.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.2)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.44.1 slicer-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydantic==1.8.2 typing-extensions==4.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQD1IrVP1hrr",
        "outputId": "0c630b64-c7c2-4712-aaff-ed25466c78dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic==1.8.2\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/126.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions==4.0.1\n",
            "  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: typing-extensions, pydantic\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.1\n",
            "    Uninstalling pydantic-2.6.1:\n",
            "      Successfully uninstalled pydantic-2.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "sqlalchemy 2.0.27 requires typing-extensions>=4.6.0, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "arviz 0.15.1 requires typing-extensions>=4.1.0, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "chex 0.1.85 requires typing-extensions>=4.2.0, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "flax 0.8.1 requires typing-extensions>=4.2, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "ibis-framework 7.1.0 requires typing-extensions<5,>=4.3.0, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "inflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n",
            "librosa 0.10.1 requires typing-extensions>=4.1.1, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "pydantic-core 2.16.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-1.8.2 typing-extensions-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvZ0Yn0E1j1g",
        "outputId": "77371b0a-773f-4618-c6db-1ca93eebc4b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6WzZ3_ujTwL",
        "outputId": "b7701342-06d0-486a-f3fd-f14c125ac7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import & download statements\n",
        "# General Statements\n",
        "#!git clone https://github.com/d-atallah/implicit_gender_bias.git\n",
        "#! pip install joblib\n",
        "#! pip install shap\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import joblib\n",
        "#from implicit_gender_bias import config as cf\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import shap\n",
        "import scipy\n",
        "\n",
        "\n",
        "# Feature selection & Model tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold, cross_validate\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD,PCA, NMF\n",
        "from sklearn.metrics import confusion_matrix,precision_score, recall_score, f1_score, accuracy_score, roc_curve, roc_auc_score, log_loss, make_scorer, average_precision_score\n",
        "\n",
        "# Model options\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# NLTK resources\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "porter = PorterStemmer()\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye2rxKTw0GYd",
        "outputId": "6adef4c6-fb61-4d08-a47c-0bd065217052"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLgUwfU2z_4u"
      },
      "source": [
        "## Read Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pPZ-eni9oS-A"
      },
      "outputs": [],
      "source": [
        "# Variables\n",
        "folder_path = '/content/drive/MyDrive/Supervised Learning Notebooks/'#'/home/gibsonce/datallah-jaymefis-gibsonce/'\n",
        "\n",
        "# Load DataFrames from pkl files\n",
        "X_train = pd.read_pickle(folder_path + 'X_train_preprocessed.pkl')\n",
        "X_test = pd.read_pickle(folder_path + 'X_test_preprocessed.pkl')\n",
        "y_train = pd.read_pickle(folder_path + 'y_train.pkl')\n",
        "y_test = pd.read_pickle(folder_path + 'y_test.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zRF7xFVjBKo"
      },
      "source": [
        "## Define Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_categorize(text):\n",
        "    # Tokenize and process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "    word_features = ' '.join([token.text for token in doc])\n",
        "    pos_tags = ' '.join([token.pos_ for token in doc])\n",
        "\n",
        "    return word_features, pos_tags"
      ],
      "metadata": {
        "id": "jWx_7KffOvfS"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_eval(pipeline, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates a specified model using accuracy, precision, recall, F-1 score, AUC-ROC & PR, log-Loss, and a confusion matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - pipeline (object): Fitted pipeline.\n",
        "    - X_test (list or array): Test set features.\n",
        "    - y_test (list or array): True labels.\n",
        "\n",
        "    Returns:\n",
        "    - metrics_df (pd.DataFrame): DataFrame containing the metrics and scores.\n",
        "    - confusion_df (pd.DataFrame): DataFrame containing a confusion matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Cross-validation\n",
        "    scoring = {\n",
        "        'f1': make_scorer(f1_score),\n",
        "        'pr_auc': make_scorer(average_precision_score),\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv_results = cross_validate(pipeline, X_test, y_test, cv=cv, scoring=scoring)\n",
        "\n",
        "    # Create DataFrame to store cross-validation results\n",
        "    cv_metrics_df = pd.DataFrame({\n",
        "    'Metric': ['F1-Score','AUC-PR'],\n",
        "    'CV_Mean': [np.mean(cv_results['test_f1']),\n",
        "                np.mean(cv_results['test_pr_auc']),\n",
        "                ],\n",
        "    'CV_Std Dev': [np.std(cv_results['test_f1']),\n",
        "                   np.std(cv_results['test_pr_auc']),\n",
        "                  ]\n",
        "    })\n",
        "\n",
        "    print(\"\\nCross Validation:\")\n",
        "    print(cv_metrics_df)\n",
        "\n",
        "    return cv_metrics_df"
      ],
      "metadata": {
        "id": "45e3jeJOU0rT"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_testing(X_train, y_train, X_test, y_test, params):\n",
        "    \"\"\"\n",
        "    Runs a specified model and dimensionality reduction method with tuned hyperparameters\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (array-like): Training set features, preprocessed.\n",
        "    - y_train (array-like): Training set labels.\n",
        "    - X_test (array-like): Test set features, preprocessed.\n",
        "    - y_test (array-like): Test set labels.\n",
        "\n",
        "    Returns:\n",
        "    - Pipeline: Trained and fit pipeline with the best hyperparameters.\n",
        "    - X_train_ (array-like): Preprocessed  and vectorized training set features.\n",
        "    - X_test_ (array-like): Preprocessed  and vectorized test set features.\n",
        "    \"\"\"\n",
        "    X_train_word_features, X_train_pos_tags = zip(*map(tokenize_and_categorize, X_train))\n",
        "    X_test_word_features, X_test_pos_tags = zip(*map(tokenize_and_categorize, X_test))\n",
        "\n",
        "    ablation_results = {}\n",
        "    # Iterate over unique part of speech categories\n",
        "    unique_pos_tags = [\n",
        "        'ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ',\n",
        "        'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X'\n",
        "    ]\n",
        "    for pos_tag in unique_pos_tags:\n",
        "        print(pos_tag)\n",
        "        # Filter out the current part of speech category\n",
        "        X_train_filtered = [doc for doc, tags in zip(X_train, X_train_pos_tags) if pos_tag not in tags]\n",
        "        X_test_filtered = [doc for doc, tags in zip(X_test, X_test_pos_tags) if pos_tag not in tags]\n",
        "\n",
        "        y_train_filtered = [label for doc, label, tags in zip(X_train, y_train, X_train_pos_tags) if pos_tag not in tags]\n",
        "        y_test_filtered = [label for doc, label, tags in zip(X_test, y_test, X_test_pos_tags) if pos_tag not in tags]\n",
        "\n",
        "        # Filter the part of speech tags\n",
        "        X_train_pos_tags_filtered = [tags for tags in X_train_pos_tags if pos_tag not in tags]\n",
        "        X_test_pos_tags_filtered = [tags for tags in X_test_pos_tags if pos_tag not in tags]\n",
        "\n",
        "        # Vectorize the word features\n",
        "        word_features_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "        X_train_word_features_ = word_features_vectorizer.fit_transform(X_train_filtered)\n",
        "        X_test_word_features_ = word_features_vectorizer.transform(X_test_filtered)\n",
        "\n",
        "        # Vectorize the parts of speech tags\n",
        "        pos_tags_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "        X_train_pos_tags_ = pos_tags_vectorizer.fit_transform(X_train_pos_tags_filtered)\n",
        "        X_test_pos_tags_ = pos_tags_vectorizer.transform(X_test_pos_tags_filtered)\n",
        "\n",
        "        # Combine the vectorized word features and parts of speech tags\n",
        "        X_train_combined = scipy.sparse.hstack([X_train_word_features_, X_train_pos_tags_])\n",
        "        X_test_combined = scipy.sparse.hstack([X_test_word_features_, X_test_pos_tags_])\n",
        "\n",
        "        model = XGBClassifier(random_state=42, **params.get('xgbclassifier', {}))\n",
        "        model.fit(X_train_combined, y_train_filtered)\n",
        "\n",
        "        metrics_df = model_eval(model, X_test_combined, y_test_filtered)\n",
        "        ablation_results[pos_tag] = metrics_df\n",
        "\n",
        "    joblib.dump(ablation_results, f'{folder_path}_ablation_results.pkl')\n",
        "\n",
        "    return ablation_results\n"
      ],
      "metadata": {
        "id": "GA1W2ZTYJv5L"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsUhDyhs-EK"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove nulls from preprocessing"
      ],
      "metadata": {
        "id": "rgqUBL--bsI4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Uwy5nbK3z_4v"
      },
      "outputs": [],
      "source": [
        "non_nan_indices_train = ~X_train.isnull()\n",
        "non_nan_indices_test = ~X_test.isnull()\n",
        "\n",
        "# Filter y_train and y_test using the non-NaN indices\n",
        "y_train = y_train[non_nan_indices_train]\n",
        "y_test = y_test[non_nan_indices_test]\n",
        "\n",
        "# Filter X_train and X_test to remove NaN records\n",
        "X_train = X_train[non_nan_indices_train]\n",
        "X_test = X_test[non_nan_indices_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ablation Testing"
      ],
      "metadata": {
        "id": "1kc6_IHdbx7r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG5T2udNz_4y",
        "outputId": "9723f934-00f2-40fc-e5b0-950861685ecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADJ\n",
            "\n",
            "Cross Validation:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.540915    0.031986\n",
            "1    AUC-PR  0.527738    0.020105\n",
            "ADP\n",
            "\n",
            "Cross Validation:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.539265    0.028309\n",
            "1    AUC-PR  0.522751    0.015566\n",
            "ADV\n",
            "\n",
            "Cross Validation:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.544264    0.028900\n",
            "1    AUC-PR  0.526063    0.017976\n",
            "AUX\n",
            "\n",
            "Cross Validation:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.528242    0.012125\n",
            "1    AUC-PR  0.512464    0.006138\n",
            "CONJ\n",
            "\n",
            "Cross Validation:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.543537    0.021847\n",
            "1    AUC-PR  0.521666    0.009091\n",
            "CCONJ\n",
            "\n",
            "Cross Validation:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.540415    0.018895\n",
            "1    AUC-PR  0.523475    0.014614\n",
            "DET\n",
            "\n",
            "Cross Validation:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.542673    0.011183\n",
            "1    AUC-PR  0.523491    0.004620\n",
            "INTJ\n",
            "\n",
            "Cross Validation:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.557407    0.019768\n",
            "1    AUC-PR  0.529264    0.010309\n",
            "NOUN\n",
            "\n",
            "Cross Validation:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.507115    0.063380\n",
            "1    AUC-PR  0.518013    0.040954\n",
            "NUM\n"
          ]
        }
      ],
      "source": [
        "# Define variables\n",
        "params = {\n",
        "    'xgbclassifier': {'subsample': 0.8, 'n_estimators': 150, 'max_depth': 9, 'learning_rate': 0.05, 'colsample_bytree': 0.5},\n",
        "    #'truncatedsvd': {'n_components': 150}\n",
        "}\n",
        "\n",
        "# Run ablation testing\n",
        "ablation_results = model_testing(X_train, y_train, X_test, y_test, params)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sdKBq7ETihSJ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}